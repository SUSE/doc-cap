<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha-cap-ha"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&productname; &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec-cap-ha-prod">
  <title>Configuring &cap; for &ha;</title>

  <para>
   There are two ways to make your &productname; deployment highly available.
   The simplest method is to set the <literal>config.HA</literal> parameter in
   your deployment configuration file to <literal>true</literal>. The second
   method is to create custom configuration files with your own sizing values.
  </para>

  <sect2 xml:id="sec-cap-ha-prereqs">
   <title>Prerequisites</title>
   <itemizedlist>
    <listitem>
     <para>
      A running deployment of &productname; where the database roles,
      <literal>mysql</literal>, are single availability for
      <literal>scf</literal>. By starting with single availability
      <literal>mysql</literal> roles then transitioning to high availability
      <literal>mysql</literal> roles, it allows other resources dependent on the
      database to come up properly. It is not recommended for initial
      deployments of &productname; to have the <literal>mysql</literal> roles in
      high availability mode.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec-cap-ha-defaults">
   <title>Finding Default and Allowable Sizing Values</title>
   <para>
    The <literal>sizing:</literal> section in the &helm;
    <filename>values.yaml</filename> files for the <literal>scf</literal> chart
    describes which roles can be scaled, and the scaling options for each role.
    You may use <command>helm inspect</command> to read the
    <literal>sizing:</literal> section in the &helm; chart:
   </para>
<screen>&prompt.user;helm inspect suse/cf | less +/sizing:</screen>
   <para>
    Another way is to use Perl to extract the information for each role from
    the <literal>sizing:</literal> section.
   </para>
<screen>
&prompt.user;helm inspect values suse/cf | \
perl -ne '/^sizing/..0 and do { print $.,":",$_ if /^ [a-z]/ || /high avail|scale|count/ }'

<!-- TODO-CAP2 replace example -->
</screen>
   <para>
    The default <filename>values.yaml</filename> files are also included in
    this guide at <xref linkend="app-scf-values-yaml"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-cap-ha-simple">
   <title>Simple &ha; Configuration</title>
   <important>
    &fresh-namespace;
    </important>
   <para>
    The simplest way to make your &productname; deployment highly available is
    to set <literal>config.HA</literal> to <literal>true</literal>. This
    changes the size of all roles to the minimum required for a highly
    available deployment. In your deployment configuration file,
    <filename>scf-config-values.yaml</filename>, include the following.
   </para>
<screen>
config:
  # Flag to activate high-availability mode
  HA: true
</screen>
   <para>
    Use <command>helm install</command> to deploy or
    <command>helm upgrade</command> to apply the change to an existing
    deployment.
   </para>
<screen>&prompt.user;kubectl create namespace <replaceable>scf</replaceable>

&prompt.user;helm install <replaceable>susecf-scf</replaceable> suse/cf \
--namespace <replaceable>scf</replaceable> \
--values scf-config-values.yaml
</screen>
   <para>
    When <literal>config.HA</literal> is set to <literal>true</literal>,
    instances groups can be made to allow sizing values other than the minimum
    required for &ha; mode. To do so, set the
    <literal>config.HA_strict</literal> flag to <literal>false</literal> in
    conjunction with specifying the count for a given instance group in the
    <literal>sizing:</literal> section. As an example, the following
    configuration enables &ha; mode while only using 1 instance of the
    <literal>mysql</literal> instance group.
   </para>
<screen>
config:
  # Flag to activate high-availability mode
  HA: true
  HA_strict: false
sizing:
  mysql:
    count: 1
</screen>
  </sect2>

  <sect2 xml:id="sec-cap-ha-custom">
   <title>Example Custom &ha; Configurations</title>
   <para>
    The following example &ha; configuration file is for the
    <literal>scf</literal> namespaces. The example values are not meant to be
    copied, as these depend on your particular deployment and requirements.
   </para>
   <para>
    This example is for <literal>scf</literal>,
    <filename>scf-sizing.yaml</filename>. The values specified are the minimum
    required for a &ha; deployment, equivalent to setting
    <literal>config.HA</literal> to true.
   </para>
<screen>
sizing:
  adapter:
    count: 2
  api_group:
    count: 2
  cc_clock:
    count: 2
  cc_uploader:
    count: 2
  cc_worker:
    count: 2
  diego_api:
    count: 2
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  diego_ssh:
    count: 2
  doppler:
    count: 2
  locket:
    count: 2
  log_api:
    count: 2
  log_cache_scheduler:
    count: 2
  loggregator_agent:
    count: 2
  mysql:
    count: 3
  mysql_proxy:
    count: 2
  nats:
    count: 2
  nfs_broker:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2
  syslog_scheduler:
    count: 2
  tcp_router:
    count: 2
</screen>
   <para>
    When using custom sizing configurations, take note that the
    <literal>mysql</literal> instance group for <literal>scf</literal> must have
    have an odd number of instances.
   </para>
   <important>
    &fresh-namespace;
   </important>
   <para>
    Deploy SCF with these commands:
   </para>
<screen>&prompt.user;kubectl create namespace <replaceable>scf</replaceable>

&prompt.user;helm install <replaceable>susecf-scf</replaceable> suse/cf \
--namespace <replaceable>scf</replaceable> \
--values scf-config-values.yaml \
--values scf-sizing.yaml
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-cap-availability-zones">
  <title>Availability Zones</title>

  <para>
   Availability Zones (AZ) are logical arrangements of compute nodes within a
   region that provide isolation from each other. A deployment that is
   distributed across multiple AZs can use this separation to increase
   resiliency against downtime in the event a given zone experiences issues.
  </para>
  <para>
   Refer to the following for platform-specific information about availability
   zones:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="https://azure.microsoft.com/en-ca/updates/availability-zones-az-support-for-aks/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://cloud.google.com/compute/docs/regions-zones/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://docs.cloudfoundry.org/concepts/high-availability.html#azs"/>
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-cap-availability-zone-handling">
   <title>Availability Zone Information Handling</title>
   <para>
    In &cap;, availability zone handling is done using the
    <literal>AZ_LABEL_NAME</literal> &helm; chart value. By default,
    <literal>AZ_LABEL_NAME</literal> is set to
    <literal>failure-domain.beta.kubernetes.io/zone</literal>, which is the
    predefined &kube; label for availability zones. On most public cloud
    providers, nodes will already have this label set and availability zone
    support will work without further configuration. For on-premise
    installations, it is recommended that nodes are labeled with the same label.
   </para>
   <para>
    Run the following to see the labels on your nodes.
   </para>
<screen>&prompt.user;kubectl get nodes --show-labels</screen>
   <para>
    To label a node, use <command>kubectl label nodes </command>. For example:
   </para>
<screen>&prompt.user;kubectl label nodes <replaceable>cap-worker-1</replaceable> <replaceable>failure-domain.beta.kubernetes.io/zone=zone-1</replaceable></screen>
   <para>
    To see which node and availability zone a given
    <literal>diego-cell</literal> pod is assigned to, refer to the following
    example:
   </para>
<screen>&prompt.user;kubectl logs <replaceable>diego-cell-0</replaceable> --namespace scf | grep ^AZ</screen>
   <para>
    For more information on the
    <literal>failure-domain.beta.kubernetes.io/zone</literal> label, see
    <link xlink:href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone"/>.
   </para>
   <para>
    Note that due to a bug in &cap; 1.4 and earlier, this label was not working for <literal>AZ_LABEL_NAME</literal>.
   </para>
   <important>
    <title>Performance with Availability Zones</title>
    <para>
     For the best performance, all availability zones should have a similar
     number of nodes because app instances will be evenly distributed, so that
     each zone has about the same number of instances.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
