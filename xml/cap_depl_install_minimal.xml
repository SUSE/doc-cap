<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal &productname; Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     You can set up a minimal &productname; deployment on a laptop or workstation 
     for testing, using a hypervisor such as KVM or VirtualBox, provided your 
     computer has sufficient memory, storage, and CPU. This is an extremely 
     minimal deployment that uses your hypervisor's name services, 
     and &kube;'s hostpath storage type. (Of course, you may use separate name 
     and storage servers.)
 </para>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any quad-core AMD64/Intel EM64T processor, 8GB of memory per node, and 
        500GB of storage. 32GB of memory will run a four-node deployment, 
        though more is better. Allocate a minimum of 60GB of storage for each of 
        the &kube; nodes, and 40GB is adequate for the admin node.  
        </para>
      </listitem>  
      </varlistentry>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2
	  </term>
	<listitem>
        <para>
            After installing &caasp; and logging into the Velum Web interface, 
            check the box to install Tiller (Helm's server component). 
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install Tiller</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>        
	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, one 
        Kubernetes master, and two Kubernetes minions. (See the 
        <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">CaaSP 
        documentation</link>). You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        minions will each download ~10GB of Docker images.
        </para>
      <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>. When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
              Bootstrapping the Cluster</link> you can proceed to 
          the next steps to install &productname;.
      </para>
	</listitem>
	</varlistentry>
 <varlistentry>
	  <term>
          Install &productname; 
      </term>
      <listitem>
 <para>
     Installing &suse; &cap; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will use Helm to install the required Kubernetes 
     applications to set up &cap;.
 </para>

  </listitem>
 </varlistentry>
</variablelist>
</sect1>
 
<sect1  xml:id="sec.cap.install">
    <title>Initiate Helm </title>
    <para>
        You should have already installed Tiller, Helm's server component, from 
        &suse; &caasp;'s Velum dashboard. Now you must initiate Helm, the &kube;
        package manager, on your &kube; master. You cannot log into the &kube; 
        nodes directly, but must first open an SSH session on your &caasp; admin 
        node, using the root login you created when you installed &caasp;, and 
        then open an SSH session to your &kube; master from your admin node.
    </para>
    <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        You can set up direct logins to your &kube; nodes from your host PC by 
        copying the SSH keys from your admin node to your &kube; nodes, and then 
        you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier, especially if you want to use mouse 
        select/middle-click paste to copy commands and output from a virtual 
        terminal on your host PC.
    </para>
</tip>
<para>
    Use this command to initialize Helm on your &kube; master:
</para>
    <screen>
&prompt.root;helm init --client only
</screen>  
</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>
 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
<prompt>root # </prompt>systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", \
"metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -
</screen>
<para>
    Verify that your new storage class has been created:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link>
    for detailed information on storage classes.
</para>
</sect1> 
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a new configuration file for Helm to use. In this example it is
        called <filename>scf-config-values.yaml</filename>, and it is stored in 
        the <filename>/root/deploy</filename> directory. &caasp; has a read-only
        filesystem; <filename>/root/deploy</filename> is outside of this so
        you can use it like a normal read-write directory.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your 
    # User Account and Authentication (UAA) server,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The external IP address of your &kube; master
    external_ip: <replaceable>IP_ADDRESS</replaceable>
    
    # Run <command>kubectl get storageclasses</command>
    # to view your available storage classes
    storage_class:
    persistent: "<replaceable>hostpath</replaceable>"
        shared: "shared"
        
    # The registry the images will be fetched from. 
    # The values below should work for
    # a default installation from the &suse; registry.
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap-beta"

    # Required for CaaSP 2
    auth: rbac        
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with Helm</title>
 <para>
Run the following Helm commands to complete the deployment. There are five steps:
</para>
<itemizedlist>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        Copy storage secret to UAA and &suse; &cf; (SCF) namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA Certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<para>
   ==== TODO reorg the Helm commands ====
</para>

<sect2 xml:id="sec.cap.uaa-deploy">
    <title>title</title>
<para>
 deploy the CloudFoundry User Account and Authentication (UAA) server:
</para>
<screen>
&prompt.root;kubectl create namespace uaa
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -"
 
&prompt.root;helm install helm/uaa \
    --namespace uaa \
    --values scf-config-values.yaml
</screen>
</sect2>   

<sect2 xml:id="sec.cap.scf-deploy">
    <title>title2</title>
    <para>
        werd
    </para>
<screen>
&prompt.root;kubectl create namespace scf
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -


&prompt.root;helm install helm/cf \
    --namespace scf \
    --values certs/scf-cert-values.yaml \
    --values scf-config-values.yaml 
    </screen>
<para>
    Now sit back and wait for everything to be ready. Run this command to watch
    the pods come online:
</para>
    <screen>
&prompt.root;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
You should see output similar to this (which is shortened for this example):
</para>
<screen>
NAMESPACE     NAME                           READY  STATUS             RESTARTS AGE
kube-system   kube-dns-2830115714-tdl38      3/3    Completed          18       1m
kube-system   tiller-deploy-833494864-33ndq  0/1    ContainerCreating  0        1m
scf           api-0                          0/1    ContainerCreating  0        1m
scf           api-worker-3486565911-dptbh    0/1    ContainerCreating  0        1m
scf           blobstore-0                    0/1    Pending            0        1m
</screen>
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos on your &kube; master with 
        Helm. First add the Stratos repository:
    </para>        
        <screen>
&prompt.root;helm repo add stratos-ui https://suse.github.io/stratos-ui
</screen>
    <para>
        Change to the directory where your <filename>scf-config-values.yaml</filename>
        is stored, and run these commands:
    </para>
        <screen>
&prompt.root;helm install stratos-ui/console \
    --namespace stratos \
    --values scf-config-values.yaml 
</screen>
<para>
    Point a Web browser to your domain at port 8443, for example 
    https://host:&exampledomain;:8443, and log in with the &cap; 
    credentials you created in <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure> 


</sect1>
</chapter>