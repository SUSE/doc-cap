<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-production"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Production Installation with Optional &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>    
 A basic &productname; production deployment with &ha; requires at least eight 
 nodes plus a storage cluster: one &suse; &caasp; admin node, three &kube; 
 masters, three &kube; workers, a DNS server, and a storage cluster such as 
 &ses;. This is a bare minimum, and actual requirements are likely to be much 
 larger, depending on your workloads. You also need an external workstation for 
 administering your cluster.
</para>
 <note>
        <title>Remote Administration</title>
        <para>
        You will run most of the commands in this chapter from a remote
        workstation, rather than directly on any of the &susecf; nodes. These are
        indicated by the unprivileged user Tux, while root prompts are on a
        cluster node. There are few tasks that need to be performed directly on
        any of the nodes.
    </para>
</note>
 <para>
 The optional &ha; example in this chapter provides HA only for the &cap; 
 cluster, and not for &caasp; or &ses;. You will modify your configuration file
  (see <xref linkend="sec.cap.configure-prod"/>) to implement HA; see 
  <xref linkend="sec.cap.ha-prod"/> for an example HA configuration.
 </para>
 
<sect1 xml:id="sec.cap.prereqs-prod">
  <title>Prerequisites</title>
  <para>
      Calculating hardware requirements is best done with an analysis of your 
      expected workloads, traffic patterns, storage needs, and &kube; 
      application requirements. The following examples are minimums.
  </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Minimum Hardware Requirements
	  </term>
	<listitem>
        <para>
          8GB of memory per &caasp; dashboard and &kube; master nodes.
        </para>
        <para>
          16GB of memory per &kube; worker.
      </para>
      <para>
          40GB disk space per &caasp; dashboard and &kube; master nodes.
      </para>
      <para>
         60GB disk space per &kube; worker.
     </para>
       </listitem>  
      </varlistentry>
      <varlistentry>
          <term>Domain and Network</term>
          <listitem>
          <para>
         Your cluster needs its own domain and network. Each node should resolve
         to its hostname, and to its fully-qualified domain name. Typically, a
         &kube; cluster sits behind a load balancer, which also provides external 
         access to the cluster. Another option is to use DNS round-robin to the 
         &kube; workers to provide external access. This guide does not describe
         how to set up a load balancer or name services, as these depend on 
         customer requirements and existing network architectures.
     </para>
      </listitem>  
      </varlistentry>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2
	  </term>
	<listitem>
        <para>
            After installing <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link> and logging into the Velum Web interface, 
            check the box to install Tiller (&helm;'s server component). 
        </para>
<figure xml:id="fig.cap.install-tiller-prod">
   <title>Install Tiller</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the networks that are exclusive to the internal cluster
      communications. They are not externally accessible, and you must avoid
      address collisions.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
        <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with AutoYaST</link>. Set up &caasp; with one admin 
          node and at least three Kubernetes masters
        and three Kubernetes workers. You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        workers will each download ~10GB of Docker images.
        </para>
  <figure xml:id="fig.cap.select-master-nodes">
   <title>Assigning Roles to Nodes</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="select-master-nodes.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
              Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file. This
takes you to a login screen; use the login you created to access Velum. Save the
file as <filename>~/.kube/config</filename> on your workstation. This 
file enables the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download-prod">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download-prod.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
  <term>Install kubectl</term>          
  <listitem>
 <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your workstation. After installation, run this command to verify
     that it is installed, and that is communicating correctly with your cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                  STATUS                     ROLES     AGE  VERSION
b70748d.example.com   Ready                      &lt;none>    4h   v1.7.7
cb77881.example.com   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
d028551.example.com   Ready                      &lt;none>    4h   v1.7.7
[...]
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Installing &susecf; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will use the &helm; client on your remote workstation 
     to install the required Kubernetes applications to set up &susecf;, and to 
     administer your cluster remotely.
 </para>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the &caasp; installation. You do not want two &tiller;
            instances. Also make sure to install the &helm; version that matches
            your &tiller; version.
    </para>
</warning>
 <para>
     See the 
     <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
     usage examples. You should match the &helm; version with the version of &tiller; 
     that is running on your cluster. To get your &tiller; version, log into your
     &kube; master and query the logs.
 </para>
 <screen>
&prompt.root;kubectl logs -l name=tiller --namespace=kube-system | grep "Starting &tiller;"
[main] 2018/01/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
</screen>
     <note>
        <title>Logging in to &kube; Nodes</title>
        <para>
        By default, &suse; &caasp; allows logging into the &kube; nodes only from
        the admin node.
        </para>
</note>
<para>
    Copy the matching &helm; binary into any directory that is in your PATH
    on your workstation, such as your <filename>~/bin</filename> directory. Then 
    initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>
</sect1>  

<sect1 xml:id="sec.cap.storageclass-prod">
 <title>Choose Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. Your available storage classes depend on which
     storage cluster you are using 
     (&ses; users, see <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/integration.html">&suse; &caasp; Integration with SES</link>). 
     After connecting your storage backend use <command>kubectl</command>
     to see your available storage classes:
 </para>
 <screen>
&prompt.user;kubectl get storageclasses
</screen>
<para>
    See <xref linkend="sec.cap.configure-prod"/> to learn where to configure your 
    storage class for &susecf;. See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link>
    for detailed information on storage classes.
</para>
</sect1>
    

<sect1 xml:id="sec.cap.configure-prod">
    <title>Configuring the Production Deployment</title>
    <para>
        Create a new configuration file on your workstation for &helm; to use. 
        In this example it is
        called <filename>scf-config-values.yaml</filename>.
    </para>
        <screen>
env:
    # Create a password for your &susecf; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>changeme</replaceable>
    
    # Enter the domain you created for your cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your UAA host
    # This is the external access to the cluster
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>changeme</replaceable>

kube:
    # The external IP address of the &kube; cluster
    external_ip: <replaceable>&kubeip;</replaceable>
    
    # Your chosen storage class
    storage_class:
        persistent: <replaceable>"persistent"</replaceable>
        shared: "shared"
    
    # The Docker registry the images will be fetched from
    # This is the default
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap"

    # Required for CaaSP 2
    auth: rbac
    
    # example high availability configuration
    # see <xref linkend="sec.cap.ha-prod"/> for 
    # an example HA configuration
</screen>        
</sect1>

<sect1  xml:id="sec.cap.helm-deploy-prod">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the SUSE Kubernetes charts repository 
    </para>
</listitem>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        Copy the storage secret of your storage cluster to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.addrepo-prod">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse
NAME         VERSION   DESCRIPTION                                  
suse/cf      2.6.1-rc1 A Helm chart for SUSE Cloud Foundry          
suse/console 1.0.0     A Helm chart for deploying Stratos UI Console
suse/uaa     2.6.1-rc1 A Helm chart for SUSE UAA   
</screen>
</sect2>

<sect2 xml:id="sec.cap.create-namespaces-prod">
    <title>Create Namespaces</title>
<para>
    Create the UAA (User Account and Authentication) and 
    SCF (SUSE Cloud Foundry) namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret-prod">
    <title>Copy &ses; Secret</title>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa-prod">
    <title>Install UAA</title>
<para>
    Use &helm; to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install suse/uaa --name uaa --namespace uaa --values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
When the status shows RUNNING for all of the UAA nodes, then proceed to the next
step.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf-prod">
    <title>Install SCF</title>
<para>
  First copy your UAA certificate authority (CA) into a shell variable, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;CA_CERT="$(kubectl get secret secret --namespace uaa -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
&prompt.user;helm install -f scf-config-values.yaml --namespace scf suse/cf --set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
For getting information on how to access your SCF deployment you can always query the helm release:
</para>
   <screen>
&prompt.user;helm status $(helm list | awk '/cf-([0-9]).([0-9]).*/{print$1}') | sed -n -e '/NOTES/,$p'
NOTES:
    Welcome to your new deployment of SCF.

    The endpoint for use by the `cf` client is
        https://api.example.com

    To target this endpoint run
        cf api --skip-ssl-validation https://api.example.com

    Your administrative credentials are:
        Username: admin
        Password: changeme

    Please remember, it may take some time for everything to come online.

    You can use
        kubectl get pods --namespace scf

    to spot-check if everything is up and running, or
        watch -c 'kubectl get pods --namespace scf'

    to monitor continuously.

</screen>
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos-prod">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos with 
        &helm; after all of the UAA and SCF pods are running. Start by preparing 
        the environment:
    </para>
    <screen>        
&prompt.user;kubectl create namespace stratos
</screen>
<para>
If you are using &ses; as your storage backend, copy the secret into the Stratos
namespace. Skip this step if you are using the hostpath storage type.
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "stratos"/' | \
kubectl create -f -       
</screen>        

<para>
If your storage class is not set as default, you will also need to add the argument to <command>helm install</command> later on:
</para>
<screen>
--set storageClass=&lt;STORAGE_CLASS_NAME>
</screen>
<para>
You can verify your storage class with:
</para>
<screen>
&prompt.user;kubectl get storageclasses
NAME                   TYPE
persistent (default)   kubernetes.io/rbd
</screen>
<para>       
You should already have the Stratos charts when you downloaded the SUSE charts
repository:
</para>        
<screen>
&prompt.user;helm search suse
NAME             VERSION   DESCRIPTION                                  
suse/cf      2.6.1-rc1 A Helm chart for SUSE Cloud Foundry          
suse/console 1.0.0     A Helm chart for deploying Stratos UI Console
suse/uaa     2.6.1-rc1 A Helm chart for SUSE UAA 
</screen>
<para>
Install Stratos:
</para>
<screen>
&prompt.user;helm install suse/console \
    --namespace stratos \
    --values scf-config-values.yaml
</screen>   
<para>
Use &helm; to check installation status:
</para>
<screen>
&prompt.user;helm status <replaceable>console</replaceable>    
</screen>
<para>
    When it reaches the DEPLOYED state, get its IP address and port number:
</para>
<screen>
&prompt.user;helm status <replaceable>console</replaceable> | grep ui-ext
console-ui-ext 10.0.0.162 192.168.77.1  80:30933/TCP,443:30941/TCP  1m 
</screen>
<para>
    In this example, pointing your web browser to https://192.168.77.1:30941 
    opens the console. If you see a certificate warning you may safely
    ignore it. Log in with the &cap; credentials you created in 
    <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos-prod">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure> 
</sect1>

 <sect1 xml:id="sec.cap.update">
  <title>Updating &suse; &cf;, UAA, and Stratos</title>
  <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. Check for updates:
  </para>
<screen>
&prompt.user;helm repo update
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "suse" chart repository
Update Complete. ⎈ Happy Helming!⎈   
</screen>      
<para>
      Get your release and chart names, and then apply the updates:
</para>
  <screen>
&prompt.user;helm list
NAME             REVISION UPDATED                 STATUS   CHART         NAMESPACE
bunking-manta    1        Tue Jan 9 08:29:57 2018 DEPLOYED uaa-2.0.2     uaa
irreverant-sloth 1        Tue Jan 9 09:36:29 2018 DEPLOYED cf-2.0.2      scf        
olfactory-turtle 1        Tue Jan 9 10:09:56 2018 DEPLOYED console-1.0.0 stratos

&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/             

&prompt.user;helm search suse
NAME              VERSION     DESCRIPTION                                  
suse/cf       2.6.1-rc1   A Helm chart for SUSE Cloud Foundry          
suse/console  1.0.0       A Helm chart for deploying Stratos UI Console
suse/uaa      2.6.1-rc1   A Helm chart for SUSE UAA

&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/uaa
&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/cf
&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/console
</screen>
<para>
    In the above example, the <replaceable>release-names</replaceable> are found
    with the <command>helm list</command>, bunking-manta, irreverant-sloth, and
    olfactory-turtle. Your release names will be different.
</para>

<!-- helm rollback does not work, so upgrades are one-way for now
-->
</sect1>
<sect1 xml:id="sec.cap.ha-prod">
  <title>Example &ha; Configuration</title>
  <para>
This example HA configuration goes in your cluster configuration file (see
<xref linkend="sec.cap.configure-prod"/>).
</para>
<screen>
sizing:
  api:
    count: 2
  cf_usb:
    count: 2
  consul:
    count: 3
  diego_access:
    count: 2
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  doppler:
    count: 2
  etcd:
    count: 3
  loggregator:
    count: 2
  mysql:
    count: 3
  nats:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2 
</screen>

<para>
    The HA pods with the following roles will enter in both passive and ready 
    states; there should always be at least one pod in each role that is ready.
</para>
    <itemizedlist>
	 <listitem>
         <para>
     diego-brain
     </para>
     </listitem>
     <listitem>
         <para>
     diego-database
     </para>
     </listitem>
     <listitem>
         <para>
     routing-api
     </para>
     </listitem>
    </itemizedlist>
 </sect1>
</chapter>
