<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.depl-azure"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing Microsoft Azure for &productname;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  &productname; supports deployment on Microsoft Azure Kubernetes Service
  (AKS), Microsoft's managed &kube; service. This chapter describes the steps
  for preparing Azure for a &productname; deployment, with a basic Azure load
  balancer. (See
  <link xlink:href="https://azure.microsoft.com/en-us/services/container-service/">
  Azure Kubernetes Service (AKS)</link> for more information.)
 </para>
 <para>
  In &kube; terminology a node used to be a minion, which was the name for a
  worker node. Now the correct term is simply node (see
  <link xlink:href="https://kubernetes.io/docs/concepts/architecture/nodes/"></link>).
  This can be confusing, as computing nodes have traditionally been defined as
  any device in a network that has an IP address. In Azure they are called
  agent nodes. In this chapter we call them agent nodes or &kube; nodes.
 </para>
 <sect1 xml:id="sec.cap.prereqs-azure">
  <title>Prerequisites</title>

  <para>
   Install <command>az</command>, the Azure command-line client, on your remote
   administration machine. See
   <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">
   Install Azure CLI 2.0</link> for instructions.
  </para>

  <para>
   See the
   <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/reference-index?view=azure-cli-latest">
   Azure CLI 2.0 Reference</link> for a complete <command>az</command> command
   reference.
  </para>

  <para>
   You also need the <command>kubectl</command>, <command>curl</command>,
   <command>sed</command>, and <command>jq</command> commands, &helm; 2.9 or
   newer, and the name of the SSH key that is attached to your Azure account.
   (Get &helm; from
   <link xlink:href="https://github.com/helm/helm/releases">Helm
   Releases</link>.)
  </para>

  <para>
   Log in to your Azure Account:
  </para>

<screen>&prompt.user;az login</screen>

  <para>
   Your Azure user needs the User Access Administrator role. Check your
   assigned roles with the <command>az</command> command:
  </para>

<screen>&prompt.user;az role assignment list --assignee <replaceable>login-name</replaceable>
[...]
"roleDefinitionName": "User Access Administrator",</screen>

  <para>
   If you do not have this role, then you must request it from your Azure
   administrator.
  </para>

  <para>
   You need your Azure subscription ID. Extract it with <command>az</command>:
  </para>

<screen>&prompt.user;az account show --query "{ subscription_id: id }"
{
"subscription_id": <replaceable>"a900cdi2-5983-0376-s7je-d4jdmsif84ca"</replaceable>
}</screen>

  <para>
   Replace <varname>subscription-id</varname> in the next command with your
   <varname>subscription-id</varname>. Then export it as an environment
   variable and set it as the current subscription:
  </para>

<screen>&prompt.user;export SUBSCRIPTION_ID=<replaceable>"a900cdi2-5983-0376-s7je-d4jdmsif84ca"</replaceable>

&prompt.user;az account set --subscription $SUBSCRIPTION_ID</screen>

  <para>
   Verify that the <literal>Microsoft.Network</literal>,
   <literal>Microsoft.Storage</literal>, <literal>Microsoft.Compute</literal>,
   and <literal>Microsoft.ContainerService</literal> providers are enabled:
  </para>

<screen>&prompt.user;az provider list | egrep -w 'Microsoft.Network|Microsoft.Storage|Microsoft.Compute|Microsoft.ContainerService'</screen>

  <para>
   If any of these are missing, enable them with the <command>az provider
   register --name <replaceable>provider</replaceable></command> command.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.create-aks-instance">
  <title>Create Resource Group and AKS Instance</title>

  <para>
   Now you can create a new Azure resource group and AKS instance. Set the
   required variables as environment variables, which helps to speed up the
   setup, and to reduce errors. Verify your environment variables at any time
   with <command>echo $<replaceable>VARNAME</replaceable></command>, for
   example:
  </para>

<screen>&prompt.user;echo $RGNAME
cap-aks</screen>

  <para>
   This is especially useful when you run long compound commands to extract and
   set environment variables.
  </para>

  <note>
   <title>Use different names</title>
   <para>
    It is better to use unique resource group and managed cluster names, and
    not copy the examples, especially when your Azure subscription supports
    multiple users. Azure has no tools for sorting resources by user, so
    creating unique names and putting everything in your deployment in a single
    resource group helps you keep track, and you can delete the whole
    deployment by deleting the resource group.
   </para>
  </note>

  <procedure xml:id="pro.envvars-azure">
   <step>
    <para>
     Create the resource group name:
    </para>
<screen>&prompt.user;export RGNAME="<replaceable>cap-aks</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create the AKS managed cluster name. Azure's default is to use the
     resource group name, then prepend it with <literal>MC</literal> and append
     the location, e.g. <literal>MC_cap-aks_cap-aks_eastus</literal>. This
     example uses the creator's initials for the <literal>AKSNAME</literal>
     environment variable, which will be mapped to the <command>az</command>
     command's <command>--name</command> option. The <command>--name</command>
     option is for creating arbitrary names for your AKS resources. This
     example will create a managed cluster named
     <literal>MC_cap-aks_cjs_eastus</literal>:
    </para>
<screen>&prompt.user;export AKSNAME=<replaceable>cjs</replaceable></screen>
   </step>
   <step>
    <para>
     Set the Azure location. (See
     <link xlink:href="https://docs.microsoft.com/en-us/azure/aks/container-service-quotas">
     Quotas and region availability for Azure Kubernetes Service (AKS)</link>
     for supported locations.)
    </para>
<screen>&prompt.user;export REGION="<replaceable>eastus</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the Kubernetes agent node count. (&cap; requires a minimum of 3.)
    </para>
<screen>&prompt.user;export NODECOUNT="<replaceable>3</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the virtual machine size (see
     <link xlink:href="https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs">
     Sizes for Cloud Services</link>):
    </para>
<screen>&prompt.user;export NODEVMSIZE="<replaceable>Standard_D2_v2</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the public SSH key name associated with your Azure account:
    </para>
<screen>&prompt.user;export SSHKEYVALUE="<replaceable>~/.ssh/id_rsa.pub</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create and set a new admin username:
    </para>
<screen>&prompt.user;export ADMINUSERNAME="<replaceable>scf-admin</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create a unique nodepool name. The default is
     <literal>aks-nodepool</literal> followed by an auto-generated number, e.g.
     <literal>aks-nodepool1-39318075-2</literal>. You have the option to change
     <literal>nodepool1</literal> and create your own unique identifier, for
     example, <literal>mypool</literal>:
    </para>
<screen>&prompt.user;export NODEPOOLNAME="<replaceable>mypool</replaceable>"</screen>
    <para>
     Which results in something like <literal>aks-mypool-39318075-2</literal>.
    </para>
   </step>
  </procedure>

  <para>
   Now that your environment variables are in place, create a new resource
   group:
  </para>

<screen>&prompt.user;az group create --name $RGNAME --location $REGION</screen>

  <para>
   Now you can create a new AKS managed cluster:
  </para>

<screen>&prompt.user;az aks create --resource-group $RGNAME --name $AKSNAME \
 --node-count $NODECOUNT --admin-username $ADMINUSERNAME \
 --ssh-key-value $SSHKEYVALUE --node-vm-size $NODEVMSIZE \
 --node-osdisk-size=60 --nodepool-name $NODEPOOLNAME</screen>

  <note>
   <para>
    An OS disk size of at least 60GB must be specified using the
    <command>--node-osdisk-size</command> flag.
   </para>
  </note>

  <para>
   This takes a few minutes. When it is completed, fetch your
   <command>kubectl</command> credentials. The default behavior for <command>az
   aks get-credentials</command> is to merge the new credentials with the
   existing default configuration, and to set the new credentials as as the
   current &kube; context. The context name is your AKSNAME value. You should
   first backup your current configuration, or move it to a different location,
   then fetch the new credentials:
  </para>

<screen>&prompt.user;az aks get-credentials --resource-group $RGNAME --name $AKSNAME
 Merged "cap-aks" as current context in /home/&exampleuser_plain;/.kube/config</screen>

  <para>
   Verify that you can connect to your cluster:
  </para>

<screen>&prompt.user;kubectl get nodes
NAME                    STATUS    ROLES     AGE       VERSION
aks-mypool-47788232-0   Ready     agent     5m        v1.9.6
aks-mypool-47788232-1   Ready     agent     6m        v1.9.6
aks-mypool-47788232-2   Ready     agent     6m        v1.9.6

&prompt.user;kubectl get pods --all-namespaces
NAMESPACE     NAME                                READY  STATUS    RESTARTS   AGE
kube-system   azureproxy-79c5db744-fwqcx          1/1    Running   2          6m
kube-system   heapster-55f855b47-c4mf9            2/2    Running   0          5m
kube-system   kube-dns-v20-7c556f89c5-spgbf       3/3    Running   0          6m
kube-system   kube-dns-v20-7c556f89c5-z2g7b       3/3    Running   0          6m
kube-system   kube-proxy-g9zpk                    1/1    Running   0          6m
kube-system   kube-proxy-kph4v                    1/1    Running   0          6m
kube-system   kube-proxy-xfngh                    1/1    Running   0          6m
kube-system   kube-svc-redirect-2knsj             1/1    Running   0          6m
kube-system   kube-svc-redirect-5nz2p             1/1    Running   0          6m
kube-system   kube-svc-redirect-hlh22             1/1    Running   0          6m
kube-system   kubernetes-dashboard-546686-mr9hz   1/1    Running   1          6m
kube-system   tunnelfront-595565bc78-j8msn        1/1    Running   0          6m</screen>

  <para>
   When all nodes are in a ready state and all pods are running, proceed to the
   next steps.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.tiller-service">
  <title>Create &tiller; Service Account</title>

  <para>
   You must create a &tiller; service account in order to give &tiller;
   sufficient permissions to make changes on your cluster. There are two ways
   to create a service account: from the command line with
   <command>kubectl</command>, or applying a configuration file with
   <command>kubectl</command> and <command>helm</command>.
  </para>

  <para>
   Run these <command>kubectl</command> commands to create your &tiller;
   service account:
  </para>

<screen>&prompt.user;kubectl create serviceaccount tiller --namespace kube-system
 serviceaccount "tiller" created

&prompt.user;kubectl create clusterrolebinding tiller \
--clusterrole cluster-admin \
--serviceaccount=kube-system:tiller
 clusterrolebinding.rbac.authorization.k8s.io "tiller" created

&prompt.user;helm init --upgrade --service-account tiller
 $HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.

 Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
 Happy Helming!</screen>

  <para>
   Creating your &tiller; service account with a configuration file requires
   &helm; version 2.9 or newer (see
   <link xlink:href="https://github.com/helm/helm/releases"/>
  </para>

  <para>
   First create the configuration file, which this example is
   <filename>rbac-config.yaml</filename>:
  </para>

<screen>apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system</screen>

  <para>
   Apply it to your cluster with these commands:
  </para>

<screen>&prompt.user;kubectl create -f rbac-config.yaml
&prompt.user;helm init --service-account tiller</screen>
 </sect1>
 <sect1 xml:id="sec.cap.psp">
  <title>Apply Pod Security Policies</title>

  <para>
   Role-based access control (RBAC) is enabled by default on AKS. Follow the
   instructions in <xref linkend="sec.cap.caasp-3"/> to apply pod security
   policies (PSPs) to your new AKS cluster. Also note that when you create your
   &cap; configuration file (e.g. <filename>scf-config-values.yaml</filename>),
   you must set <literal>auth: "rbac"</literal>. (See
   <xref linkend="sec.cap.cap-on-azure"/> for an example configuration file.)
  </para>

  <para>
   You may disable RBAC during cluster creation with the
   <command>--disable-rbac</command> option, and then set <literal>auth:
   none</literal>. Do not apply the PSPs. This is less secure, but may be
   useful for testing.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.swap-accounting">
  <title>Enable Swap Accounting</title>

  <para>
   Identify and set the cluster resource group, then enable kernel swap
   accounting. Swap accounting is required by &cap;, but it is not the default
   in AKS nodes. The following commands use the <command>az</command> command
   to modify the GRUB configuration on each node, and then reboot the virtual
   machines.
  </para>

  <procedure xml:id="pro.swap-account">
   <step>
<screen>&prompt.user;export MCRGNAME=$(az group list -o table | grep MC_"$RGNAME"_ | awk '{print $1}')</screen>
   </step>
   <step>
    <para>
     Replace <literal>mypool</literal> with your NODEPOOLNAME value:
    </para>
<screen>&prompt.user;VMNODES=$(az vm list --resource-group $MCRGNAME -o json | jq -r '.[] | select (.tags.poolName | contains("<replaceable>mypool</replaceable>")) | .name')</screen>
   </step>
   <step>
<screen>&prompt.user;for i in $VMNODES
 do
   az vm run-command invoke --resource-group $MCRGNAME --name $i --command-id RunShellScript \
   --scripts "sudo sed -i 's|linux.*./boot/vmlinuz-.*|&amp; swapaccount=1|' /boot/grub/grub.cfg"
done</screen>
   </step>
   <step>
    <para>
     Look for <literal>"Provisioning succeeded"</literal> messages when step 3
     has completed, then reboot your virtual machines, which takes a few
     minutes:
    </para>
<screen>&prompt.user;for i in $VMNODES; 
do 
    az vm restart --resource-group $MCRGNAME --name $i;
done
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.loadbalancer-azure">
  <title>Create a Basic Load Balancer and Public IP Address</title>

  <para>
   Azure offers two load balancers, Basic and Standard. Currently Basic is
   free, while you have to pay for Standard. (See
   <link xlink:href="https://azure.microsoft.com/en-us/services/load-balancer/">
   Load Balancer</link>.) The following steps create a Basic load balancer
   (Basic is the default.) Look for <literal>"provisioningState":
   "Succeeded"</literal> messages in the command output to verify that the
   commands succeeded.
  </para>

  <procedure xml:id="pro.loadbalancer">
   <step>
    <para>
     Create a static public IPv4 address:
    </para>
<screen>&prompt.user;az network public-ip create \
 --resource-group $MCRGNAME \
 --name $AKSNAME-public-ip \
 --allocation-method Static</screen>
   </step>
   <step>
    <para>
     Create the load balancer:
    </para>
<screen>&prompt.user;az network lb create \
 --resource-group $MCRGNAME \
 --name $AKSNAME-lb \
 --public-ip-address $AKSNAME-public-ip \
 --frontend-ip-name $AKSNAME-lb-front \
 --backend-pool-name $AKSNAME-lb-back</screen>
   </step>
   <step>
    <para>
     Set the virtual machine network interfaces, then add them to the load
     balancer:
    </para>
<screen>&prompt.user;NICNAMES=$(az network nic list --resource-group $MCRGNAME -o json | jq -r '.[].name')

&prompt.user;for i in $NICNAMES
do
    az network nic ip-config address-pool add \
    --resource-group $MCRGNAME \
    --nic-name $i \
    --ip-config-name ipconfig1 \
    --lb-name $AKSNAME-lb \
    --address-pool $AKSNAME-lb-back
done</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.security-rules">
  <title>Configure Load Balancing and Network Security Rules</title>

  <procedure xml:id="pro.netsec-rules">
   <step>
    <para>
     Set the required ports to allow access to &productname;. Port 8443 is
     optional for the Stratos Web Console.
    </para>
<screen>&prompt.user;export CAPPORTS="80 443 4443 2222 2793 8443"</screen>
   </step>
   <step>
    <para>
     Create network and load balancer rules:
    </para>
<screen>&prompt.user;for i in $CAPPORTS
do
    az network lb probe create \
    --resource-group $MCRGNAME \
    --lb-name $AKSNAME-lb \
    --name probe-$i \
    --protocol tcp \
    --port $i 
    
    az network lb rule create \
    --resource-group $MCRGNAME \
    --lb-name $AKSNAME-lb \
    --name rule-$i \
    --protocol Tcp \
    --frontend-ip-name $AKSNAME-lb-front \
    --backend-pool-name $AKSNAME-lb-back \
    --frontend-port $i \
    --backend-port $i \
    --probe probe-$i 
done</screen>
   </step>
   <step>
    <para>
     Verify port setup:
    </para>
<screen>&prompt.user;az network lb rule list --resource-group $MCRGNAME --lb-name $AKSNAME-lb|grep -i port
    
    "backendPort": 8443,
    "frontendPort": 8443,
    "backendPort": 80,
    "frontendPort": 80,
    "backendPort": 443,
    "frontendPort": 443,
    "backendPort": 4443,
    "frontendPort": 4443,
    "backendPort": 2222,
    "frontendPort": 2222,
    "backendPort": 2793,
    "frontendPort": 2793,</screen>
   </step>
   <step>
    <para>
     Set the network security group name and priority level. The priority
     levels range from 100-4096, with 100 the highest priority. Each rule must
     have a unique priority level:
    </para>
<screen>&prompt.user;NSG=$(az network nsg list --resource-group=$MCRGNAME -o json | jq -r '.[].name')
&prompt.user;PRI=200</screen>
   </step>
   <step>
    <para>
     Create the network security rule:
    </para>
<screen>&prompt.user;for i in $CAPPORTS
do
    az network nsg rule create \
    --resource-group $MCRGNAME \
    --priority $PRI \
    --nsg-name $NSG \
    --name $AKSNAME-$i \
    --direction Inbound \
    --destination-port-ranges $i \
    --access Allow
    PRI=$(expr $PRI + 10)
done</screen>
   </step>
   <step>
    <para>
     Print the public and private IP addresses for later use:
    </para>
<screen>&prompt.user;echo -e "\n Resource Group:\t$RGNAME\n \
Public IP:\t\t$(az network public-ip show --resource-group $MCRGNAME --name $AKSNAME-public-ip --query ipAddress)\n \
Private IPs:\t\t\"$(az network nic list --resource-group $MCRGNAME -o json | jq -r '.[].ipConfigurations[].privateIpAddress' | paste -s -d " " | sed -e 's/ /", "/g')\"\n"

 Resource Group:        cap-aks
 Public IP:             "&publicip;"
 Private IPs:           "10.240.0.4", "10.240.0.6", "10.240.0.5"</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.cap-on-azure">
  <title>Deploy &productname;</title>

  <para>
   Now Azure is ready, and you can deploy &productname; on it. After reviewing
   the rest of this section proceed to
   <xref linkend="sec.cap.helm-deploy-prod"/>. (Note that you will not install
   &suse; &caasp;, which provides a &kube; cluster, because AKS provides a
   managed &kube; cluster.)
  </para>

  <para>
   The following example <filename>scf-azure-values.yaml</filename> contains
   parameters particular to running &productname; on Azure Kubernetes Service.
   You need the IP addresses from the last command in
   <xref linkend="sec.cap.security-rules"/>. This is a simplified example that
   does not use Azure's DNS services. For quick testing and proof of concept,
   you can use the free wildcard DNS services,
   <link xlink:href="http://xip.io/">xip.io</link> or
   <link xlink:href="http://nip.io/">nip.io</link>. See
   <link xlink:href="https://docs.microsoft.com/en-us/azure/dns/"> Azure DNS
   Documentation</link> to learn more about Azure's name services.
  </para>

  <warning>
   <title>Do not use xip.io or nip.io on production systems</title>
   <para>
    Never use xip.io or nip.io on production systems! You must provide proper
    DNS and DHCP services on production clusters.
   </para>
  </warning>

<screen>secrets:
  # Password for user 'admin' in the cluster
  CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>

  # Password for SCF to authenticate with UAA
  UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

env:
  # Use the public IP address
  DOMAIN: <replaceable>&publicip;.xip.io</replaceable>
            
  # uaa prefix is required
  UAA_HOST: uaa.<replaceable>&publicip;.xip.io</replaceable>
  UAA_PORT: 2793
    
  #Azure deployment requires overlay
  GARDEN_ROOTFS_DRIVER: "overlay-xfs"
    
kube:
  # List the private IP addresses
  external_ips: ["<replaceable>10.240.0.5", "10.240.0.6", "10.240.0.4</replaceable>"]
  storage_class:
    # Azure supports only "default" or "managed-premium"
    persistent: "<replaceable>default</replaceable>"
    shared: "shared"
    
  registry:
    hostname: "registry.suse.com"
    username: ""
    password: ""
  organization: "cap"
  auth: "rbac"</screen>

  <para>
   When your UAA deployment has completed, test that it is operating correctly
   by running <command>curl</command> on the DNS name that you configured for
   your UAA_HOST:
  </para>

<screen>&prompt.user;curl -k https://uaa.&publicip;.xip.io:2793/.well-known/openid-configuration</screen>

  <para>
   This should return a JSON object, as this abbreviated example shows:
  </para>

<screen>{"issuer":"https://uaa.&publicip;.xip.io:2793/oauth/token",
"authorization_endpoint":"https://uaa.&publicip;.xip.io:2793
/oauth/authorize","token_endpoint":"https://uaa.&publicip;.
xip.io:2793/oauth/token"</screen>

  <para>
   Now you can complete your deployment.
  </para>
 </sect1>
</chapter>
