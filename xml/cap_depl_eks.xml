<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.eks"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deploying &productname; on Amazon EKS</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
  <important>
     &readmefirst;
 </important>
 <para>
  Starting with the 1.2 release, &productname; supports deployment on
  <link xlink:href="https://aws.amazon.com/eks/">Amazon EKS</link>, Amazon's
  managed &kube; service.
 </para>
 <sect1 xml:id="sec.cap.eks-prereqs">
  <title>Prerequisites</title>

  <para>
   You need an <link xlink:href="https://aws.amazon.com/">Amazon EKS
   account</link>. See
   <link xlink:href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html">Getting
   Started with Amazon EKS</link> for instructions on creating a &kube; cluster
   for your &productname; deployment.
  </para>

  <para>
   When you create your cluster, use node sizes that are at least a
   <literal>t2.large</literal>.
  </para>

  <para>
   See <xref linkend="sec.cap.eks-iam"/> for guidance on configuring Identity
   and Access Management (IAM) for your users.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.eks-iam">
  <title>IAM Requirements for EKS</title>

  <para>
   These IAM policies provide sufficient access to use EKS.
  </para>

  <sect2 xml:id="sec.cap.eks-unscoped">
   <title>Unscoped Operations</title>
   <para>
    Some of these permissions are very broad. They are difficult to scope
    effectively, in part because many resources are created and named
    dynamically when deploying an EKS cluster using the
    <link xlink:href="https://console.aws.amazon.com/cloudformation">
    CloudFormation</link> console. It may be helpful to enforce certain naming
    conventions, such as prefixing cluster names with
    <varname>${aws:username}</varname> for pattern-matching in
    <guimenu>Conditions</guimenu>. However, this requires special consideration
    beyond the EKS deployment guide, and should be evaluated in the broader
    context of organizational IAM policies.
   </para>
<screen>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "UnscopedOperations",
            "Effect": "Allow",
            "Action": [
                "cloudformation:CreateUploadBucket",
                "cloudformation:EstimateTemplateCost",
                "cloudformation:ListExports",
                "cloudformation:ListStacks",
                "cloudformation:ListImports",
                "cloudformation:DescribeAccountLimits",
                "eks:ListClusters",
                "cloudformation:ValidateTemplate",
                "cloudformation:GetTemplateSummary",
                "eks:CreateCluster"
            ],
            "Resource": "*"
        },
        {
            "Sid": "EffectivelyUnscopedOperations",
            "Effect": "Allow",
            "Action": [
                "iam:CreateInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:GetRole",
                "iam:DetachRolePolicy",
                "iam:RemoveRoleFromInstanceProfile",
                "cloudformation:*",
                "iam:CreateRole",
                "iam:DeleteRole",
                "eks:*"
            ],
            "Resource": [
                "arn:aws:eks:*:*:cluster/*",
                "arn:aws:cloudformation:*:*:stack/*/*",
                "arn:aws:cloudformation:*:*:stackset/*:*",
                "arn:aws:iam::*:instance-profile/*",
                "arn:aws:iam::*:role/*"
            ]
        }
    ]
}</screen>
  </sect2>

  <sect2 xml:id="sec.cap.eks-scoped">
   <title>Scoped Operations</title>
   <para>
    These policies deal with sensitive access controls, such as passing roles
    and attaching/detaching policies from roles.
   </para>
   <para>
    This policy, as written, allows unrestricted use of only customer-managed
    policies, and not Amazon-managed policies. This prevents potential security
    holes such as attaching the <literal>IAMFullAccess</literal> policy to a
    role. If you are using roles in a way that would be undermined by this, you
    should strongly consider integrating a
    <link xlink:href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">Permissions
    Boundary</link> before using this policy.
   </para>
<screen>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "UseCustomPoliciesWithCustomRoles",
            "Effect": "Allow",
            "Action": [
                "iam:DetachRolePolicy",
                "iam:AttachRolePolicy"
            ],
            "Resource": [
                "arn:aws:iam::&lt;YOUR_ACCOUNT_ID>:role/*",
                "arn:aws:iam::&lt;YOUR_ACCOUNT_ID>:policy/*"
            ],
            "Condition": {
                "ForAllValues:ArnNotLike": {
                    "iam:PolicyARN": "arn:aws:iam::aws:policy/*"
                }
            }
        },
        {
            "Sid": "AllowPassingRoles",
            "Effect": "Allow",
            "Action": "iam:PassRole",
            "Resource": "arn:aws:iam::&lt;YOUR_ACCOUNT_ID>:role/*"
        },
        {
            "Sid": "AddCustomRolesToInstanceProfiles",
            "Effect": "Allow",
            "Action": "iam:AddRoleToInstanceProfile",
            "Resource": "arn:aws:iam::&lt;YOUR_ACCOUNT_ID>:instance-profile/*"
        },
        {
            "Sid": "AssumeServiceRoleForEKS",
            "Effect": "Allow",
            "Action": "sts:AssumeRole",
            "Resource": "arn:aws:iam::&lt;YOUR_ACCOUNT_ID>:role/&lt;EKS_SERVICE_ROLE_NAME>"
        },
        {
            "Sid": "DenyUsingAmazonManagedPoliciesUnlessNeededForEKS",
            "Effect": "Deny",
            "Action": "iam:*",
            "Resource": "arn:aws:iam::aws:policy/*",
            "Condition": {
                "ArnNotEquals": {
                    "iam:PolicyARN": [
                        "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
                        "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
                        "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
                    ]
                }
            }
        },
        {
            "Sid": "AllowAttachingSpecificAmazonManagedPoliciesForEKS",
            "Effect": "Allow",
            "Action": [
                "iam:DetachRolePolicy",
                "iam:AttachRolePolicy"
            ],
            "Resource": "*",
            "Condition": {
                "ArnEquals": {
                    "iam:PolicyARN": [
                        "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
                        "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
                        "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
                    ]
                }
            }
        }
    ]
}</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.cap.eks-diskspace">
  <title>Disk Space</title>

  <para>
   Usually EC2 nodes come with 20GB of disk space. This is insufficient for a
   &productname; deployment. Make sure to increase the disk space for each node
   after creating the &kube; cluster. First enable SSH to the nodes:
  </para>

  <procedure xml:id="pro.ssh">
   <step>
    <para>
     Go to Services -> EC2
    </para>
   </step>
   <step>
    <para>
     Select Security Groups
    </para>
   </step>
   <step>
    <para>
     Click on the Node Security Group for your nodes
    </para>
   </step>
   <step>
    <para>
     Click Actions -> Edit inbound rules
    </para>
   </step>
   <step>
    <para>
     Click Add Rule
    </para>
   </step>
   <step>
    <para>
     Choose "SSH" in the Type column and "Anywhere" in the Source column
    </para>
   </step>
   <step>
    <para>
     Click "Save"
    </para>
   </step>
  </procedure>

  <para>
   Next, grow the volumes in AWS:
  </para>

  <procedure xml:id="pro.grow-volumes">
   <step>
    <para>
     Go to Services -> EC2
    </para>
   </step>
   <step>
    <para>
     Select Volumes
    </para>
   </step>
   <step>
    <para>
     Increase the size for all volumes attached to your nodes from 20GB to at
     least 60GB.
    </para>
   </step>
  </procedure>

  <para>
   Make the growth available to the filesystems:
  </para>

  <procedure xml:id="pro.attach-filesystems">
   <step>
    <para>
     Log into each node and run <command>sudo growpart /dev/nvme0n1 1
     &amp;&amp; sudo xfs_growfs -d /</command>
    </para>
    <para>
     These commands might change, depending on the actual disk device and the
     filesystem in use. Details on resizing are documented at
     <link xlink:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html">
     Extending a Linux File System after Resizing the Volume </link>
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.eks-helm">
  <title>The &helm; CLI and &tiller;</title>

  <para>
   Use this version of &helm;, or newer:
   <link xlink:href="https://storage.googleapis.com/kubernetes-helm/helm-v2.9.0-rc4-linux-amd64.tar.gz"/>
  </para>

  <para>
   In <filename>rbac-config.yaml</filename>:
  </para>

<screen>apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system</screen>

  <para>
   Apply it to your cluster with these commands:
  </para>

<screen>&prompt.user;kubectl create -f rbac-config.yaml
&prompt.user;helm init --service-account tiller</screen>
 </sect1>
 <sect1 xml:id="sec.cap.eks-storage-class">
  <title>Default Storage Class</title>

  <para>
   This example creates a simple storage class for your cluster in
   <filename>storage-class.yaml</filename>:
  </para>

<screen>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp2
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  labels:
    kubernetes.io/cluster-service: "true"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2</screen>

  <para>
   Then apply the new storage class configuration with this command:
  </para>

<screen>&prompt.user;kubectl create -f storage-class.yaml</screen>
 </sect1>
 <sect1 xml:id="sec.cap.eks-secgroup-rules">
  <title>Security Group rules</title>

  <para>
   In your EC2 virtual machine list, add the following rules to the security
   group to any one of your nodes:
  </para>

<screen>Type		   Protocol     Port Range      Source          Description
HTTP               TCP          80              0.0.0.0/0       CAP HTTP
Custom TCP Rule    TCP          2793            0.0.0.0/0       CAP UAA
Custom TCP Rule    TCP          2222            0.0.0.0/0       CAP SSH
Custom TCP Rule    TCP          4443            0.0.0.0/0       CAP WSS
Custom TCP Rule    TCP          443             0.0.0.0/0       CAP HTTPS
Custom TCP Rule    TCP          20000-20009     0.0.0.0/0       TCP Routing</screen>
 </sect1>
 <sect1 xml:id="sec.cap.eks-external-ips">
  <title>Find your <literal>kube.external_ips</literal></title>

  <para>
   In your EC2 VM List, look up the private IP addresses of one of the nodes,
   and note the address that is used in its private DNS, which looks like
   <literal>ip-<replaceable>address</replaceable>.us-west-2.compute.internal</literal>.
   Also note the public IP address, which you will need for the DOMAIN of the
   cluster in your &productname; configuration file.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.eks-deployment">
  <title>Configuring and Deploying &productname;</title>

  <para>
   Follow the instructions in <xref linkend="sec.cap.storageclass-prod"/> to
   deploy &productname;. When you create your
   <filename>scf-config-values.yaml</filename> file, make the following
   changes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Use <literal>overlay-xfs</literal> for
     <literal>env.GARDEN_ROOTFS_DRIVER</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     use <literal>""</literal> for
     <literal>env.GARDEN_APPARMOR_PROFILE</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     Set <literal>kube.storage_class.persistent</literal> and
     <literal>kube.storage_class.shared</literal> to <literal>gp2</literal>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following roles require <literal>SYS_RESOURCE</literal> capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     cc_uploader
    </para>
   </listitem>
   <listitem>
    <para>
     diego_api
    </para>
   </listitem>
   <listitem>
    <para>
     diego_brain
    </para>
   </listitem>
   <listitem>
    <para>
     diego_ssh
    </para>
   </listitem>
   <listitem>
    <para>
     nats
    </para>
   </listitem>
   <listitem>
    <para>
     router
    </para>
   </listitem>
   <listitem>
    <para>
     routing_api
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Use this example <filename>scf-config-values.yaml</filename> as a template
   for your configuration.
  </para>

<screen>env:
  # Domain for SCF. DNS for *.DOMAIN must point to a kube node's (not master)
  # external IP address.
  DOMAIN: <replaceable>public_ip_of_node-VM.example.com</replaceable>

  #### The UAA hostname is hardcoded to uaa.$DOMAIN, so shouldn't be
  #### specified when deploying
  # UAA host/port that SCF will talk to. If you have a custom UAA
  # provide its host and port here.
  UAA_HOST: uaa.<replaceable>public_ip_of_node-VM.example.com</replaceable>
  UAA_PORT: 2793
 
  GARDEN_ROOTFS_DRIVER: overlay-xfs
  GARDEN_APPARMOR_PROFILE: ""

sizing:
  cc_uploader:
    capabilities: ["SYS_RESOURCE"]
  diego_api:
    capabilities: ["SYS_RESOURCE"]
  diego_brain:
    capabilities: ["SYS_RESOURCE"]
  diego_ssh:
    capabilities: ["SYS_RESOURCE"]
  nats:
    capabilities: ["SYS_RESOURCE"]
  router:
    capabilities: ["SYS_RESOURCE"]
  routing_api:
    capabilities: ["SYS_RESOURCE"]

kube:
  # The IP address assigned to the kube node pointed to by the domain.
  #### the external_ip setting changed to accept a list of IPs, and was
  #### renamed to external_ips
  external_ips:
  - <replaceable>private_ip_of_node-VM</replaceable>

  storage_class:
    # Make sure to change the value in here to whatever storage class you use
    persistent: "gp2"
    shared: "gp2"

  # The registry the images will be fetched from. The values below should work for
  # a default installation from the suse registry.
  registry:
    hostname: "registry.suse.com"
    username: ""
    password: ""
  organization: "cap"
secrets:
  # Password for user 'admin' in the cluster
  CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>

  # Password for SCF to authenticate with UAA
  UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable></screen>
 </sect1>
 <sect1 xml:id="sec.cap.aws-service-broker">
  <title>Deploying and Using the AWS Service Broker</title>

  <para>
   The <link xlink:href="https://aws.amazon.com/partners/servicebroker/">AWS
   Service Broker</link> provides integration of native AWS services with
   &suse; &cap;.
  </para>

  <sect2 xml:id="sec.cap.aws-service-broker-prereqs">
   <title>Prerequisites</title>
   <para>
    Deploying and using the AWS Service Broker requires the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &cap; has been successfully deployed on Amazon EKS as described earlier
      in this chapter (see <xref linkend="cha.cap.eks"/>)
     </para>
    </listitem>
    <listitem>
     <para>
      The <link xlink:href="https://aws.amazon.com/cli/">AWS Command Line
      Interface (CLI)</link>
     </para>
    </listitem>
    <listitem>
     <para>
      The
      <link xlink:href="https://www.openssl.org/docs/man1.0.2/apps/openssl.html">OpenSSL
      command line tool</link>
     </para>
    </listitem>
    <listitem>
     <para>
      Ensure the user or role running the broker has an IAM policy set as
      specified in the
      <link xlink:href="https://github.com/awslabs/aws-servicebroker/blob/master/docs/install_prereqs.md#iam">IAM
      section of the AWS Service Broker</link> documentation on Github
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec.cap.aws-service-broker-setup">
   <title>Setup</title>
   <procedure>
    <step>
     <para>
      Create the required DynamoDB table where the AWS service broker will
      store its data. This example creates a table named
      <literal>awssb</literal>:
     </para>
<screen>&prompt.user;aws dynamodb create-table \
		--attribute-definitions \
			AttributeName=id,AttributeType=S \
			AttributeName=userid,AttributeType=S \
			AttributeName=type,AttributeType=S \
		--key-schema \
			AttributeName=id,KeyType=HASH \
			AttributeName=userid,KeyType=RANGE \
		--global-secondary-indexes \
			'IndexName=type-userid-index,KeySchema=[{AttributeName=type,KeyType=HASH},{AttributeName=userid,KeyType=RANGE}],Projection={ProjectionType=INCLUDE,NonKeyAttributes=[id,userid,type,locked]},ProvisionedThroughput={ReadCapacityUnits=5,WriteCapacityUnits=5}' \
		--provisioned-throughput \
			ReadCapacityUnits=5,WriteCapacityUnits=5 \
		--region ${AWS_REGION} --table-name <replaceable>awssb</replaceable>
</screen>
    </step>
    <step>
     <para>
      Wait until the table has been created. When it is ready, the
      <literal>TableStatus</literal> will change to <literal>ACTIVE</literal>.
      Check the status using the <command>describe-table</command> command:
     </para>
<screen>aws dynamodb describe-table --table-name <replaceable>awssb</replaceable></screen>
     <para>
      (For more information about the <command>describe-table</command>
      command, see
      <link xlink:href="https://docs.aws.amazon.com/cli/latest/reference/dynamodb/describe-table.html"/>.)
     </para>
    </step>
    <step>
     <para>
      Set a name for the &kube; namespace you will install the service broker
      to. This name will also be used in the service broker URL:
     </para>
<screen>&prompt.user;BROKER_NAMESPACE=aws-sb</screen>
    </step>
    <step>
     <para>
      Create a server certificate for the service broker:
     </para>
     <substeps>
      <step>
       <para>
        Create and use a separate directory to avoid conflicts with other CA
        files:
       </para>
<screen>&prompt.user;mkdir /tmp/aws-service-broker-certificates &amp;&amp; cd $_</screen>
      </step>
      <step>
       <para>
        Get the CA certificate:
       </para>
<screen>kubectl get secret --namespace scf --output jsonpath='{.items[*].data.internal-ca-cert}' | base64 -di > ca.pem</screen>
      </step>
      <step>
       <para>
        Get the CA private key:
       </para>
<screen>kubectl get secret --namespace scf --output jsonpath='{.items[*].data.internal-ca-cert-key}' | base64 -di > ca.key</screen>
      </step>
      <step>
       <para>
        Create a signing request:
       </para>
<screen>&prompt.user;openssl req -newkey rsa:4096 -keyout tls.key.encrypted -out tls.req -days 365 \
  -passout pass:<replaceable>1234</replaceable> \
  -subj '/CN=aws-servicebroker.${BROKER_NAMESPACE}' -batch \
  &lt;/dev/null
</screen>
      </step>
      <step>
       <para>
        Decrypt the generated broker private key:
       </para>
<screen>&prompt.user;openssl rsa -in tls.key.encrypted -passin pass:<replaceable>1234</replaceable> -out tls.key</screen>
      </step>
      <step>
       <para>
        Sign the request with the CA certificate:
       </para>
<screen>&prompt.user;openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in tls.req -out tls.pem</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Install the AWS service broker as documented at
      <link xlink:href="https://github.com/awslabs/aws-servicebroker/blob/master/docs/getting-started-k8s.md"/>.
      Skip the installation of the Kubernetes Service Catalog. While installing
      the AWS Service Broker, make sure to update the &helm; chart version (the
      version as of this writing is <literal>1.0.0-beta.3</literal>). For the
      broker install, pass in a value indicating the Cluster Service Broker
      should not be installed (for example <command>--set
      deployClusterServiceBroker=false</command>). Ensure an account and role
      with adequate IAM rights is chosen (see
      <xref linkend="sec.cap.aws-service-broker-prereqs"/>:
     </para>
<screen>&prompt.user;helm install aws-sb/aws-servicebroker \
	     --name aws-servicebroker \
	     --namespace ${BROKER_NAMESPACE} \
	     --version 1.0.0-beta.3 \
	     --set aws.secretkey=$aws_access_key \
	     --set aws.accesskeyid=$aws_key_id \
	     --set deployClusterServiceBroker=false \
	     --set tls.cert="$(base64 -w0 tls.pem)" \
	     --set tls.key="$(base64 -w0 tls.key)" \
	     --set-string aws.targetaccountid=${AWS_TARGET_ACCOUNT_ID} \
	     --set aws.targetrolename=${AWS_TARGET_ROLE_NAME} \
	     --set aws.tablename=awssb \
	     --set aws.vpcid=$vpcid \
	     --set aws.region=$aws_region \
	     --set authenticate=false
</screen>
    </step>
    <step>
     <para>
      Log into your &cap; deployment. Select an organization and space to work
      with, creating them if needed:
     </para>
<screen>&prompt.user;cf api --skip-ssl-validation <replaceable>https://api.example.com</replaceable>
&prompt.user;cf login -u <replaceable>admin</replaceable> -p <replaceable>password</replaceable>
&prompt.user;cf create-org <replaceable>org</replaceable>
&prompt.user;cf create-space <replaceable>space</replaceable>
&prompt.user;cf target -o <replaceable>org</replaceable> -s <replaceable>space</replaceable>
</screen>
    </step>
    <step>
     <para>
      Create a service broker in SCF. Note the name of the service broker
      should be the same as the one specified for the <literal>--name</literal>
      flag in the <command>helm install</command> step (for example
      <literal>aws-servicebroker</literal>. Note that the username and password
      parameters are only used as dummy values to pass to the
      <command>cf</command> command:
     </para>
<screen>&prompt.user;cf create-service-broker aws-servicebroker <replaceable>username</replaceable> <replaceable>password</replaceable>  https://aws-servicebroker.${BROKER_NAMESPACE}</screen>
    </step>
    <step>
     <para>
      Verify the service broker has been registered:
     </para>
<screen>&prompt.user;cf service-brokers</screen>
    </step>
    <step>
     <para>
      List the available service plans:
     </para>
<screen>&prompt.user;cf service-access</screen>
    </step>
    <step>
     <para>
      Enable access to a service. This example uses the <literal>-p</literal>
      to enable access to a specific service plan. See
      <link xlink:href="https://github.com/awslabs/aws-servicebroker/blob/master/templates/rdsmysql/template.yaml"/>
      for information about all available services and their associated plans:
     </para>
<screen>&prompt.user;cf enable-service-access rdsmysql -p custom</screen>
    </step>
    <step>
     <para>
      Create a service instance. As an example, a custom MySQL instance can be
      created as:
     </para>
<screen>&prompt.user;cf create-service rdsmysql custom mysql-instance-name -c '{
  "AccessCidr": "192.0.2.24/32",
  "BackupRetentionPeriod": 0,
  "MasterUsername": "master",
  "DBInstanceClass": "db.t2.micro",
  "PubliclyAccessible": "true",
  "region": "${AWS_REGION}",
  "StorageEncrypted": "false",
  "VpcId": "${AWS_VPC}",
  "target_account_id": "${AWS_TARGET_ACCOUNT_ID}",
  "target_role_name": "${AWS_TARGET_ROLE_NAME}"
}'
     </screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.cap.aws-service-broker-cleanup">
   <title>Cleanup</title>
   <para>
    When the AWS Service Broker and its services are no longer required,
    perform the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Unbind any applications using any service instances then delete the
      service instance:
     </para>
<screen>&prompt.user;cf unbind-service my_app mysql-instance-name
&prompt.user;cf delete-service mysql-instance-name
</screen>
    </step>
    <step>
     <para>
      Delete the service broker in SCF:
     </para>
<screen>&prompt.user;cf delete-service-broker aws-servicebroker</screen>
    </step>
    <step>
     <para>
      Delete the deployed &helm; chart and the namespace:
     </para>
<screen>&prompt.user;helm delete --purge aws-servicebroker
&prompt.user;kubectl delete namespace ${BROKER_NAMESPACE}
</screen>
    </step>
    <step>
     <para>
      The manually created DynamoDB table will need to be deleted as well:
     </para>
<screen>&prompt.user;aws dynamodb delete-table --table-name awssb --region ${AWS_REGION}</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
