<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-production"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Production Installation with Optional &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>    
 A basic &productname; production deployment requires at least eight 
 hosts plus a storage backend: one &suse; &caasp; admin server, three &kube; 
 masters, three &kube; workers, a DNS/DHCP server, and a storage backend such as 
 &ses;. This is a bare minimum, and actual requirements are likely to be much 
 larger, depending on your workloads. You also need an external workstation for 
 administering your cluster. You may optionally make your &productname; instance 
 highly-available.
</para>
 <note>
        <title>Remote Administration</title>
        <para>
        You will run most of the commands in this chapter from a remote
        workstation, rather than directly on any of the &productname; nodes. These are
        indicated by the unprivileged user Tux, while root prompts are on a
        cluster node. There are few tasks that need to be performed directly on
        any of the cluster hosts.
    </para>
</note>
 <para>
 The optional &ha; example in this chapter provides HA only for the &productname; 
 cluster, and not for &caasp; or &ses;. See 
  <xref linkend="sec.cap.ha-prod"/>.
 </para>
 
<sect1 xml:id="sec.cap.prereqs-prod">
  <title>Prerequisites</title>
  <para>
      Calculating hardware requirements is best done with an analysis of your 
      expected workloads, traffic patterns, storage needs, and application 
      requirements. The following examples are bare minimums to deploy a
      running cluster, and any production deployment will require more.
  </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Minimum Hardware Requirements
	  </term>
	<listitem>
        <para>
          8GB of memory per &caasp; dashboard and &kube; master nodes.
        </para>
        <para>
          16GB of memory per &kube; worker.
      </para>
      <para>
          40GB disk space per &caasp; dashboard and &kube; master nodes.
      </para>
      <para>
         60GB disk space per &kube; worker.
     </para>
       </listitem>  
      </varlistentry>
      <varlistentry>
          <term>Network Requirements</term>
          <listitem>
          <para>
         Your &kube; cluster needs its own domain and network. Each node should 
         resolve to its hostname, and to its fully-qualified domain name. 
         Typically, a &kube; cluster sits behind a load balancer, which also 
         provides external access to the cluster. Another option is to use DNS 
         round-robin to the &kube; workers to provide external access. It is 
         also a common practice to create a wildcard DNS entry pointing to the 
         domain, e.g. *.&exampledomain;, so that applications can be deployed 
         without creating DNS entries for each application. This guide does not 
         describe how to set up a load balancer or name services, as these 
         depend on customer requirements and existing network architectures.
     </para>
      </listitem>  
      </varlistentry>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2
	  </term>
	<listitem>
        <para>
            After installing <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link> and logging into the Velum Web interface, 
            check the box to install Tiller (&helm;'s server component). 
        </para>
<figure xml:id="fig.cap.install-tiller-prod">
   <title>Install Tiller</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the networks that are exclusive to the internal &kube; cluster
      communications. They are not externally accessible. You may assign 
      different networks to avoid address collisions.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
        <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with AutoYaST</link>. Set up &caasp; with one admin 
          node and at least three Kubernetes masters
        and three Kubernetes workers. You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        workers will each download ~10GB of Docker images.
        </para>
  <figure xml:id="fig.cap.select-master-nodes">
   <title>Assigning Roles to Nodes</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="select-master-nodes.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
              Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file. This
takes you to a login screen; use the login you created to access Velum. Save the
file as <filename>~/.kube/config</filename> on your workstation. This 
file enables the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download-prod">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download-prod.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
  <term>Install kubectl</term>          
  <listitem>
 <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your workstation. After installation, run this command to verify
     that it is installed, and that is communicating correctly with your cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
    The following <command>kubectl</command> examples query the cluster 
    configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                  STATUS                     ROLES     AGE  VERSION
b70748d.example.com   Ready                      &lt;none>    4h   v1.7.7
cb77881.example.com   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
d028551.example.com   Ready                      &lt;none>    4h   v1.7.7
[...]
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Deploying &productname; is different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will install the &helm; client on your workstation 
     to install the required Kubernetes applications to set up &productname;, and to 
     administer your cluster remotely.
 </para>
 <para>
     Helm client version 2.6 or higher is required.
 </para>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the &caasp; installation. You do not want two &tiller;
            instances.
    </para>
</warning>
<para>
    If the Linux distribution on your workstation doesn't provide the correct 
    &helm; version, or you are using some other platform, see the 
    <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
    usage examples. Download the &helm; binary into any directory that 
    is in your PATH on your workstation, such as your <filename>~/bin</filename> 
    directory. Then initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>
</sect1>  

<sect1 xml:id="sec.cap.storageclass-prod">
 <title>Choose Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. Your available storage classes depend on which
     storage cluster you are using 
     (&ses; users, see <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/integration.html">&suse; &caasp; Integration with SES</link>). 
     After connecting your storage backend use <command>kubectl</command>
     to see your available storage classes:
 </para>
 <screen>
&prompt.user;kubectl get storageclasses
</screen>
<para>
    See <xref linkend="sec.cap.configure-prod"/> to learn where to configure your 
    storage class for &productname;. See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link>
    for detailed information on storage classes.
</para>
</sect1>

<sect1 xml:id="sec.cap.teststorageclass-prod">
 <title>Test Storage Class</title>
<para>
    You may test that your storage class is properly configured before deploying
    &productname; by creating a persistent volume claim on your storage class, then 
    verifying that the status of the claim is <literal>bound</literal>, and a 
    volume has been created.
</para>
<para>
    First copy the following configuration file, which in this example is named
    <filename>test-storage-class.yaml</filename>, substituting the name of 
    your <replaceable>storageClassName</replaceable>:
</para>
<screen>
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-sc-persistent
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: <replaceable>persistent</replaceable>
</screen>
    <para>
        Create your persistent volume claim:
    </para>
<screen>
&prompt.user;kubectl create -f test-storage-class.yaml
persistentvolumeclaim "test-sc-persistent" created
</screen>

<para>
    Check that the claim has been created, and that the status is
    <literal>bound</literal>:
</para>
<screen>
&prompt.user;kubectl get pv,pvc
NAME                                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                        STORAGECLASS   REASON    AGE
pv/pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c   1Gi        RWO            Delete           Bound     default/test-sc-persistent   persistent               2m

NAME                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc/test-sc-persistent   Bound     pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c   1Gi        RWO            persistent     2m
</screen>

<para>
    This verifies that your storage class is correctly configured. Delete your
    volume claims when you're finished:
</para>
<screen>
&prompt.user;kubectl delete pv/pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c
persistentvolume "pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c" deleted
&prompt.user;kubectl delete pvc/test-sc-persistent
persistentvolumeclaim "test-sc-persistent" deleted
</screen>

<para>
    If something goes wrong and your volume claims get stuck in
    <literal>pending</literal> status, you can force deletion with the
    <command>--grace-period=0</command> option:
</para>
<screen>
&prompt.user;kubectl delete pvc/test-sc-persistent --grace-period=0 
</screen>    
</sect1>

<sect1 xml:id="sec.cap.configure-prod">
    <title>Configuring the &productname; Production Deployment</title>
    <para>
        Create a configuration file on your workstation for &helm; to use. 
        In this example it is
        called <filename>scf-config-values.yaml</filename>. (See the
        <link xlink:href="https://www.suse.com/releasenotes/">Release Notes</link>
        for information on configuration changes.)
    </para>
    <screen>
env:    
    # Enter the domain you created for your CAP cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # UAA host and port
    UAA_HOST: uaa.<replaceable>&exampledomain;</replaceable>
    UAA_PORT: 2793

kube:
    # The IP address assigned to the kube node pointed to by the domain.
    external_ips: ["<replaceable>&kubeip;</replaceable>"]
    
    # Run kubectl get storageclasses
    # to view your available storage classes
    storage_class: 
        persistent: <replaceable>"persistent"</replaceable>
        shared: "shared"
        
    # The registry the images will be fetched from. 
    # The values below should work for
    # a default installation from the SUSE registry.
    registry: 
        hostname: "registry.suse.com"
        username: ""
        password: ""
    organization: "cap"

    # Required for CaaSP 2
    auth: rbac 

secrets:
    # Create a password for your CAP cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable> 
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>
</screen>        
</sect1>

<sect1  xml:id="sec.cap.helm-deploy-prod">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository 
    </para>
</listitem>
  <listitem>
    <para>
        Create the UAA and SCF namespaces
    </para>
</listitem>
  <listitem>
    <para>
        Copy the storage secret of your storage cluster to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Deploy UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy the UAA secret and certificate to the SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Deploy SCF
    </para>
  </listitem>  
</itemizedlist>  
</sect1>

<sect1 xml:id="sec.cap.addrepo-prod">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse
NAME         VERSION   DESCRIPTION                                  
suse/cf      2.8.0   A Helm chart for SUSE Cloud Foundry          
suse/console 1.1.0   A Helm chart for deploying Stratos UI Console
suse/uaa     2.8.0   A Helm chart for SUSE UAA
</screen>
</sect1>

<sect1 xml:id="sec.cap.create-namespaces-prod">
    <title>Create Namespaces</title>
<para>
    Create the UAA (User Account and Authentication) and 
    SCF (SUSE Cloud Foundry) namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
</screen>
</sect1>

<sect1 xml:id="sec.cap.copy-secret-prod">
    <title>Copy &ses; Secret</title>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect1>

<sect1 xml:id="sec.cap.install-uaa-prod">
    <title>Deploy UAA</title>
<para>
    Use &helm; to deploy the UAA (User Account and Authentication) server. You 
    may create your own release <literal>--name</literal>:
</para>
<screen>    
&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
When the status shows RUNNING for all of the UAA nodes, proceed to deploying 
&suse; &cf;. Pressing <keycombo>
    <keycap function="control"/>
    <keycap>C</keycap>
     </keycombo>
     stops the <command>watch</command> command.
</para>
</sect1>   

<sect1 xml:id="sec.cap.install-scf-prod">
    <title>Deploy &productname;</title>
<para>
  First pass your UAA secret and certificate to SCF, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
    When all services are running use the &cf; command-line interface 
to log in to &suse; &cf; to deploy and manage your applications. (See
<xref linkend="sec.cap.cf-cli"/>)
</para>
</sect1>

<sect1 xml:id="sec.cap.cf-cli">
<title>Deploying and Managing Applications with the &cf; Client</title>
    <para>
    The &cf; command line interface (cf-cli) is for deploying and managing your
    applications. You may use it for all the orgs and spaces that you are a 
    member of. Install the client on a workstation for remote administration of 
    your &suse; &cf; instances. 
    </para>
    <para>
The complete guide is at
    <link xlink:href="https://docs.cloudfoundry.org/cf-cli/">Using the Cloud 
        Foundry Command Line Interface</link>, and source code with a 
    demo video is on GitHub at 
    <link xlink:href="https://github.com/cloudfoundry/cli/blob/master/README.md">Cloud Foundry CLI</link>.
</para>
    <para>
      The following examples demonstrate some of the commonly-used commands.
      The first task is to log into your new &suse; &cf; instance. When your
      installation completes it prints a welcome screen with the information
      you need to access it.
    </para>
    <screen>
       NOTES:
    Welcome to your new deployment of SCF.

    The endpoint for use by the `cf` client is
        https://api.example.com

    To target this endpoint run
        cf api --skip-ssl-validation https://api.example.com

    Your administrative credentials are:
        Username: admin
        Password: password

    Please remember, it may take some time for everything to come online.

    You can use
        kubectl get pods --namespace scf

    to spot-check if everything is up and running, or
        watch -c 'kubectl get pods --namespace scf'

    to monitor continuously.    
</screen>
<para>
    You can display this message anytime with this command:
</para>
<screen>
&prompt.user;helm status $(helm list | awk '/cf-([0-9]).([0-9]).*/{print$1}') | \
sed -n -e '/NOTES/,$p'
</screen>
<para>
You need to provide the API endpoint of your &productname; instance to log in. The API 
endpoint is the <envar>DOMAIN</envar> value you provided in 
<filename>scf-config-values.yaml</filename>, plus the 
<literal>api.</literal> prefix, as it shows in the above welcome screen. Set 
your endpoint, and use <command>--skip-ssl-validation</command> when you have 
self-signed SSL certificates. It asks for an email address, but you must enter 
<literal>admin</literal> instead (you cannot change this to a different username, 
though you may create additional users), and the password is the one you created 
in <filename>scf-config-values.yaml</filename>:
</para>
<screen>
&prompt.user;cf login --skip-ssl-validation  -a https://api.example.com 
API endpoint: https://api.example.com

Email> admin

Password> 
Authenticating...
OK

Targeted org system

API endpoint:   https://api.example.com (API version: 2.101.0)
User:           admin
Org:            system
Space:          No space targeted, use 'cf target -s SPACE'
</screen>

<para>
    <command>cf help</command> displays a list of commands and options.
    <command>cf help [command]</command> provides information on specific
    commands.
</para>
<para>
    You may pass in your credentials and set the API endpoint in a single command:
</para>
<screen>
&prompt.user;cf login -u admin -p password --skip-ssl-validation -a https://api.example.com    
</screen>        
<para>
    Log out with <command>cf logout</command>.
</para>
<para>
    View your current API endpoint, user, org, and space:
</para>
<screen>
&prompt.user;cf target  
</screen>
<para>
    Switch to a different org or space:
</para>
<screen>
&prompt.user;cf target -o <replaceable>org</replaceable>
&prompt.user;cf target -s <replaceable>space</replaceable>   
</screen>
<para>
  List all apps in the current space:  
</para>
<screen>
&prompt.user;cf apps   
</screen>
<para>
    Query the health and status of a particular app:
</para>
<screen>
&prompt.user;cf app <replaceable>appname</replaceable> 
</screen>
<para>
    View app logs. The first example tails the log of a running app. 
    The <command>--recent</command> option dumps recent logs instead of tailing,
    which is useful for stopped and crashed apps:
</para>
<screen>
&prompt.user;cf logs <replaceable>appname</replaceable>
&prompt.user;cf logs --recent <replaceable>appname</replaceable>
</screen>
<para>
    Restart all instances of an app:
</para>
<screen>
&prompt.user;cf restart <replaceable>appname</replaceable>
</screen>
<para>
    Restart a single instance of an app, identified by its index number, and
    restart it with the same index number:
</para>
<screen>
&prompt.user;cf restart-app-instance <replaceable>appname</replaceable> <replaceable>index</replaceable>
</screen>
<para>
    After you have set up a service broker 
    (see <xref linkend="cha.cap.depl-sbs"/>), create new services: 
</para>
<screen>
&prompt.user;cf create-service <replaceable>service-name</replaceable> <replaceable>default</replaceable> <replaceable>mydb</replaceable>
</screen>
<para>
 Then you may bind a service instance to an app:
</para>
<screen>
&prompt.user;cf bind-service <replaceable>appname</replaceable> <replaceable>service-instance</replaceable>
</screen>
<para>
    The most-used command is <command>cf push</command>, for pushing new apps and
    changes to existing apps.
</para>
<screen>
 &prompt.user;cf push <replaceable>new-app</replaceable> -b <replaceable>buildpack</replaceable>
</screen>
</sect1>
        
<sect1 xml:id="sec.cap.stratos-prod">
    <title>Installing the Stratos Web Console</title>
    <para>
        Stratos UI is a modern web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos with 
        &helm; after all of the UAA and SCF pods are running. 
        Start by preparing the environment:
    </para>
    <screen>        
&prompt.user;kubectl create namespace stratos
</screen>
<para>
If you are using &ses; as your storage backend, copy the secret into the Stratos
namespace.
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "stratos"/' | \
kubectl create -f -       
</screen>        
<para>       
You should already have the Stratos charts when you downloaded the SUSE charts
repository:
</para>        
<screen>
&prompt.user;helm search suse
NAME         VERSION DESCRIPTION                                  
suse/cf         2.8.0   A Helm chart for SUSE Cloud Foundry          
suse/console    1.1.0   A Helm chart for deploying Stratos UI Console
suse/uaa        2.8.0   A Helm chart for SUSE UAA
</screen>
<para>
Install Stratos, and if you have not set a default storage class you must specify
it:
</para>
<screen>
&prompt.user;helm install suse/console \
    --name <replaceable>susecf-console</replaceable> \
    --namespace stratos \
    --values scf-config-values.yaml \
    --set storageClass=<replaceable>persistent</replaceable>
</screen>

<para>
    Monitor progress:
</para>

<screen>
$ watch -c 'kubectl get pods --namespace stratos'
 Every 2.0s: kubectl get pods --namespace stratos
 
NAME                               READY     STATUS    RESTARTS   AGE
console-0                          3/3       Running   0          30m
console-mariadb-3697248891-5drf5   1/1       Running   0          30m
</screen> 

<para>
    When all statuses show Ready, press 
    <keycombo>
    <keycap function="control"/>
    <keycap>C</keycap>
     </keycombo> to exit and to view your release information:
</para>

<screen>
NAME:   susecf-console
LAST DEPLOYED: Thu Apr 12 10:28:34 2018
NAMESPACE: stratos
STATUS: DEPLOYED

RESOURCES:
==> v1/Secret
NAME                           TYPE    DATA  AGE
susecf-console-mariadb-secret  Opaque  2     2s
susecf-console-secret          Opaque  2     2s

==> v1/PersistentVolumeClaim
NAME                                  STATUS  VOLUME                                    CAPACITY  ACCESSMODES  STORAGECLASS    AGE
console-mariadb                       Bound   pvc-ef3a120d-3e76-11e8-946a-90b8d00d625f  1Gi       RWO          persistent      2s
susecf-console-upgrade-volume         Bound   pvc-ef409e41-3e76-11e8-946a-90b8d00d625f  20Mi      RWO          persistent      2s
susecf-console-encryption-key-volume  Bound   pvc-ef49b860-3e76-11e8-946a-90b8d00d625f  20Mi      RWO          persistent      2s

==> v1/Service
NAME                    CLUSTER-IP      EXTERNAL-IP    PORT(S)         AGE
susecf-console-mariadb  172.24.181.255  &lt;none>         3306/TCP        2s
susecf-console-ui-ext   172.24.84.50    10.10.100.82   8443:32511/TCP  1s

==> v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
console-mariadb  1        1        1           0          1s

==> v1beta1/StatefulSet
NAME     DESIRED  CURRENT  AGE
console  1        1        1s
</screen>

<para>
    In this example, pointing your web browser to https://10.10.100.82:8443 
    opens the console. Wade through the nag screens about the self-signed
    certificates and log in as <literal>admin</literal> with the password you 
    created in <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos-prod">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure>

<para>
    Another way to get the release name is with the <command>helm ls</command>
    command, then query the release name to get its IP address and port number:
</para>

<screen>
&prompt.user;helm ls
NAME            REVISION UPDATED                   STATUS    CHART          NAMESPACE
susecf-console  1        Thu Apr 12 10:28:34 2018  DEPLOYED  console-1.1.0  stratos  
susecf-scf      1        Wed Apr 11 14:55:23 2018  DEPLOYED  cf-2.8.0       scf      
susecf-uaa      1        Wed Apr 11 14:48:01 2018  DEPLOYED  uaa-2.8.0      uaa

&prompt.user;helm status susecf-console
LAST DEPLOYED: Thu Apr 12 10:28:34 2018
NAMESPACE: stratos
STATUS: DEPLOYED
[...]
==> v1/Service
NAME                    CLUSTER-IP      EXTERNAL-IP    PORT(S)         AGE
susecf-console-mariadb  172.24.181.255  &lt;none>         3306/TCP        19m
susecf-console-ui-ext   172.24.84.50    10.10.100.82   8443:32511/TCP  19m
</screen> 
</sect1>

<!-- helm rollback does not work, so upgrades are one-way for now
-->
 <sect1 xml:id="sec.cap.update">
  <title>Upgrading &suse; &cf;, UAA, and Stratos</title>
  <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. (See the 
      <link xlink:href="https://www.suse.com/releasenotes/">Release Notes</link>
      for additional upgrade information.)
      Check for available updates:
  </para>
<screen>
&prompt.user;helm repo update
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "suse" chart repository
Update Complete. ⎈ Happy Helming!⎈   
</screen>

<para>
    For the &productname; 1.1 release, update your 
    <filename>scf-config-values.yaml</filename> file with the changes
    for secrets handling and external IP addresses. (See 
    <xref linkend="sec.cap.configure-prod"/> for an example.)
</para>
<para>
      Get your release and chart names (your releases may have different names than
      the examples), and then apply the updates:
</para>
  <screen>
&prompt.user;helm list
NAME            REVISION  UPDATED                   STATUS    CHART           NAMESPACE
<replaceable>susecf-console</replaceable>  1         Thu Apr 12 10:28:34 2018  DEPLOYED  console-1.0.2   stratos  
<replaceable>susecf-scf</replaceable>      1         Wed Apr 11 14:55:23 2018  DEPLOYED  cf-2.7.0        scf      
<replaceable>susecf-uaa</replaceable>      1         Wed Apr 11 14:48:01 2018  DEPLOYED  uaa-2.7.0       uaa 

&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/             

&prompt.user;helm search suse
NAME          VERSION  DESCRIPTION                                  
suse/cf       2.8.0    A Helm chart for SUSE Cloud Foundry          
suse/console  1.1.0    A Helm chart for deploying Stratos UI Console
suse/uaa      2.8.0    A Helm chart for SUSE UAA
</screen>

<para>
    Run the following commands to perform the upgrade. Wait for each command to
    complete before running the next command. Note the new commands for extracting
    and using secrets and certificates.
</para>
<screen>
&prompt.user;helm upgrade --recreate-pods <replaceable>susecf-uaa</replaceable> suse/uaa \
 --values scf-config-values.yaml

&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
 -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
 -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm upgrade --recreate-pods susecf-scf <replaceable>suse/cf</replaceable> \
 --values scf-config-values.yaml --set "secrets.UAA_CA_CERT=${CA_CERT}"

&prompt.user;helm upgrade --recreate-pods <replaceable>susecf-console</replaceable> suse/console \
 --values scf-config-values.yaml
</screen>
</sect1>

<sect1 xml:id="sec.cap.ha-prod">
  <title>Example &ha; Configuration</title>
  <para>
This example &ha; configuration needs two separate configuration files, one for 
UAA and one for SCF. It may be helpful to read the comments in the 
<literal>sizing:</literal> section in the default
<filename>values.yaml</filename> file for each namespace, which describes which
roles can be scaled and the scaling options for each role. One way is to use 
<command>helm inspect</command> to read the <literal>sizing:</literal> section 
in your installed charts:
</para>

<screen>
&prompt.user;helm inspect suse/uaa | less +/sizing:
&prompt.user;helm inspect suse/scf | less +/sizing:
</screen>

<para>
Another way is to download a copy of the &helm; charts, 
unpack them, and then use Perl to extract the information for each role from the 
<literal>sizing:</literal> section. The following example is for UAA.
You may see a few non-relevant lines from the sections preceding and following 
the <literal>sizing:</literal> section. In the following example these are lines 
29-53, and lines 226-245.
</para>

<screen>
&prompt.user;helm fetch suse/uaa
&prompt.user;tar xzvf cf-2.8.0.tgz
&prompt.user;cd uaa
&prompt.user;perl -ne '/^sizing/..0 and do { print $.,":",$_ if /^  [a-z]/ || /high avail|scale|count/ }' values.yaml
 29:  cpu:
 37:  memory:
 53:  mysql:
 60:    # The mysql role can scale between 1 and 3 instances.
 61:    # For high availability it needs at least 2 instances.
 62:    count: 1
 86:  mysql_proxy:
 93:    # The mysql-proxy role can scale between 1 and 3 instances.
 94:    # For high availability it needs at least 2 instances.
 95:    count: 1
 110:  secret_generation:
 117:    # The secret-generation role cannot be scaled.
 118:    count: 1
 142:  uaa:
 149:    # The uaa role can scale between 1 and 65535 instances.
 150:    count: 1
 226:  loadbalanced: false
 228:  external_ips: []
 230:  # Increment this counter to rotate all generated secrets
 231:  secrets_generation_counter: 1
 233:  storage_class:
 238:  hostpath_available: false
 240:  registry:
 244:  organization: "cap"
 245:  auth: "rbac"
</screen>

<para>
The first example configuration is for UAA, 
<filename>uaa-sizing.yaml</filename>. The values for each role depend on your
particular deployment and requirements.
</para>

<screen>
sizing:
  mysql:
    count: 3
  mysql_proxy:
    count: 2
  uaa:
    count: 2
</screen>

<para>
    The second example is for SCF, <filename>scf-sizing.yaml</filename>.
</para>
<screen>
sizing:
  api:
    count: 6
  cc_uploader:
    count: 3
  cc_worker:
    count: 6
  cf_usb:
    count: 3
  consul:
    count: 1
  diego_access:
    count: 3
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  diego_locket:
    count: 3
  doppler:
    count: 4
  loggregator:
    count: 7
  mysql:
    count: 3
  mysql_proxy:
    count: 3
  nats:
    count: 2
  nfs_broker:
    count: 3
  postgres:
    count: 3
  router:
    count: 13
  routing_api:
    count: 3
  syslog_adapter:
    count: 7
  syslog_rlp:
    count: 7
  tcp_router:
    count: 3
</screen>

<para>
    After creating your configuration files, follow the steps in 
    <xref linkend="sec.cap.configure-prod"/> until you get to
    <xref linkend="sec.cap.install-uaa-prod"/>. Then deploy UAA with this command:
</para>

<screen>    
&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml
</screen>

<para>
    When the status shows RUNNING for all of the UAA nodes, deploy SCF with 
    these commands:
</para>
<screen>
&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
 -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
 -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable>
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>

<para>
    The HA pods with the following roles will enter in both passive and ready 
    states; there should always be at least one pod in each role that is ready.
</para>
    <itemizedlist>
	 <listitem>
         <para>
     diego-brain
     </para>
     </listitem>
     <listitem>
         <para>
     diego-database
     </para>
     </listitem>
     <listitem>
         <para>
     routing-api
     </para>
     </listitem>
    </itemizedlist>
    
    <para>
        You can confirm this by looking at the logs inside the container. Look
        for <literal>.consul-lock.acquiring-lock</literal>.
    </para>
    <para>
        Some roles cannot be scaled. <command>mysql-proxy</command> needs a 
        proper active/passive configuration. <command>tcp-router</command> has 
        no mechanism for exposing ports correctly. <command>blobstore</command> 
        needs shared volume support and an active/passive configuration.
    </para>
    <para>
Some roles follow an active/passive scaling model, meaning all pods except 
the active one will be shown as NOT READY by &kube;. This is appropriate and 
expected behavior.
</para>

<sect2 xml:id="sec.cap.ha-prod-upgrade">
    <title>Upgrading a non-&ha; Deployment to &ha;</title>
    <para>
        You may make a non-&ha; deployment highly available by upgrading with
        &helm;:
    </para>
    <screen>
&prompt.user;helm upgrade suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml 

&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
 -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
 -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm upgrade suse/cf \
--name <replaceable>susecf-scf</replaceable>
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    This may take a long time, and your cluster will be unavailable until the
    upgrade is complete.
</para>
</sect2>
 </sect1>
</chapter>
