<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha-cap-depl-troubleshooting"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Troubleshooting</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  Cloud stacks are complex, and debugging deployment issues often requires
  digging through multiple layers to find the information you need. Remember
  that the &scf; releases must be deployed in the correct order, and that each
  release must deploy successfully, with no failed pods, before deploying the
  next release.
 </para>
 <sect1  xml:id="sec-cap-tbl-supportconfig">
  <title>Using Supportconfig</title>

  <para>
   If you ever need to request support, or just want to generate detailed
   system information and logs, use the <command>supportconfig</command>
   utility. Run it with no options to collect basic system information, and
   also cluster logs including Docker, etcd, flannel, and Velum.
   <command>supportconfig</command> may give you all the information you need.
  </para>

  <para>
   <command>supportconfig -h</command> prints the options. Read the "Gathering
   System Information for Support" chapter in any &sle; Administration Guide to
   learn more.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-toolong">
  <title>Deployment Is Taking Too Long</title>

  <para>
   A deployment step seems to take too long, or you see that some pods are not
   in a ready state hours after all the others are ready, or a pod shows a lot
   of restarts. This example shows not-ready pods many hours after the others
   have become ready:
  </para>

<screen>&prompt.user;kubectl get pods --namespace scf
NAME                     READY STATUS    RESTARTS  AGE
router-3137013061-wlhxb  0/1   Running   0         16h
routing-api-0            0/1   Running   0         16h</screen>

  <para>
   The <literal>Running</literal> status means the pod is bound to a node and
   all of its containers have been created. However, it is not
   <literal>Ready</literal>, which means it is not ready to service requests.
   Use <command>kubectl</command> to print a detailed description of pod events
   and status:
  </para>

<screen>&prompt.user;kubectl describe pod --namespace scf router-3137013061-wlhxb</screen>

  <para>
   This prints a lot of information, including IP addresses, routine events,
   warnings, and errors. You should find the reason for the failure in this
   output.
  </para>

  <important>
&deployment-pod-status;
  </important>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-rebuild-depl">
  <title>Deleting and Rebuilding a Deployment</title>

  <para>
   There may be times when you want to delete and rebuild a deployment, for
   example when there are errors in your
   <filename>scf-config-values.yaml</filename> file, you wish to test
   configuration changes, or a deployment fails and you want to try it again.
   This has five steps: first delete the StatefulSets of the namespace
   associated with the release or releases you want to re-deploy, then delete
   the release or releases, delete its namespace, then re-create the namespace
   and re-deploy the release.
  </para>

  <para>
   The namespace is also deleted as part of the process because the SCF and UAA
   namespaces contain generated secrets which Helm is not aware of and will not
   remove when a release is deleted. When deleting a release, busy systems may
   encounter timeouts. By first deleting the StatefulSets, it ensures that this
   operation is more likely to succeed. Using the <command>delete
   statefulsets</command> command requires <command>kubectl</command> v1.9.6 or
   newer.
  </para>

  <para>
   Use <command>helm</command> to see your releases:
  </para>

<screen>&prompt.user;helm list
NAME            REVISION  UPDATED                  STATUS    CHART           NAMESPACE
<replaceable>susecf-console</replaceable>  1         Tue Aug 14 11:53:28 2018 DEPLOYED  console-&lateststratoschart;   stratos
<replaceable>susecf-scf</replaceable>      1         Tue Aug 14 10:58:16 2018 DEPLOYED  cf-&latestscfchart;       scf
<replaceable>susecf-uaa</replaceable>      1         Tue Aug 14 10:49:30 2018 DEPLOYED  uaa-&latestuaachart;      uaa</screen>

  <para>
   This example deletes the <literal>susecf-uaa</literal> release and
   <literal>uaa</literal> namespace:
  </para>

<screen>&prompt.user;kubectl delete statefulsets --all --namespace uaa
statefulset "mysql" deleted
statefulset "uaa" deleted

&prompt.user;helm delete --purge susecf-uaa
release "susecf-uaa" deleted

&prompt.user;kubectl delete namespace uaa
namespace "uaa" deleted</screen>

  <para>
   Repeat the same process for the <literal>susecf-scf</literal> release
   and <literal>scf</literal> namespace:
  </para>
<screen>&prompt.user;kubectl delete statefulsets --all --namespace scf
statefulset "adapter" deleted
statefulset "api-group" deleted
...

&prompt.user;helm delete --purge susecf-scf
release "susecf-scf" deleted

&prompt.user;kubectl delete namespace scf
namespace "scf" deleted</screen>

  <para>
   Then you can start over. Be sure to create new release and namespace names.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-kubectl-queries">
  <title>Querying with Kubectl</title>

  <para>
   You can safely query with <command>kubectl</command> to get information
   about resources inside your &kube; cluster. <command>kubectl cluster-info
   dump | tee clusterinfo.txt</command> outputs a large amount of information
   about the &kube; master and cluster services to a text file.
  </para>

  <para>
   The following commands give more targeted information about your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     List all cluster resources:
    </para>
<screen>&prompt.user;kubectl get all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods, their internal IP addresses, and which
     &kube; nodes they are running on:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output wide</screen>
   </listitem>
   <listitem>
    <para>
     See all pods, including those with Completed or Failed statuses:
    </para>
<screen>&prompt.user;kubectl get pods --show-all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List pods in one namespace:
    </para>
<screen>&prompt.user;kubectl get pods --namespace scf</screen>
   </listitem>
   <listitem>
    <para>
     Get detailed information about one pod:
    </para>
<screen>&prompt.user;kubectl describe --namespace scf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     Read the log file of a pod:
    </para>
<screen>&prompt.user;kubectl logs --namespace scf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     List all &kube; nodes, then print detailed information about a single
     node:
    </para>
<screen>&prompt.user;kubectl get nodes
&prompt.user;kubectl describe node 6a2752b6fab54bb889029f60de6fa4d5.infra.caasp.local</screen>
   </listitem>
   <listitem>
    <para>
     List all containers in all namespaces, formatted for readability:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output jsonpath="{..image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c</screen>
   </listitem>
   <listitem>
    <para>
     These two commands check node capacities, to verify that there are enough
     resources for the pods:
    </para>
<screen>&prompt.user;kubectl get nodes --output yaml | grep '\sname\|cpu\|memory'
&prompt.user;kubectl get nodes --output json | \
jq '.items[] | {name: .metadata.name, cap: .status.capacity}'</screen>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-autoscaler-metrics-lock">
  <title><literal>autoscaler-metrics</literal> Pod Liquibase Error</title>

  <para>
   In some situations, the <literal>autoscaler-metrics</literal> pod may fail to
   reach a fully ready state due to a Liquibase error,
   <literal>liquibase.exception.LockException: Could not acquire change log lock
   </literal>.
  </para>
  <para>
   When this occurs, use the following procedure to resolve the issue.
  </para>
  <procedure>
   <step>
    <para>
     Verify whether the error has occurred by examining the logs.
    </para>
<screen>&prompt.user;kubectl logs autoscaler-metrics-0 autoscaler-metrics --follow --namespace scf</screen>
    <para>
     If the error has occurred, the log will contain entires similar to the following.
    </para>
<screen># Starting Liquibase at Wed, 21 Aug 2019 22:38:00 UTC (version 3.6.3 built at 2019-01-29 11:34:48)
# Unexpected error running Liquibase: Could not acquire change log lock.  Currently locked by autoscaler-metrics-0.autoscaler-metrics-set.cf.svc.cluster.local (172.16.0.173) since 8/21/19, 9:20 PM
# liquibase.exception.LockException: Could not acquire change log lock.  Currently locked by autoscaler-metrics-0.autoscaler-metrics-set.cf.svc.cluster.local (172.16.0.173) since 8/21/19, 9:20 PM
#         at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:230)
#         at liquibase.Liquibase.update(Liquibase.java:184)
#         at liquibase.Liquibase.update(Liquibase.java:179)
#         at liquibase.integration.commandline.Main.doMigration(Main.java:1220)
#         at liquibase.integration.commandline.Main.run(Main.java:199)
#         at liquibase.integration.commandline.Main.main(Main.java:137)
</screen>
   </step>
   <step>
    <para>
     After confirming the error has occurred, use the provided Ruby script to
     recover from the error.
    </para>
    <substeps>
     <step>
      <para>
       Save the script below to a file. In this example, it is saved to a file
       named <filename>recovery-autoscaler-metrics-liquidbase-stale-lock</filename>.
      </para>
<screen>#!/usr/bin/env ruby

# This script recovers from autoscaler-metrics failing to start because it could
# not continue schema migrations.  The error looks something like:

# Starting Liquibase at Wed, 21 Aug 2019 22:38:00 UTC (version 3.6.3 built at 2019-01-29 11:34:48)
# Unexpected error running Liquibase: Could not acquire change log lock.  Currently locked by autoscaler-metrics-0.autoscaler-metrics-set.cf.svc.cluster.local (172.16.0.173) since 8/21/19, 9:20 PM
# liquibase.exception.LockException: Could not acquire change log lock.  Currently locked by autoscaler-metrics-0.autoscaler-metrics-set.cf.svc.cluster.local (172.16.0.173) since 8/21/19, 9:20 PM
#         at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:230)
#         at liquibase.Liquibase.update(Liquibase.java:184)
#         at liquibase.Liquibase.update(Liquibase.java:179)
#         at liquibase.integration.commandline.Main.doMigration(Main.java:1220)
#         at liquibase.integration.commandline.Main.run(Main.java:199)
#         at liquibase.integration.commandline.Main.main(Main.java:137)

require 'json'
require 'open3'

EXPECTED_ERROR='Could not acquire change log lock.  Currently locked by'

def capture(*args)
    stdout, status = Open3.capture2(*args)
    return stdout if status.success?
    cmd = args.dup
    cmd.shift while Hash.try_convert(cmd.first)
    cmd.pop while Hash.try_convert(cmd.last)
    fail %Q&lt;"#{cmd.join(' ')}" failed with exit status #{status.exitstatus}>
end

def namespace
    $namespace ||= JSON.parse(capture('helm list --output json'))['Releases']
        .select { |r| r['Status'].downcase == 'deployed' }
        .select { |r| r['Chart'].start_with? 'cf-' }
        .first['Namespace']
end

def run(*args)
    Process.wait Process.spawn(*args)
    return if $?.success?
    cmd = args.dup
    cmd.shift while Hash.try_convert(cmd.first)
    cmd.pop while Hash.try_convert(cmd.last)
    fail %Q&lt;"#{cmd.join(' ')}" failed with exit status #{$?.exitstatus}>
end

postgres_pod = JSON.load(capture('kubectl', 'get', 'pods',
    '--namespace', namespace,
    '--selector', 'skiff-role-name==autoscaler-postgres',
    '--output', 'json'))['items']
    .find { |pod|
        pod['status']['conditions']
            .find {|status| status['type'] == 'Ready'}['status'] == 'True'
    }['metadata']['name']

psql_binary = capture('kubectl', 'exec', postgres_pod,
    '--namespace', namespace,
    '--', 'sh', '-c', 'echo /var/vcap/packages/postgres-*/bin/psql').split.first

run 'kubectl', 'exec', postgres_pod,
    '--namespace', namespace,
    '--',
    psql_binary,
    '--username', 'postgres',
    '--dbname', 'autoscaler',
    '--command', 'SELECT * FROM databasechangeloglock'

locked_bys = (1..Float::INFINITY).each do |count|
    STDOUT.printf "\rWaiting for pod to get stuck (try %s)...", count
    candidate_locked_bys = capture('kubectl', 'get', 'pods',
                        '--namespace', namespace,
                        '--selector', 'skiff-role-name==autoscaler-metrics',
                        '--output', 'jsonpath={.items[*].metadata.name}')
        .split
        .map do |pod_name|
            capture('kubectl', 'logs', pod_name,
                    '--namespace', namespace,
                    '--container', 'autoscaler-metrics')
        end
        .select { |message| message.include? EXPECTED_ERROR }
        .map { |message| /Currently locked by (.*?) since /.match(message)[1]}

    break candidate_locked_bys unless candidate_locked_bys.empty?
    sleep 1
end
puts '' # new line after the wait
locked_bys.each do |locked_by|
    puts "Releasing lock for #{locked_by}"
    run 'kubectl', 'exec', postgres_pod,
        '--namespace', namespace,
        '--',
        psql_binary,
        '--username', 'postgres',
        '--dbname', 'autoscaler',
        '--command', %Q&lt;UPDATE databasechangeloglock SET locked = FALSE WHERE lockedby = '#{locked_by}'>
end
</screen>
     </step>
     <step>
      <para>
       Run the script.
      </para>
<screen>&prompt.user;ruby <replaceable>recovery-autoscaler-metrics-liquidbase-stale-lock</replaceable></screen>
      <para>
       All <literal>autoscaler</literal> pods should now enter a fully ready state.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-buildpack-with-stack">
  <title><literal>api-group</literal> Pod Not Ready after Buildpack Update</title>
  <para>
   When a built-in buildpack is updated as described in one of the scenarios
   below, the <literal>api-group</literal> pod will fail to reach a fully ready
   state upon restart. A built-in buildpack is a buildpack that is included as
   part of a &productname; release and has no stack association by default.
  </para>
  <para>
   The following scenarios can result in the <literal>api-group</literal> not
   being fully ready:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     A built-in buildpack is assigned a stack association. For example:
    </para>
<screen>&prompt.user;cf update-buildpack <replaceable>ruby_buildpack</replaceable> --assign-stack <replaceable>sle15</replaceable></screen>
   </listitem>
   <listitem>
    <para>
     A built-in buildpack is updated to to a version with an inherent stack
     association, such as those provided by upstream &cf;. For example:
    </para>
<screen>&prompt.user;cf update-buildpack <replaceable>ruby_buildpack</replaceable> -p <replaceable>https://github.com/cloudfoundry/ruby-buildpack/releases/download/v1.7.43/ruby-buildpack-cflinuxfs3-v1.7.43.zip</replaceable></screen>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec-cap-tbl-buildpack-with-stack-recovery-prevention">
   <title>Recovery and Prevention</title>
   <para>
    In order to bring the <literal>api-group</literal> pod back to a fully ready
    state, use the following procedure.
   </para>
   <procedure>
    <step>
     <para>
      Delete the buildpack that was updated to have a stack association.
     </para>
<screen>&prompt.user;cf delete-buildpack <replaceable>ruby_buildpack</replaceable></screen>
    </step>
    <step>
     <para>
      Restart the <literal>api-group</literal> pod.
     </para>
<screen>&prompt.user;kubectl delete pod api-group-0 --namespace <replaceable>scf</replaceable></screen>
    </step>
   </procedure>
   <para>
    In order to use a buildpack with a stack association and not encounter the
    issue described in this section, do the following. Instead of updating one
    of the built-in buildpacks to either assign a stack or update to one with an
    inherent stack association, create a new buildpack with the desired buildpack
    files and associated stack.
   </para>
<screen>&prompt.user;cf create-buildpack <replaceable>ruby_cflinuxfs3</replaceable> <replaceable>https://github.com/cloudfoundry/ruby-buildpack/releases/download/v1.7.41/ruby-buildpack-cflinuxfs3-v1.7.41.zip</replaceable> <replaceable>1</replaceable> <replaceable>--enable</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-mysql-failure">
  <title><literal>mysql</literal> Pods Fail to Start</title>
  <para>
   In some cases, <literal>mysql</literal> pods may fail to start due to data
   initialization problems or incomplete transactions from a previously aborted
   operation. This can affect the <literal>mysql</literal> pod in both
   <literal>uaa</literal> and <literal>scf</literal>.
  </para>
  <procedure>
   <step>
    <para>
     Confirm whether the <literal>mysql</literal> pod failure is related to
     data initialization problems or incomplete transactions.
    </para>
    <substeps>
     <step>
      <para>
       Perform an initial inspection of the the <literal>mysql</literal>
       container's logs. A <literal>pre-start</literal> related error should be
       observed. If the <literal>mysql</literal> pod from <literal>scf</literal>
       is experiencing issues, substitute <literal>scf</literal> as the
       namespace.
      </para>
<screen>&prompt.user;kubectl logs --namespace <replaceable>uaa</replaceable> <replaceable>mysql-0</replaceable></screen>
     </step>
     <step>
      <para>
       Examine the <filename>/var/vcap/sys/log/pxc-mysql/mysql.err.log</filename>
       file from the <literal>mysql</literal> container filesystem. If the
       <literal>mysql</literal> pod from <literal>scf</literal> is experiencing
       issues, substitute <literal>scf</literal> as the namespace.
      </para>
<screen>&prompt.user;kubectl exec --stdin --tty --namespace <replaceable>uaa</replaceable> <replaceable>mysql-0</replaceable> -- bash -c 'cat /var/vcap/sys/log/pxc-mysql/mysql.err.log'</screen>
      <para>
       One of the following error messages should be found:
      </para>
      <itemizedlist>
       <listitem>
<screen>[ERROR] Can't open and lock privilege tables: Table 'mysql.servers' doesn't exist
[ERROR] Fatal error: Can't open and lock privilege tables: Table 'mysql.user' doesn't exist
</screen>
       </listitem>
       <listitem>
<screen>[ERROR] Found 1 prepared transactions! It means that mysqld was not shut down properly last time and critical recovery information (last binlog or tc.log file) was manually deleted after a crash. You have to start mysqld with --tc-heuristic-
recover switch to commit or rollback pending transactions.
[ERROR] Aborting
</screen>
       </listitem>
      </itemizedlist>
       <para>
        This can happen when <literal>mysql</literal> fails to initialize and
        read its data directory, possibly due to resource limitations in the
        cluster.
       </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     If the <literal>mysql</literal> pod failure is confirmed to be due to
     aborted transactions, perform the following procedure to return it to
     a running state.
    </para>
    <para>
     When using the procedure, it is important to note that:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In a high availability setup, replication will synchronize data to the
       new, clean persistent volume claim (PVC).
      </para>
     </listitem>
     <listitem>
      <para>
       In a single availability setup, backup your data before applying the
       procedure as it <emphasis>will result in data loss</emphasis>. It is a
       recommended practice to regulary backup your data.
      </para>
     </listitem>
    </itemizedlist>
    <substeps>
     <step>
      <para>
       Delete the persistent volume claim (PVC) attached to the failed
       <literal>mysql</literal> pod. Identify the name of the PVC by using
       <command>kubectl describe pod</command> and inspecting the
       <literal>ClaimName</literal> field. If the <literal>mysql</literal> pod
       from <literal>scf</literal> is experiencing issues, substitute
       <literal>scf</literal> as the namespace.
      </para>
<screen>&prompt.user;kubectl describe pod --namespace <replaceable>uaa</replaceable> <replaceable>mysql-0</replaceable>

&prompt.user;kubectl delete persistentvolumeclaim --namespace <replaceable>uaa</replaceable> <replaceable>mysql-data-mysql-0</replaceable>
</screen>
     </step>
     <step>
      <para>
       Immediately delete the stuck <literal>mysql</literal> pod. It is
       important to get the order right as otherwise, PVC deletion will never
       terminate because of the attached pod. If the <literal>mysql</literal>
       pod from <literal>scf</literal> is experiencing issues, substitute
       <literal>scf</literal> as the namespace.
      </para>
<screen>&prompt.user;kubectl delete pod --namespace <replaceable>uaa</replaceable> <replaceable>mysql-0</replaceable></screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
</chapter>
