<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha-cap-depl-troubleshooting"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Troubleshooting</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  Cloud stacks are complex, and debugging deployment issues often requires
  digging through multiple layers to find the information you need. Remember
  that the &scf; releases must be deployed in the correct order, and that each
  release must deploy successfully, with no failed pods, before deploying the
  next release.
 </para>
 <sect1  xml:id="sec-cap-tbl-supportconfig">
  <title>Using Supportconfig</title>

  <para>
   If you ever need to request support, or just want to generate detailed
   system information and logs, use the <command>supportconfig</command>
   utility. Run it with no options to collect basic system information, and
   also cluster logs including Docker, etcd, flannel, and Velum.
   <command>supportconfig</command> may give you all the information you need.
  </para>

  <para>
   <command>supportconfig -h</command> prints the options. Read the "Gathering
   System Information for Support" chapter in any &sle; Administration Guide to
   learn more.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-toolong">
  <title>Deployment Is Taking Too Long</title>

  <para>
   A deployment step seems to take too long, or you see that some pods are not
   in a ready state hours after all the others are ready, or a pod shows a lot
   of restarts. This example shows not-ready pods many hours after the others
   have become ready:
  </para>

<screen>&prompt.user;kubectl get pods --namespace scf
NAME                     READY STATUS    RESTARTS  AGE
router-3137013061-wlhxb  0/1   Running   0         16h
routing-api-0            0/1   Running   0         16h</screen>

  <para>
   The <literal>Running</literal> status means the pod is bound to a node and
   all of its containers have been created. However, it is not
   <literal>Ready</literal>, which means it is not ready to service requests.
   Use <command>kubectl</command> to print a detailed description of pod events
   and status:
  </para>

<screen>&prompt.user;kubectl describe pod --namespace scf router-3137013061-wlhxb</screen>

  <para>
   This prints a lot of information, including IP addresses, routine events,
   warnings, and errors. You should find the reason for the failure in this
   output.
  </para>

  <important>
&deployment-pod-status;
  </important>
 </sect1>
 <sect1 xml:id="sec-cap-rebuild-depl">
  <title>Deleting and Rebuilding a Deployment</title>

  <para>
   There may be times when you want to delete and rebuild a deployment, for
   example when there are errors in your
   <filename>scf-config-values.yaml</filename> file, you wish to test
   configuration changes, or a deployment fails and you want to try it again.
   This has five steps: first delete the StatefulSets of the namespace
   associated with the release or releases you want to re-deploy, then delete
   the release or releases, delete its namespace, then re-create the namespace
   and re-deploy the release.
  </para>

  <para>
   The namespace is also deleted as part of the process because the SCF and UAA
   namespaces contain generated secrets which Helm is not aware of and will not
   remove when a release is deleted. When deleting a release, busy systems may
   encounter timeouts. By first deleting the StatefulSets, it ensures that this
   operation is more likely to succeed. Using the <command>delete
   statefulsets</command> command requires <command>kubectl</command> v1.9.6 or
   newer.
  </para>

  <para>
   Use <command>helm</command> to see your releases:
  </para>

<screen>&prompt.user;helm list
NAME            REVISION  UPDATED                  STATUS    CHART           NAMESPACE
<replaceable>susecf-console</replaceable>  1         Tue Aug 14 11:53:28 2018 DEPLOYED  console-&lateststratoschart;   stratos
<replaceable>susecf-scf</replaceable>      1         Tue Aug 14 10:58:16 2018 DEPLOYED  cf-&latestscfchart;       scf
<replaceable>susecf-uaa</replaceable>      1         Tue Aug 14 10:49:30 2018 DEPLOYED  uaa-&latestuaachart;      uaa</screen>

  <para>
   This example deletes the <literal>susecf-uaa</literal> release and
   <literal>uaa</literal> namespace:
  </para>

<screen>&prompt.user;kubectl delete statefulsets --all --namespace uaa
statefulset "mysql" deleted
statefulset "uaa" deleted

&prompt.user;helm delete --purge susecf-uaa
release "susecf-uaa" deleted

&prompt.user;kubectl delete namespace uaa
namespace "uaa" deleted</screen>

  <para>
   Repeat the same process for the <literal>susecf-scf</literal> release
   and <literal>scf</literal> namespace:
  </para>
<screen>&prompt.user;kubectl delete statefulsets --all --namespace scf
statefulset "adapter" deleted
statefulset "api-group" deleted
...

&prompt.user;helm delete --purge susecf-scf
release "susecf-scf" deleted

&prompt.user;kubectl delete namespace scf
namespace "scf" deleted</screen>

  <para>
   Then you can start over. Be sure to create new release and namespace names.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-kubectl-queries">
  <title>Querying with Kubectl</title>

  <para>
   You can safely query with <command>kubectl</command> to get information
   about resources inside your &kube; cluster. <command>kubectl cluster-info
   dump | tee clusterinfo.txt</command> outputs a large amount of information
   about the &kube; master and cluster services to a text file.
  </para>

  <para>
   The following commands give more targeted information about your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     List all cluster resources:
    </para>
<screen>&prompt.user;kubectl get all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods, their internal IP addresses, and which
     &kube; nodes they are running on:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output wide</screen>
   </listitem>
   <listitem>
    <para>
     See all pods, including those with Completed or Failed statuses:
    </para>
<screen>&prompt.user;kubectl get pods --show-all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List pods in one namespace:
    </para>
<screen>&prompt.user;kubectl get pods --namespace scf</screen>
   </listitem>
   <listitem>
    <para>
     Get detailed information about one pod:
    </para>
<screen>&prompt.user;kubectl describe --namespace scf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     Read the log file of a pod:
    </para>
<screen>&prompt.user;kubectl logs --namespace scf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     List all &kube; nodes, then print detailed information about a single
     node:
    </para>
<screen>&prompt.user;kubectl get nodes
&prompt.user;kubectl describe node 6a2752b6fab54bb889029f60de6fa4d5.infra.caasp.local</screen>
   </listitem>
   <listitem>
    <para>
     List all containers in all namespaces, formatted for readability:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output jsonpath="{..image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c</screen>
   </listitem>
   <listitem>
    <para>
     These two commands check node capacities, to verify that there are enough
     resources for the pods:
    </para>
<screen>&prompt.user;kubectl get nodes --output yaml | grep '\sname\|cpu\|memory'
&prompt.user;kubectl get nodes --output json | \
jq '.items[] | {name: .metadata.name, cap: .status.capacity}'</screen>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
