<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     A production deployment of &productname; requires a 
     significant number of physical or virtual nodes. For testing and learning, 
     you can set up a minimal four-node deployment of &susecf; on &suse; &caasp;, 
     on a single workstation in a hypervisor such as KVM or VirtualBox. This 
     extremely minimal deployment uses &kube;' hostpath storage type instead of 
     a storage server, such as &ses;. You must also provide DNS, DHCP, and a 
     network space for your cluster. KVM and VirtualBox include name services 
     and network management.
 </para>
 <para>
     This minimal four-node deployment will run on a minimum of 32GB host system 
     memory, though more memory is better. This is enough to test setting up 
     and configuring &suse; &caasp; and &susecf;, and to run a few lightweight 
     workloads. You may also test connecting external servers with your cluster, 
     such as a separate name server, a storage server (e.g. &ses;), &scc;, or 
     &smtool;. You must be familiar with installing and configuring &caasp; (see
     the <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/index.html">&suse; &caasp; 2 Deployment Guide</link>).
 </para>
 <para>
     After you have completed installing &caasp; you will install and administer 
     &susecf; remotely from your host workstation, using tools such as the 
     <link xlink:href="https://docs.helm.sh/">&helm; package manager for
         Kubernetes</link>, and the Kubernetes command-line tool 
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/kubectl">
         kubectl</link>.
 </para>
   <warning>
        <title>Limitations of minimal test environment</title>
        <para>
        This is a limited deployment that is useful for testing basic 
        deployment and functionality, but it is NOT a production system, and
        cannot be upgraded to a production system. Its reduced complexity allows 
        basic testing, it is portable (on laptops with enough memory), and is 
        useful in environments that have resource constraints.
    </para>
</warning>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
 
 <important>
     <title>You must be familiar with &suse; &caasp;</title>
     <para>
        Setting up &suse; &caasp;, and knowledge of basic administration is 
        essential to a successful &productname; deployment. See
     the <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/index.html">&suse; &caasp; 2 Deployment Guide</link>
 </para>
</important>
 	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, 
        one Kubernetes master, and two Kubernetes workers. You also need an 
        Internet connection, as the installer downloads additional packages, 
        and the &kube; workers will each download ~10GB of Docker images.
        </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any quad-core AMD64/Intel EM64T processor. This table describes the 
        minimum requirements per node.
        </para>
<informaltable>
  <tgroup cols="4">
    <thead>
      <row>
        <entry>Node</entry>
        <entry>CPU</entry>
        <entry>RAM</entry>
        <entry>Disk</entry>
    </row>
</thead>
<tbody>
    <row>
    <entry>&caasp; Dashboard</entry>
    <entry>1</entry>
    <entry>8GB</entry>
    <entry>40GB</entry>
    </row>

    <row>
    <entry>&caasp; Master</entry>
    <entry>2</entry>
    <entry>8GB</entry>
    <entry>40GB</entry>
    </row>

    <row>
    <entry>&caasp; Workers</entry>
    <entry>4</entry>
    <entry>16GB</entry>
    <entry>60GB</entry>
    </row>
</tbody>
</tgroup>
</informaltable>
</listitem>  
</varlistentry>
  <varlistentry>
      <term>Network and Name Services</term>
     <listitem>
         <para>
         You must provide DNS and DHCP services, either via your hypervisor, or 
         with a separate name server. Your cluster needs its own domain. Every 
         node needs a hostname and a fully-qualified domain name, and should all 
         be on the same network. By default, the &caasp; installer requests a 
         hostname from the DHCP server. When you install the admin server you 
         may adjust its network settings manually. When you deploy the &kube; 
         workers you will create their hostnames as a kernel boot option.         
     </para>
     <para>
         After your &kube; nodes are running select one &kube; worker to act as 
         the external access  point for your cluster and map your domain name to 
         it. On production clusters it is a common practice to use wildcard DNS,
         rather than trying to manage DNS for hundreds or thousands of 
         applications. Map this to the IP address of the &kube; worker you 
         selected as the external access point to your cluster.
  </para>
     </listitem>
      </varlistentry>
<varlistentry>
 <term>
	    Install &suse; &caasp; 2
  </term>
	<listitem>
        <para>
            Install &suse; &caasp; 2 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link>. 
            When you reach the step where you log
            into the Velum Web interface, check the box to install &tiller; (&helm;'s 
            server component).  
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install &tiller;</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
    <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the cluster and services networks that are exclusive to the internal 
      cluster communications. They are not accessible outside of the cluster. 
      You may change the default overlay network assignments to avoid address 
      collisions with your existing network.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
  <para>
          The easiest way to create the &kube; nodes is to use &ay; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with &ay;</link>. Pass in these kernel boot
          options to each worker: <literal>hostname, netsetup</literal>, and the 
          autoyast path, which you find in Velum on the "Bootstrap your CaaS 
          Platform" page.


  </para>
  
 <figure xml:id="fig.cap.autoyast">
   <title>Kernel boot options</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="autoyast.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure> 
  
  <para>
          When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
    Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file. This
takes you to a login screen; use the login you created to access Velum. Save the
file as <filename>~/.kube/config</filename> on your host workstation. This 
file enables the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
    
<term>Install kubectl</term>  
  <listitem>
  <note>
        <title>Remote Cluster Administration</title>
        <para>
        You will administer your cluster from your host workstation, rather than 
        directly on any of your cluster nodes. The remote environment is 
        indicated by the unprivileged user Tux, while root prompts are on a node. 
        There are few tasks that need to be performed directly on
        any of the nodes.
    </para>
</note>
      <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your host workstation. After installation, run this command to verify
     that it is installed, and that it is communicating correctly with your 
     cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                         STATUS                     ROLES     AGE  VERSION
4a10db2c.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
87c9e8ff.infra.caasp.local   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
34ce7eb0.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Installing &susecf; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will install the &helm; client on your host workstation 
     to install the required Kubernetes applications to set up &susecf;, and to 
     administer your cluster remotely.
 </para>
 <para>
     You should match the &helm; version with the version of &tiller; 
     that is running on your cluster. The &tiller; binary cannot report its
     version, and you need the version that is packaged inside the &tiller; 
     container. Run the following command from your workstation to query the
     logs:
 </para>
 <screen>
&prompt.user;kubectl logs -l name=tiller --namespace=kube-system | grep "Starting &tiller;"
  [main] 2018/01/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
</screen>
<para>
   If the log gets overwritten and loses this information, the following command 
   queries the <command>rpm</command> package manager inside the container. This
   works only on &caasp;/&cap; installations:
</para>
<screen>
&prompt.user;kubectl exec -it $(kubectl get pods -n kube-system | awk '/tiller/{print$1}') -n kube-system -- rpm -q helm
  helm-2.6.1-1.6.x86_64
</screen>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the &caasp; installation. You do not want two &tiller;
            instances. Also make sure to install the &helm; version that matches
            your &tiller; version.
    </para>
</warning>
<para>
    If the Linux distribution on your workstation doesn't provide the correct 
    &helm; version, or you are using some other platform, see the 
    <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
    usage examples. Download the matching &helm; binary into any directory that 
    is in your PATH on your workstation, such as your <filename>~/bin</filename> 
    directory. Then initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>

</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>
 <warning>
        <title>Enable hostpath-provisioner on &caasp;</title>
        <para>
        &caasp; is configured as a multi-node &kube; setup with a minimum of 
        one master and two workers. Enabling hostpath provisioning on &caasp; 
        will use local storage on each of these nodes. Therefore persistent 
        data stored will also only be available locally on the respective 
        &kube; nodes. That directly impacts use cases, where &susecf; containers 
        could restart on a different &kube; worker than before. Typically that 
        is about restarting containers for high availability or update tests. 
        If a container starts on a different worker than before, it will miss 
        its persistent data, leading to various other side effects. In 
        addition, hostpath-provisioner uses the local root filesystem of the 
        &kube; node. If it runs out of disk space your &kube; node won't work 
        anymore.
    </para>
</warning>

 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
&prompt.root;vim /etc/kubernetes/controller-manager 
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
&prompt.root;systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class named 
    <literal>hostpath</literal>: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1",
"metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -

storageclass "hostpath" created
</screen>
<para>
    Verify that your new storage class has been created:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    Log into all of your &kube; nodes and change the permissions on 
    <filename>/tmp/hostpath_pv</filename> to 0777:
</para>
<screen>
&prompt.root;chmod -R 0777 /tmp/hostpath_pv
</screen>

<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</link> for detailed information on storage classes.
</para>
     <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        By default, &suse; &caasp; allows logging into the &kube; nodes only from
        the admin node. You can set up direct logins to your &kube; nodes from 
        your workstation by copying the SSH keys from your admin node to your 
        &kube; nodes, and then you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier, especially if you want to use mouse 
        select/middle-click paste to copy commands and output from a virtual 
        terminal on your host workstation.
    </para>
</tip>
</sect1> 
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a new configuration file for &helm; to use. In this example it is
        called <filename>scf-config-values.yaml</filename>.
    </para>
        <screen>
env:
    # Create a password for your &susecf; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # The domain you created for your cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your 
    # User Account and Authentication (UAA) server
    # Use the worker you selected for external 
    # access to your cluster
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The external IP address of the &kube; worker
    # you selected for external access
    external_ip: <replaceable>&kubeip;</replaceable>
    
    # Your chosen storage class
    storage_class:
        persistent: "<replaceable>hostpath</replaceable>"
        shared: "shared"
        
    # The Docker registry the images will be fetched from
    # This is the default
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap"

    # Required for CaaSP 2
    auth: rbac        
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository
    </para>
</listitem>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        If you are using &ses;, copy the storage secret to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.addrepo-min">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME        URL                                             
stable      https://kubernetes-charts.storage.googleapis.com
local       http://127.0.0.1:8879/charts                    
suse        https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse
NAME             VERSION   DESCRIPTION                                  
suse/cf      2.6.11 A Helm chart for SUSE Cloud Foundry          
suse/console 1.0.0     A Helm chart for deploying Stratos UI Console
suse/uaa     2.6.11 A Helm chart for SUSE UAA   
</screen>
</sect2>

<sect2 xml:id="sec.cap.create-namespaces">
    <title>Create Namespaces</title>
<para>
    Use <command>kubectl</command> on your host workstation to create and verify 
    the UAA (User Account and Authentication) and SCF (SUSE Cloud Foundry) 
    namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
&prompt.user;kubectl get namespaces
NAME          STATUS    AGE
default       Active    27m
kube-public   Active    27m
kube-system   Active    27m
scf           Active    1m
uaa           Active    1m
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret">
    <title>Copy &ses; Secret</title>
<para>
    If you are using the hostpath storage class 
    (see <xref linkend="sec.cap.storageclass"/>
    there is no secret so skip this step.
</para>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa">
    <title>Install UAA</title>
<para>
    Use &helm; to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install suse/uaa --name uaa \
--namespace uaa --values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
When the status shows RUNNING for all of the UAA nodes, then proceed to the next
step.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf">
    <title>Install &susecf;</title>
<para>
  First copy your UAA certificate authority (CA) into a shell variable, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;CA_CERT="$(kubectl get secret secret --namespace uaa -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
&prompt.user;helm install -f scf-config-values.yaml --namespace scf suse/cf --set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
    When all services are running you can use the &cf; command-line interface 
    to log in to &suse; &cf;. (See <xref linkend="sec.cap.install-scf-prod"/>.)
</para>    
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos-min">
    <title>Install the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. (See <xref linkend="sec.cap.stratos-prod"/>).
    </para>
  </sect1>

 <sect1 xml:id="sec.cap.update-min">
  <title>Updating &suse; &cf;, UAA, and Stratos</title> 
     <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. See <xref linkend="sec.cap.update"/>.
  </para> 
  <note>
      <title>No Updates with Hostpath</title>
      <para>
          Updates do not work with the hostpath storage type.
      </para>
  </note>
  
  </sect1>
  
  </chapter>
