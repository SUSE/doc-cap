<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.ha"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&productname; &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

<sect1 xml:id="sec.cap.ha-prod">
  <title>Example &ha; Configuration</title>
  <para>
      There are two ways to make your &productname; deployment highly available.
      The simplest method is to set the <literal>HA</literal> parameter in your
      deployment configuration file to <literal>true</literal>. The 
      second method is to create custom configuration files with your own custom
      values.
  </para>
  
<sect2 xml:id="sec.cap.ha-defaults">
  <title>Finding Default and Allowable Sizing Values</title>  
  <para>
  The <literal>sizing:</literal> section in the &helm; 
<filename>values.yaml</filename> files for each namespace describes which
roles can be scaled, and the scaling options for each role. You may use 
<command>helm inspect</command> to read the <literal>sizing:</literal> section 
in the &helm; charts:
</para>

<screen>
&prompt.user;helm inspect suse/uaa | less +/sizing:
&prompt.user;helm inspect suse/cf | less +/sizing:
</screen>

<para>
Another way is to use Perl to extract the information for each role from the 
<literal>sizing:</literal> section. The following example is for the UAA
namespace.
</para>

<note>
    <title>mysql_proxy does not scale</title>
    <para>
    Currently <literal>mysql_proxy</literal> does not scale. This will change in
    the &productname; 1.2 release, and then scaling will be supported.
</para>
</note>

<screen>
&prompt.user;helm inspect values suse/uaa | \
perl -ne '/^sizing/..0 and do { print $.,":",$_ if /^ [a-z]/ || /high avail|scale|count/ }'
 60:    # The mysql role can scale between 1 and 3 instances.
 61:    # For high availability it needs at least 2 instances.
 62:    count: 1
 93:    # The mysql-proxy role can scale between 1 and 3 instances.
 94:    # For high availability it needs at least 2 instances.
 95:    count: 1
 117:    # The secret-generation role cannot be scaled.
 118:    count: 1
 140:  #   for managing user accounts and for registering OAuth2 clients, as well as
 149:    # The uaa role can scale between 1 and 65535 instances.
 150:    count: 1
 230:  # Increment this counter to rotate all generated secrets
 231:  secrets_generation_counter: 1
</screen>

<para>
    The default <filename>values.yaml</filename> files are also included in this
    guide at <xref linkend="app.configuration.files"/>.
</para>
</sect2>

<sect2 xml:id="sec.cap.ha-simple"> 
    <title>Simple &ha; Configuration</title>
    <para>
        The simplest way to make your &productname; deployment highly available
        is to set <literal>HA</literal> to <literal>true</literal> in your
        deployment configuration file, e.g. 
        <filename>scf-config-values.yaml</filename>:
    </para>
    
  <screen>
config:
  # Flag to activate high-availability mode
  HA: true
</screen>

<para>
    Or, you may pass it as a command-line option when you are deploying with
    &helm;, for example:
</para>

<screen>
&prompt.user;helm install suse/uaa \
 --name susecf-uaa \
 --namespace uaa \ 
 --values scf-config-values.yaml \
 --set config.HA=true
</screen>

<para>
   This changes all roles with a default size of 1 to the minimum required for
   a &ha; deployment. It is not possible to customize any of the sizing values.
</para>
</sect2>
    
<sect2 xml:id="sec.cap.ha-custom"> 
    <title>Example Custom &ha; Configurations</title>
  
    <para>
The following two example &ha; configuration files are for the UAA and SCF 
namespaces. The example values are not meant to be copied, as these depend on 
your particular deployment and requirements. Do not change the 
<literal>config.HA</literal> flag to <literal>true</literal> 
(see <xref linkend="sec.cap.ha-simple"/>.) 
</para>

<para>
    The first example is for the UAA namespace, 
    <filename>uaa-sizing.yaml</filename>:
</para>

<screen>
sizing:
  mysql:
    count: 3
  uaa:
    count: 2
</screen>

<para>
    The second example is for SCF, <filename>scf-sizing.yaml</filename>.
</para>
<screen>
sizing:
  api:
    count: 6
  cc_clock:
    count: 3
  cc_uploader:
    count: 3
  cc_worker:
    count: 6
  cf_usb:
    count: 3
  consul:
    count: 1
  diego_access:
    count: 3
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  diego_locket:
    count: 3
  doppler:
    count: 4
  loggregator:
    count: 7
  mysql:
    count: 3
  nats:
    count: 2
  nfs_broker:
    count: 3
  postgres:
    count: 3
  router:
    count: 13
  routing_api:
    count: 3
  syslog_adapter:
    count: 7
  syslog_rlp:
    count: 7
  syslog_scheduler:
    count: 3
  tcp_router:
    count: 3
</screen>

<para>
    After creating your configuration files, follow the steps in 
    <xref linkend="sec.cap.configure-prod"/> until you get to
    <xref linkend="sec.cap.install-uaa-prod"/>. Then deploy UAA with this command:
</para>

<screen>    
&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml
</screen>

<para>
    When the status shows RUNNING for all of the UAA nodes, deploy SCF with 
    these commands:
</para>
<screen>
&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
 -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}') | awk {'print $1'})

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
 -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>

<para>
    The HA pods with the following roles will enter in both passive and ready 
    states; there should always be at least one pod in each role that is ready.
</para>
    <itemizedlist>
	 <listitem>
         <para>
     diego-brain
     </para>
     </listitem>
     <listitem>
         <para>
     diego-database
     </para>
     </listitem>
     <listitem>
         <para>
     routing-api
     </para>
     </listitem>
    </itemizedlist>
    
    <para>
        You can confirm this by looking at the logs inside the container. Look
        for <literal>.consul-lock.acquiring-lock</literal>.
    </para>
    
<!-- TODO commenting this out until 1.2 release. cs june 28 2018, then revisiting
    for correct information
    <para>
        Some roles cannot be scaled. <command>mysql_proxy</command> needs a 
        proper active/passive configuration. <command>tcp_router</command> has 
        no mechanism for exposing ports correctly. <command>blobstore</command> 
        needs shared volume support and an active/passive configuration.
    </para> -->
    
    <!-- this will change in 1.2 release  cs june 28 2018 -->   
    <para>
Some roles follow an active/passive scaling model, meaning all pods except 
the active one will be shown as NOT READY by &kube;. This is appropriate and 
expected behavior.
</para>
</sect2>

<sect2 xml:id="sec.cap.ha-prod-upgrade">
    <title>Upgrading a non-&ha; Deployment to &ha;</title>
    <para>
        You may make a non-&ha; deployment highly available by upgrading with
        &helm;:
    </para>
    <screen>
&prompt.user;helm upgrade suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml 

&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
 -o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')| awk {'print $1'})

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
 -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm upgrade suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    This may take a long time, and your cluster will be unavailable until the
    upgrade is complete.
</para>
</sect2>
</sect1>
</chapter>

<!-- meaningless comment to trigger new travis/CI build on github -->
