<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-production"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&cap; Production Installation with &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     ==== TODO rewrite this chapter ===
 </para>
 <para>    
 A basic &productname; production deployment with &ha; requires
     at least seven nodes: one &ses; server, one &suse; &caasp; admin node, two
     &kube; masters, and three &kube; minions, plus a DNS/DHCP server. This is a 
     bare minimum, and actual requirements depend on your workloads. The example
     configuration in this chapter provides &ha; only for the &cap; cluster, and
     not for &caasp; or &ses;.
 </para>
 
<sect1 xml:id="sec.cap.prereqs-production">
 <title>Prerequisites to Installing &productname;</title>
  <variablelist>
	<varlistentry>
	  <term>
	    Install &suse; &caasp;
	  </term>
	<listitem>
        <para>
        After installing &caasp;, check the box in Velum to install Tiller 
        (Helm's server component).
        </para>
        <para>
        Seven physical or virtual nodes: one admin, 
        two Kubernetes masters, and three Kubernetes minions. (See the 
        <link xlink:href="https://www.suse.com/documentation/suse-caasp-1/">CaaSP 
        documentation</link>). You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        minions will download ~10GB of Docker images each.
        </para>
        <para>
        System requirements depend on your workloads; the &kube; nodes should 
        have a minimum of 60GB of storage, and 40GB for the admin node.
	  </para>
      <para>
         Any quad-core AMD64/Intel* EM64T processor, and a minimum of 8GB of
         memory per node.
      </para>
      <para>
          The easiest way to create the Kubernetes nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-1/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>. When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-1/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
              Bootstrapping the Cluster</link> you can proceed to 
          the next steps.
      </para>
	</listitem>
	</varlistentry>

	<varlistentry>
	  <term>
          Install &ses;
      </term>
      <listitem>
      <para>
          See the <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/">&ses; Deployment Guide</link> for instructions.
      </para>
      	</listitem>
	</varlistentry>
    
        <varlistentry>
	  <term>
          Set up name services for &cap;
      </term>
      <listitem>
 <para>
     &productname; needs its own domain and network, and your workloads will 
     also need their own domains and networks.
 </para>
 <para>
     Note that the Salt manager on &caasp; uses <filename>/etc/hosts</filename> 
     to create the internal &caasp; network, infra.caasp.local. This is 
     exclusively for &caasp; internal use, and you will want to avoid address
     collisions.
 </para>
</listitem>
</varlistentry>    
<varlistentry>
	  <term>
          Install &suse; &cap;
      </term>
      <listitem>
 <para>
     Installing &productname; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, install the Helm charts package on your Kubernetes master
     with Zypper, and then use Helm to install the required Kubernetes 
     applications to set up &productname;.
 </para>

   </listitem>
  </varlistentry>
 </variablelist>
</sect1>  

<sect1  xml:id="sec.cap.install-production">
    <title>Install Helm</title>
<!-- TODO update when real package & registry are available -->
<para>
    TODO update when real package and registry are available
</para>
<para>
    Install the Helm charts package with Zypper on the kube-master, and then 
    initialize and install Tiller:
    </para>
    <screen>
&prompt.root;zypper in helmpkg
&prompt.root;helm init --client-only
</screen>    
</sect1>
<!-- not needed; leaving in until I have confirmation - cs
<para>
    <filename>helmpkg</filename> installs the following files and directories in 
    <filename>/root/deploy</filename>:
</para>
    <screen>
&prompt.root;ls -c1 deploy/*
deploy/cert-generator.sh

deploy/certs:
scf-cert-values.yaml
uaa-cert-values.yaml

deploy/scripts:
certstrap
generate-scf-certs.sh
generate-uaa-certs.sh

deploy/kube:
uaa
cf

deploy/helm:
uaa
cf
</screen>
</sect1>

<sect1  xml:id="sec.cap.certs-production"> 
    <title>Create SSL Certificates</title>
    <para>
     The Helm package includes a helper script for generating SSL
     certificates. Change to the <filename>deploy</filename> directory and create the 
        <filename>certs</filename> directory:
    </para>
    <screen>
&prompt.root;cd deploy
&prompt.root;mkdir certs
</screen>
<para>
    Run the <command>cert-generator.sh</command> script to generate the required
    custom SSL certificates. Replace example.com with your domain:
</para>
    <screen>
&prompt.root;sh cert-generator.sh -d <replaceable>example.com</replaceable> -n scf -o certs     
    </screen>
</sect1> -->

<sect1 xml:id="sec.cap.configure-production">
    <title>Configuring the Minimal Production Deployment</title>
    <para>
        Create a new configuration file for Helm to use. In this example it is
        called <filename>scf-config-values.yaml</filename>, which matches
        the default Helm chart names. If you change the name, change the Helm 
        charts to match. In the following examples it is stored in the 
        <filename>deploy</filename> directory.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>changeme</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    # TODO is this the domain, or the FQDN of a Kube node?
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your UAA host,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>changeme</replaceable>

kube:
    # The IP address of the &kube; master
    # TODO is this correct?
    external_ip: <replaceable>&kubeip;</replaceable>
    # Run <command>kubectl get storageclasses</command>
    # to view your available storage classes
    storage_class:
        persistent: "persistent"
        shared: "shared"
    # The registry the images will be fetched from. 
    # The values below should work for
    # a default installation from the suse registry.
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap-beta"

    # Required for CaaSP 2
    auth: rbac        
        
    # example high availability configuration    
sizing:
  api:
    count: 2
  cf_usb:
    count: 2
  consul:
    count: 3
  diego_access:
    count: 2
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  doppler:
    count: 2
  etcd:
    count: 3
  loggregator:
    count: 2
  mysql:
    count: 3
  nats:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2 
    </screen>

<para>
    The HA pods with the following roles will enter in passive state, and won't 
    show a ready state:
</para>
    <itemizedlist>
	 <listitem>
         <para>
     diego-brain
     </para>
     </listitem>
     <listitem>
         <para>
     diego-database
     </para>
     </listitem>
     <listitem>
         <para>
     routing-api
     </para>
     </listitem>
     <listitem>
         <para>
     mysql
     </para>
     </listitem>
    </itemizedlist> 
<para>
Review the logs inside the container for confirmation; the logs will 
state <literal>.consul-lock.acquiring-lock.</literal>
</para>

<!-- TODO how to configure 2 Kube masters -->
<para>
TODO how to configure 2 Kube masters, explanation of HA configuration
</para>
</sect1>

<sect1  xml:id="sec.cap.setup-production">
    <title>Deploy with Helm</title>
 <para>
Run the following Helm commands to complete the deployment.
</para>
<screen>
&prompt.root;kubectl create namespace uaa
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -"
 
&prompt.root;helm install helm/uaa \
    --namespace uaa \
    --values certs/uaa-cert-values.yaml \
    --values scf-config-values.yaml

&prompt.root;kubectl create namespace scf
&prompt.root;kubectl get secret ceph-secret -o json --namespace default | \
 jq ".metadata.namespace = \"scf\"" | kubectl create -f -

&prompt.root;helm install helm/cf \
    --namespace scf \
    --values certs/scf-cert-values.yaml \
    --values scf-config-values.yaml 
    </screen>

<para>
    Now sit back and wait for everything to be ready. Run this command to watch
    the pods come online:
</para>
    <screen>
&prompt.root;watch -c 'kubectl get pods --all-namespaces'
</screen>

<para>
You should see output similar to this (which is shortened for this example):
</para>
<screen>
NAMESPACE     NAME                           READY     STATUS              RESTARTS   AGE
kube-system   kube-dns-2830115714-tdl38      3/3       Completed           18         1m
kube-system   tiller-deploy-833494864-33ndq  0/1       ContainerCreating   0          1m
scf           api-0                          0/1       ContainerCreating   0          1m
scf           api-worker-3486565911-dptbh    0/1       ContainerCreating   0          1m
scf           blobstore-0                    0/1       Pending             0          1m
</screen>
</sect1>

<sect1 xml:id="sec.cap.stratos-2">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos on your &kube; master with 
        Helm. First add the Stratos repository:
    </para>        
        <screen>
&prompt.root;helm repo add stratos-ui https://suse.github.io/stratos-ui
</screen>
    <para>
        Change to the directory where your <filename>scf-config-values.yaml</filename>
        is stored, and run these commands:
    </para>
        <screen>
&prompt.root;helm install stratos-ui/console \
    --namespace stratos \
    --values scf-config-values.yaml 
</screen>
<para>
    Point a Web browser to your domain at port 8443, for example 
    https://host:&exampledomain;:8443, and log in with the &cap; 
    credentials you created in <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
</sect1>
</chapter>