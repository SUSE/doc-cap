<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal &productname; Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     A production deployment of &productname; requires a significant number of
     physical or virtual nodes. For testing and learning, you can set up a minimal &productname; four-node deployment on a single workstation in a hypervisor 
     such as KVM or VirtualBox. This extremely minimal deployment uses your 
     hypervisor's name services and networking instead of a separate name server, 
     and &kube;'s hostpath storage type instead of a storage server such as &ses;.
 </para>
 <para>
     This minimal four-node deployment will run on a minimum of 32GB system 
     memory, though more memory is better. This is enough to test setting up 
     and configuring &suse; &caasp; and &cap;, and to run one or two lightweight 
     workloads. If you wish to test with your own name server or storage, 
     these should be on separate hosts from your &cap; host.
 </para>
 <para>
     After you have set up your cluster you will administer it remotely from your
     host workstation, using tools such as the 
     <link xlink:href="https://docs.helm.sh/">Helm package manager for
         Kubernetes</link>, and the Kubernetes command-line tool 
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/kubectl">
         kubectl</link>.
 </para>
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
 	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, 
        one Kubernetes master, and two Kubernetes minions. You also need an Internet connection, as the installer downloads additional packages, and the &kube;
        minions will each download ~10GB of Docker images. ==TODO howto set up
        cache to reuse downloads==
        </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any quad-core AMD64/Intel EM64T processor, 8GB of memory per node, and 
        500GB of storage. 32GB of system memory will run a four-node deployment, 
        though more memory is better. Allocate a minimum of 60GB of storage for 
        each of the &kube; nodes, and 40GB is adequate for the admin node.  
      </para>
    </listitem>  
  </varlistentry>
<varlistentry>
 <term>
	    Install &suse; &caasp; 2
  </term>
	<listitem>
        <para>
            Install &suse; &caasp; 2 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link>. 
            When you reach the step where you log
            into the Velum Web interface, check the box to install &tiller; (Helm's 
            server component).  
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install &tiller;</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
    <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the networks that are exclusive to the internal cluster
      communications. They are not externally accessible, and you must avoid
      address collisions.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
  <para>
          The easiest way to create the &kube; nodes is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/
      book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>.
  </para>
  <para>
          When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/
      book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
    Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file, and 
save it as <filename>~/.kube/config</filename> on your host workstation.
      </para>
      <figure xml:id="fig.cap.kubectl-download">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
  <term>Install kubectl</term>          
  <listitem>
 <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your host workstation. After installation, run this command to verify
     that it is installed, and that is communicating correctly with your cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                        STATUS                     ROLES     AGE  VERSION
b70748d.infra.caasp.local   Ready                      &lt;none>    4d   v1.7.7
cb77881.infra.caasp.local   Ready,SchedulingDisabled   &lt;none>    4d   v1.7.7
d028551.infra.caasp.local   Ready                      &lt;none>    4d   v1.7.7
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install Helm
      </term>
      <listitem>
 <para>
     Installing &suse; &cap; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will use the Helm client on your host workstation 
     to install the required Kubernetes applications to set up &cap;, and to 
     administer your cluster remotely.
 </para>
      <warning>
        <title>Initialize Only the Helm Client</title>
        <para>When you initialize Helm on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the CaaSP installation. You do not want two &tiller;
            instances. Also make sure to install the &helm; version that matches
            your &tiller; version.
    </para>
</warning>
 <para>
     See the 
     <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         Helm Quickstart Guide</link> for installation instructions and basic 
     usage examples. You should match the Helm version with the version of &tiller; 
     that is running on your cluster. To get your &tiller; version, log into your
     &kube; master and query the logs.
 </para>
 <screen>
&prompt.root;kubectl logs -l name=tiller --namespace=kube-system | grep "Starting &tiller;"
[main] 2018/01/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
</screen>
<para>
    Download the matching &helm; binary into any directory that is in your PATH
    on your workstation, such as your <filename>~/bin</filename> directory. Then 
    initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>

</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>
     <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        By default, &suse; &caasp; allows logging into the &kube; nodes only from
        the admin node. You can set up direct logins to your &kube; nodes from 
        your host workstation by copying the SSH keys from your admin node to your 
        &kube; nodes, and then you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier, especially if you want to use mouse 
        select/middle-click paste to copy commands and output from a virtual 
        terminal on your host workstation.
    </para>
</tip>
 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
&prompt.root;vim /etc/kubernetes/controller-manager 
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
&prompt.root;systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class named 
    <literal>hostpath</literal>: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", \
"metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -
</screen>
<para>
    Verify that your new storage class has been created. This command should
    also succeed on your workstation:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</link>
    for detailed information on storage classes.
</para>
</sect1> 
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a new configuration file for Helm to use. In this example it is
        called <filename>~/.kube/scf-config-values.yaml</filename>.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your 
    # User Account and Authentication (UAA) server,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The external IP address of your &kube; master
    external_ip: <replaceable>IP_ADDRESS</replaceable>
    
    # Your chosen storage class
    storage_class:
        persistent: "<replaceable>hostpath</replaceable>"
        shared: "shared"
        
    # The registry the images will be fetched from
    # This is the default
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap-beta"

    # Required for CaaSP 2
    auth: rbac        
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with Helm</title>
 <para>
Run the following Helm commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository
    </para>
</listitem>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        If you are using &ses;, copy the storage secret to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.addrepo">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse-cap</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse-cap</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse-cap        https://kubernetes-charts.suse.com/
</screen>
</sect2>

<sect2 xml:id="sec.cap.create-namespaces">
    <title>Create Namespaces</title>
<para>
    Use <command>kubectl</command> on your host workstation to create and verify 
    the UAA (User Account and Authentication) and SCF (SUSE Cloud Foundry) 
    namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
&prompt.user;kubectl get namespaces
NAME          STATUS    AGE
default       Active    4d
kube-public   Active    4d
kube-system   Active    4d
scf           Active    1m
uaa           Active    1m
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret">
    <title>Copy &ses; Secret</title>
<para>
    If you are using the hostpath storage class 
    (see <xref linkend="sec.cap.storageclass"/>
    there is no secret so skip this step.
</para>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa">
    <title>Install UAA</title>
<para>
    Use Helm to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install -f ~/.kube/scf-config-values.yaml --namespace uaa \
suse-cap/uaa --dry-run
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf">
    <title>Install SCF</title>
    <para>
       First copy your UAA certificate authority (CA) into your SCF namespace, 
       then use Helm to install &suse; &cf;:
    </para>
<screen>
&prompt.root;CA_CERT="$(kubectl get secret secret --namespace uaa -o \ 
jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.root;helm install helm/cf \
    --namespace scf \
    --values scf-config-values.yaml
    --set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for everything to be ready. Run this command to watch
    the pods come online:
</para>
    <screen>
&prompt.root;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
You should see output similar to this (which is shortened for this example):
</para>
<screen>
NAMESPACE     NAME                           READY  STATUS             RESTARTS AGE
kube-system   kube-dns-2830115714-tdl38      3/3    Completed          18       1m
kube-system   tiller-deploy-833494864-33ndq  1/1    Completed          1        1m
scf           api-0                          0/1    ContainerCreating  0        1m
scf           api-worker-3486565911-dptbh    0/1    ContainerCreating  0        1m
scf           blobstore-0                    0/1    Pending            0        1m
</screen>
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos on your &kube; master with 
        Helm. Start by preparing the environment:
    </para>
    <screen>        
&prompt.root;kubectl create namespace stratos-cap
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "stratos-cap"/' | \
kubectl create -f -       
</screen>
<para>
    Add the Stratos repository:
</para>
<screen>
&prompt.root;helm repo add stratos-ui https://suse.github.io/stratos-ui
</screen>
<para>
    Verify that the repo was added successfully:
</para>
<screen>
&prompt.root;helm search console 
NAME                    VERSION DESCRIPTION                                  
stratos-ui/console      0.9.7   A Helm chart for deploying Stratos UI Console
</screen>   
<para>
   Install the Stratos console with this command:
</para>
<screen>
&prompt.root;helm install stratos-ui/console \
    --namespace stratos-cap \
    --values scf-config-values.yaml
    --name <replaceable>my-console</replaceable> 
</screen>
<para>
Use Helm to check installation status:
</para>
<screen>
&prompt.root;helm status <replaceable>my-console</replaceable>    
</screen>
<para>
    When it reaches the DEPLOYED state, get its IP address and port number:
</para>
<screen>
&prompt.root;helm status <replaceable>my-console</replaceable> | grep ui-ext
console-ui-ext 10.0.0.162 192.168.77.1  80:30933/TCP,443:30941/TCP  1m 
</screen>
<para>
    In this example, pointing your web browser to https://192.168.77.1:30941 
    opens the console. If you see a certificate warning you may safely
    ignore it. Log in with the &cap; credentials you created in 
    <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure> 
</sect1>
</chapter>