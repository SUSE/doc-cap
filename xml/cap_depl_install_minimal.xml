<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     A production deployment of &productname; requires a 
     significant number of physical or virtual hosts. For testing and learning, 
     you can set up a minimal four-host deployment of &scf; on &suse; &caasp; 
     on a single workstation in a hypervisor such as KVM or VirtualBox. This 
     extremely minimal deployment uses &kube;' hostpath storage type instead of 
     a storage server, such as &ses;. You must also provide DNS, DHCP, and a 
     network space for your cluster. KVM and VirtualBox include name services 
     and network management. <xref linkend="fig.cap.min-arch"/> illustrates
     the layout of a physical minimal test installation with an external 
     administration workstation and DNS/DHCP server. Access to the cluster is
     provided by the UAA (User Account and Authentication) server on worker 1.
 </para>
  
 <figure xml:id="fig.cap.min-arch">
  <title>Minimal Network Architecture</title>
  <mediaobject>
    <imageobject role="fo">
      <imagedata fileref="cap-arch-minimal.png" width="65%"/>
    </imageobject>
    <imageobject role="html">
      <imagedata fileref="cap-arch-minimal.png"/>
    </imageobject>
    <textobject role="description">
      <phrase>network architecture of minimal test setup</phrase>
    </textobject>
  </mediaobject>
</figure>  
  
 <para>
     This minimal four-node deployment will run on a minimum of 32GB host system 
     memory, though more memory is better. 32GB is enough to test setting up 
     and configuring &suse; &caasp; and &scf;, and to run a few lightweight 
     workloads. You may also test connecting external servers with your cluster, 
     such as a separate name server, a storage server (e.g. &ses;), &scc;, or 
     &smtool;. You must be familiar with installing and configuring &caasp; (see
     the <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/index.html">&suse; &caasp; 2 Deployment Guide</link>).
 </para>
 <para>
     After you have installed &caasp; you will install and administer 
     &scf; remotely from your host workstation, using tools such as the 
     <link xlink:href="https://docs.helm.sh/">&helm; package manager for
         Kubernetes</link>, and the Kubernetes command-line tool 
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/kubectl">
         kubectl</link>.
 </para>
   <warning>
        <title>Limitations of minimal test environment</title>
        <para>
        This is a limited deployment that is useful for testing basic 
        deployment and functionality, but it is NOT a production system, and
        cannot be upgraded to a production system. Its reduced complexity allows 
        basic testing, it is portable (on laptops with enough memory), and is 
        useful in environments that have resource constraints.
    </para>
</warning>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
 
 <important>
     <title>You must be familiar with &suse; &caasp;</title>
     <para>
        Setting up &suse; &caasp; correctly, and knowledge of basic administration is 
        essential to a successful &productname; deployment. See
     the <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/index.html">&suse; &caasp; 2 Deployment Guide</link>
 </para>
</important>
 	  <para>
	    &caasp; requires a minimum of four physical or virtual hosts: one admin, 
        one &kube; master, and two &kube; workers. You also need an 
        Internet connection, as the installer has an option to download updates 
        during installation, and the &kube; workers will each download ~10GB of 
        Docker images.
        </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any AMD64/Intel EM64T processor with at least 8 virtual or physical 
        cores. This table describes the minimum requirements per node. 
        </para>
<informaltable>
  <tgroup cols="4">
    <thead>
      <row>
        <entry>Node</entry>
        <entry>CPU</entry>
        <entry>RAM</entry>
        <entry>Disk</entry>
    </row>
</thead>
<tbody>
    <row>
    <entry>&caasp; Dashboard</entry>
    <entry>1</entry>
    <entry>8GB</entry>
    <entry>40GB</entry>
    </row>

    <row>
    <entry>&caasp; Master</entry>
    <entry>2</entry>
    <entry>8GB</entry>
    <entry>40GB</entry>
    </row>

    <row>
    <entry>&caasp; Workers</entry>
    <entry>2</entry>
    <entry>16GB</entry>
    <entry>60GB</entry>
    </row>
</tbody>
</tgroup>
</informaltable>
</listitem>  
</varlistentry>
  <varlistentry>
      <term>Network and Name Services</term>
     <listitem>
         <para>
         You must provide DNS and DHCP services, either via your hypervisor, or 
         with a separate name server. Your cluster needs its own domain. Every 
         node needs a hostname and a fully-qualified domain name, and should all 
         be on the same network. By default, the &caasp; installer requests a 
         hostname from any available DHCP server. When you install the admin 
         server you may adjust its network settings manually, and should give it
         a hostname, a static IP address, and specify which name server to use
         if there is more than one.
     </para>
     <para>         
         &caasp; supports multiple methods for installing the &kube; workers. We 
         recommend using &ay;, and then when you deploy the &kube; workers 
         you will create their hostnames with a kernel boot option.         
     </para>
     <para>
         After your &kube; nodes are running select one &kube; worker to act as 
         the external access point for your cluster and map your domain name to 
         it. On production clusters it is a common practice to use wildcard DNS,
         rather than trying to manage DNS for hundreds or thousands of 
         applications. Map your domain wildcard to the IP address of the &kube; 
         worker you selected as the external access point to your cluster.
  </para>
     </listitem>
      </varlistentry>
<varlistentry>
 <term>
	    Install &suse; &caasp; 2
  </term>
	<listitem>
        <para>
            Install &suse; &caasp; 2 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link>. 
            When you reach the step where you log
            into the Velum Web interface, check the box to install &tiller; (&helm;'s 
            server component).  
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install &tiller;</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
    <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the cluster and services networks that are exclusive to the internal 
      cluster communications. They are not accessible outside of the cluster. 
      You may change the default overlay network assignments to avoid address 
      collisions with your existing network.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
  <para>
          The easiest way to create the &kube; nodes is to use &ay; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with &ay;</link>. Pass in these kernel boot
          options to each worker: <literal>hostname</literal>, 
          <literal>netsetup</literal>, and the &ay; path, which you find in 
          Velum on the "Bootstrap your CaaS Platform" page.
  </para>
  
 <figure xml:id="fig.cap.autoyast">
   <title>Kernel boot options</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="autoyast.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure> 
  
  <para>
          When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
    Bootstrapping the Cluster</link> open a Web browser to the Velum Web interface. 
If you see a "site not available" or "We're sorry, but something went wrong" 
error wait a few minutes, then try again. Click the 
<guimenu>kubectl config</guimenu> button to download your new cluster's <filename>kubeconfig</filename> file. This takes you to a login screen; use the 
login you created to access Velum. Save the file as 
<filename>~/.kube/config</filename> on your host workstation. This file enables 
the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
    
<term>Install kubectl</term>  
  <listitem>
  <note>
        <title>Remote Cluster Administration</title>
        <para>
        You will administer your cluster from your host workstation, rather than 
        directly on any of your cluster nodes. The remote environment is 
        indicated by the unprivileged user Tux, while root prompts are on a 
        cluster host. There are few tasks that need to be performed directly on
        any of the cluster hosts.
    </para>
</note>
      <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your host workstation. After installation, run this command to verify
     that it is installed, and that it is communicating correctly with your 
     cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                         STATUS                     ROLES     AGE  VERSION
4a10db2c.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
87c9e8ff.infra.caasp.local   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
34ce7eb0.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Deploying &scf; is different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will install the &helm; client on your workstation 
     to install the required Kubernetes applications to set up &scf;, and to 
     administer your cluster remotely.
 </para>
 <para>
     Helm client version 2.6 or higher is required.
 </para>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the &caasp; installation. You do not want two &tiller;
            instances.
    </para>
</warning>
<para>
    If the Linux distribution on your workstation doesn't provide the correct 
    &helm; version, or you are using some other platform, see the 
    <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
    usage examples. Download the &helm; binary into any directory that 
    is in your PATH on your workstation, such as your <filename>~/bin</filename> 
    directory. Then initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>

</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create hostpath Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>
 <warning>
        <title>Using the hostpath storage type on &caasp;</title>
        <para>
        &caasp; is configured as a multi-node &kube; setup with a minimum of 
        one master and two workers. Hostpath provisioning on &caasp; 
        uses local storage on each of these nodes, therefore persistent 
        data stored will only be available locally on the &kube; nodes. This 
        impacts use cases where &scf; containers restart on a different &kube; 
        worker, for example in high availability setups or update tests. 
        If a container starts on a different worker than before it will miss 
        its persistent data, leading to various other side effects. In 
        addition, hostpath-provisioner uses the local root filesystem of the 
        &kube; node. If it runs out of disk space your &kube; node won't work 
        anymore.
    </para>
</warning>

 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
&prompt.root;vim /etc/kubernetes/controller-manager 
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
&prompt.root;systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class named 
    <literal>hostpath</literal>: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", "metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -

storageclass "hostpath" created
</screen>
<para>
    Verify that your new storage class has been created:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    Log into all of your &kube; nodes and create the
    <filename>/tmp/hostpath_pv</filename> directory, then set its permissions to 
    read/write/execute:
</para>
<screen>
&prompt.root;mkdir /tmp/hostpath_pv  
&prompt.root;chmod -R 0777 /tmp/hostpath_pv
</screen>
<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</link> for detailed information on storage classes.
</para>
     <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        By default, &suse; &caasp; allows logging into the &kube; nodes only from
        the admin node. You can set up direct logins to your &kube; nodes from 
        your workstation by copying the SSH keys from your admin node to your 
        &kube; nodes, and then you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier.
    </para>
</tip>
</sect1>

<sect1 xml:id="sec.cap.teststorageclass">
 <title>Test Storage Class</title>
 <para>
     See <xref linkend="sec.cap.teststorageclass-prod"/> to learn how to test
         that your storage class is correctly configured before you deploy
         &scf;.
 </para>
</sect1>
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a configuration file on your workstation for &helm; to use. 
        In this example it is
        called <filename>scf-config-values.yaml</filename>. (See the
        <link xlink:href="https://www.suse.com/releasenotes/">Release Notes</link>
        for information on configuration changes.)
    </para>
        <screen>
env:    
    # Enter the domain you created for your CAP cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # UAA host and port
    UAA_HOST: uaa.<replaceable>&exampledomain;</replaceable>
    UAA_PORT: 2793

kube:
    # # The IP address assigned to the kube node pointed to by the domain.
    external_ips: ["<replaceable>&kubeip;</replaceable>"]
    
    # Run kubectl get storageclasses
    # to view your available storage classes
    storage_class: 
        persistent: "hostpath"
        shared: "shared"
        
    # The registry the images will be fetched from. 
    # The values below should work for
    # a default installation from the SUSE registry.
    registry: 
        hostname: "registry.suse.com"
        username: ""
        password: ""
    organization: "cap"

    # Required for CaaSP 2
    auth: rbac 

secrets:
    # Create a password for your CAP cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable> 
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository
    </para>
</listitem>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        If you are using &ses;, copy the storage secret to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA secret and certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.addrepo-min">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME        URL                                             
stable      https://kubernetes-charts.storage.googleapis.com
local       http://127.0.0.1:8879/charts                    
suse        https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse
NAME          VERSION  DESCRIPTION                                  
suse/cf       2.8.0    A Helm chart for SUSE Cloud Foundry          
suse/console  1.1.0    A Helm chart for deploying Stratos UI Console
suse/uaa      2.8.0    A Helm chart for SUSE UAA
</screen>
</sect2>

<sect2 xml:id="sec.cap.create-namespaces">
    <title>Create Namespaces</title>
<para>
    Use <command>kubectl</command> on your host workstation to create and verify 
    the UAA (User Account and Authentication) and SCF (SUSE Cloud Foundry) 
    namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
 namespace "uaa" created
 
&prompt.user;kubectl create namespace scf
 namespace "scf" created
 
&prompt.user;kubectl get namespaces
NAME          STATUS    AGE
default       Active    27m
kube-public   Active    27m
kube-system   Active    27m
scf           Active    1m
uaa           Active    1m
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret">
    <title>Copy &ses; Secret</title>
<para>
    If you are using the hostpath storage class 
    (see <xref linkend="sec.cap.storageclass"/>)
    there is no secret so skip this step.
</para>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa">
    <title>Install UAA</title>
<para>
    Use &helm; to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \
--values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command. This will take time, possibly an hour or two, according to your
    hardware resources:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
When the status shows RUNNING for all of the UAA nodes, then proceed to the next
step.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf">
    <title>Install &scf;</title>
<para>
  First pass your UAA secret and certificate to SCF, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[*].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
    When all services are running you can use the &cf; command-line interface 
    to log in to &scf;. (See <xref linkend="sec.cap.install-scf-prod"/>.)
</para>    
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos-min">
    <title>Install the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. (See <xref linkend="sec.cap.stratos-prod"/>).
    </para>
  </sect1>

 <sect1 xml:id="sec.cap.update-min">
  <title>Updating &scf;, UAA, and Stratos</title> 
     <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. See <xref linkend="sec.cap.update"/>.
  </para> 
  <note>
      <title>No Upgrades with Hostpath</title>
      <para>
          Upgrades do not work with the hostpath storage type, as the
          required stateful data may be lost.
      </para>
  </note>
  
  </sect1>
  
  </chapter>
