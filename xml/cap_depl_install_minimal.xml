<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal &cap; Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     You can set up a minimal &productname; deployment in a hypervisor, such as 
     KVM or VirtualBox, on a laptop or PC for testing. You need a total of five 
     nodes: four for &suse; &caasp;, and one for &ses;. You also need to provide
     name services, either from a separate DNS/DHCP server, or use the
     <filename>/etc/hosts</filename> files on your &caasp; nodes. Your &cap; 
     cluster requires its own subnet and domain.    
 </para>
 <para> 
 <xref linkend="fig.cap.architecture"/>
shows a minimal deployment with &suse; &caasp;, &ses;, and &suse; &cap;. 
 </para>

    <figure xml:id="fig.cap.architecture">
   <title>Minimal &productname; test deployment</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="cap-arch-minimal.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites to Installing &productname;</title>
  <variablelist>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2.
	  </term>
	<listitem>
        <para>
            After installing &caasp;, check the box in Velum to install Tiller 
            (Helm's server component). 
        </para>
	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, one 
        Kubernetes master, and two Kubernetes minions. (See the 
        <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">CaaSP 
        documentation</link>). You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        minions will each download 8-10GB of Docker images.
        </para>
        <para>System requirements depend on the workload; for a test system 60GB 
        of storage should be adequate for each of the &kube; nodes, and 40GB for 
        the admin node.
	  </para>
      <para>
          Any quad-core AMD64/Intel* EM64T processor, and a minimum of 8GB of
          memory per node.
      </para>
      <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>. When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
              Bootstrapping the Cluster</link> you can proceed to 
          the next steps.
      </para>
	</listitem>
	</varlistentry>

	<varlistentry>
	  <term>
          Install &ses;
      </term>
      <listitem>
      <para>
          See the <link xlink:href="https://www.suse.com/documentation/suse-enterprise-storage-5/">&ses; Deployment Guide</link> for instructions, and see
          <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/integration.html">&suse; &caasp; Integration with &ses;</link> 
          for information on integrating &ses; with &caasp;.
      </para>
      	</listitem>
	</varlistentry>
    
    <varlistentry>
	  <term>
          Set up name services for &cap;
      </term>
      <listitem>
 <para>
     &productname; needs its own domain and network.You may use your own name 
     server, or add your &cap; hostnames and domain to the 
     <filename>/etc/hosts</filename> files on your 
     nodes. The Salt manager on &caasp; uses <filename>/etc/hosts</filename> to 
     create the internal &caasp; network, <literal>infra.caasp.local</literal>. 
     Do not use this domain for the &cap; cluster as it is exclusively for 
     &caasp; internal use.
 </para>

 </listitem>
 	</varlistentry>
	<varlistentry>
	  <term>
          Install &suse; &cap;
      </term>
      <listitem>
 <para>
     Installing &productname; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, install the Helm charts package on your Kubernetes master
     with Zypper, and then use Helm to install the required Kubernetes 
     applications to set up &productname;.
 </para>

 </listitem>
 	</varlistentry>
</variablelist>
 </sect1>
 
<sect1  xml:id="sec.cap.install">
    <title>Install Helm Charts</title>
<!-- TODO update when real package & registry are available -->
<para>
    TODO update when real package and registry are available
</para>
<para>
    Install the Helm package with Zypper on the &kube; master, and then initialize 
    Tiller:
</para>
    <screen>
&prompt.root;zypper in helmpkg
&prompt.root;helm init
</screen>    
<para>
    <filename>helmpkg</filename> installs the following files and directories in 
    <filename>/root/deploy</filename>:
</para>
    <screen>
&prompt.root;ls -c1 deploy/*
deploy/cert-generator.sh

deploy/certs:
scf-cert-values.yaml
uaa-cert-values.yaml

deploy/scripts:
certstrap
generate-scf-certs.sh
generate-uaa-certs.sh

deploy/kube:
uaa
cf

deploy/helm:
uaa
cf
</screen>
</sect1>

<sect1  xml:id="sec.cap.certs"> 
    <title>Create SSL Certificates</title>
    <para>
     The Helm package includes a helper script for generating SSL
     certificates. Change to the <filename>deploy</filename> directory and create the 
        <filename>certs</filename> directory:
    </para>
        <screen>
&prompt.root;cd deploy
&prompt.root;mkdir certs
</screen>

<para>
    Run the <command>cert-generator.sh</command> script to generate the required
    custom SSL certificates, using your &cap; cluster domain.
</para>
    <screen>
&prompt.root;sh cert-generator.sh -d <replaceable>example.com</replaceable> -n scf -o certs     
    </screen>
</sect1>

<sect1 xml:id="sec.cap.configure">
    <title>Configuring the &productname; Deployment</title>
    <para>
        Create a new configuration file for Helm to use. In this example it is
        called <filename>deploy/scf-config-values.yaml</filename>, which matches
        the default Helm chart names. If you change this, change the Helm charts
        to match.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your UAA host,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The IP address of the &kube; master
    external_ip: <replaceable>&kubeip;</replaceable>
    storage_class:
        persistent: fast
    </screen>
            
</sect1>

<sect1  xml:id="sec.cap.setup">
    <title>Deploy with Helm</title>
 <para>
Run the following Helm commands to complete the deployment.
</para>
<screen>
&prompt.root;kubectl create namespace uaa
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -"
 
&prompt.root;helm install helm/uaa \
    --namespace uaa \
    --values certs/uaa-cert-values.yaml \
    --values scf-config-values.yaml

&prompt.root;kubectl create namespace scf
&prompt.root;kubectl get secret ceph-secret -o json --namespace default | \
 jq ".metadata.namespace = \"scf\"" | kubectl create -f -

&prompt.root;helm install helm/cf \
    --namespace scf \
    --values certs/scf-cert-values.yaml \
    --values scf-config-values.yaml 
    </screen>
<para>
    Now sit back and wait for everything to be ready. Run this command to watch
    the pods come online:
</para>
    <screen>
&prompt.root;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
You should see output similar to this (which is shortened for this example):
</para>
<screen>
NAMESPACE     NAME                           READY     STATUS              RESTARTS   AGE
kube-system   kube-dns-2830115714-tdl38      3/3       Completed           18         1m
kube-system   tiller-deploy-833494864-33ndq  0/1       ContainerCreating   0          1m
scf           api-0                          0/1       ContainerCreating   0          1m
scf           api-worker-3486565911-dptbh    0/1       ContainerCreating   0          1m
scf           blobstore-0                    0/1       Pending             0          1m
</screen>
</sect1>

<sect1 xml:id="sec.cap.stratos">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos on your &kube; master with 
        Helm. First add the Stratos repository:
    </para>        
        <screen>
&prompt.root;helm repo add stratos-ui https://suse.github.io/stratos-ui
</screen>
    <para>
        Change to the directory where your <filename>scf-config-values.yaml</filename>
        is stored, and run these commands:
    </para>
        <screen>
&prompt.root;helm install stratos-ui/console \
    --namespace stratos \
    --values scf-config-values.yaml 
</screen>
<para>
    Point a Web browser to your domain at port 8443, for example 
    https://host:&exampledomain;:8443, and log in with your &suse; &caasp; 
    credentials. If you see an upgrade message, wait a few minutes and try
    again.
</para>
</sect1>
</chapter>