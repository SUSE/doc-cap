<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-production"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Production Installation with Optional &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>    
 A basic &productname; production deployment requires at least eight 
 nodes plus a storage backend: one &suse; &caasp; admin node, three &kube; 
 masters, three &kube; workers, a DNS/DHCP server, and a storage backend such as 
 &ses;. This is a bare minimum, and actual requirements are likely to be much 
 larger, depending on your workloads. You also need an external workstation for 
 administering your cluster. You may optionally make your &scf; instance 
 highly-available.
</para>
 <note>
        <title>Remote Administration</title>
        <para>
        You will run most of the commands in this chapter from a remote
        workstation, rather than directly on any of the &scf; nodes. These are
        indicated by the unprivileged user Tux, while root prompts are on a
        cluster node. There are few tasks that need to be performed directly on
        any of the nodes.
    </para>
</note>
 <para>
 The optional &ha; example in this chapter provides HA only for the &scf; 
 cluster, and not for &caasp; or &ses;. See 
  <xref linkend="sec.cap.ha-prod"/>.
 </para>
 
<sect1 xml:id="sec.cap.prereqs-prod">
  <title>Prerequisites</title>
  <para>
      Calculating hardware requirements is best done with an analysis of your 
      expected workloads, traffic patterns, storage needs, and application requirements. The following examples are minimums to deploy a
      running cluster, and any production deployment will likely require more.
  </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Minimum Hardware Requirements
	  </term>
	<listitem>
        <para>
          8GB of memory per &caasp; dashboard and &kube; master nodes.
        </para>
        <para>
          16GB of memory per &kube; worker.
      </para>
      <para>
          40GB disk space per &caasp; dashboard and &kube; master nodes.
      </para>
      <para>
         60GB disk space per &kube; worker.
     </para>
       </listitem>  
      </varlistentry>
      <varlistentry>
          <term>Network Requirements</term>
          <listitem>
          <para>
         Your &kube; cluster needs its own domain and network. Each node should 
         resolve to its hostname, and to its fully-qualified domain name. 
         Typically, a &kube; cluster sits behind a load balancer, which also 
         provides external access to the cluster. Another option is to use DNS 
         round-robin to the &kube; workers to provide external access. It is 
         also a common practice to create a wildcard DNS entry pointing to the 
         domain, e.g. *.&exampledomain;, so that applications can be deployed 
         without creating DNS entries for each application. This guide does not describe how to set up a load balancer or name services, as these 
         depend on customer requirements and existing network architectures.
     </para>
      </listitem>  
      </varlistentry>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2
	  </term>
	<listitem>
        <para>
            After installing <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link> and logging into the Velum Web interface, 
            check the box to install Tiller (&helm;'s server component). 
        </para>
<figure xml:id="fig.cap.install-tiller-prod">
   <title>Install Tiller</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the networks that are exclusive to the internal cluster
      communications. They are not externally accessible. You may assign 
      different networks to avoid address collisions.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
        <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with AutoYaST</link>. Set up &caasp; with one admin 
          node and at least three Kubernetes masters
        and three Kubernetes workers. You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        workers will each download ~10GB of Docker images.
        </para>
  <figure xml:id="fig.cap.select-master-nodes">
   <title>Assigning Roles to Nodes</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="select-master-nodes.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
      When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
              Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file. This
takes you to a login screen; use the login you created to access Velum. Save the
file as <filename>~/.kube/config</filename> on your workstation. This 
file enables the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download-prod">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download-prod.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
  <term>Install kubectl</term>          
  <listitem>
 <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your workstation. After installation, run this command to verify
     that it is installed, and that is communicating correctly with your cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                  STATUS                     ROLES     AGE  VERSION
b70748d.example.com   Ready                      &lt;none>    4h   v1.7.7
cb77881.example.com   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
d028551.example.com   Ready                      &lt;none>    4h   v1.7.7
[...]
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Deploying &scf; is different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will install the &helm; client on your workstation 
     to install the required Kubernetes applications to set up &scf;, and to 
     administer your cluster remotely.
 </para>
 <para>
     You should match the &helm; version with the version of &tiller; 
     that is running on your cluster. The &tiller; binary cannot report its
     version, and you need the version that is packaged inside the &tiller; 
     container. Run the following command from your workstation to query the
     logs:
 </para>
 <screen>
&prompt.user;kubectl logs -l name=tiller --namespace=kube-system | \
grep "Starting &tiller;"
[main] 2018/01/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
</screen>
<para>
   If the log gets overwritten and loses this information, the following command 
   queries the <command>rpm</command> package manager inside the container. Use
   this only for &caasp;/&cap; installations:
</para>
<screen>
&prompt.user;kubectl exec -it $(kubectl get pods -n kube-system | \
awk '/tiller/{print$1}') -n kube-system -- rpm -q helm
helm-2.6.1-1.6.x86_64
</screen>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the &caasp; installation. You do not want two &tiller;
            instances. Also make sure to install the &helm; version that matches
            your &tiller; version.
    </para>
</warning>
<para>
    If the Linux distribution on your workstation doesn't provide the correct 
    &helm; version, or you are using some other platform, see the 
    <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
    usage examples. Download the matching &helm; binary into any directory that 
    is in your PATH on your workstation, such as your <filename>~/bin</filename> 
    directory. Then initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>
</sect1>  

<sect1 xml:id="sec.cap.storageclass-prod">
 <title>Choose Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. Your available storage classes depend on which
     storage cluster you are using 
     (&ses; users, see <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/integration.html">&suse; &caasp; Integration with SES</link>). 
     After connecting your storage backend use <command>kubectl</command>
     to see your available storage classes:
 </para>
 <screen>
&prompt.user;kubectl get storageclasses
</screen>
<para>
    See <xref linkend="sec.cap.configure-prod"/> to learn where to configure your 
    storage class for &scf;. See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link>
    for detailed information on storage classes.
</para>
</sect1>

<sect1 xml:id="sec.cap.configure-prod">
    <title>Configuring the &scf; Production Deployment</title>
    <para>
        Create a new configuration file on your workstation for &helm; to use. 
        In this example it is
        called <filename>scf-config-values.yaml</filename>.
    </para>
    <screen>
env:
    # Create a password for your &scf; instance
    CLUSTER_ADMIN_PASSWORD: <replaceable>changeme</replaceable>
    
    # Enter the domain you created for your &kube; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>changeme</replaceable>

kube:
    # The external IP address of your &kube; cluster
    external_ip: <replaceable>&kubeip;</replaceable>
    
    # Your chosen storage class
    storage_class:
        persistent: <replaceable>"persistent"</replaceable>
        shared: "shared"
    
    # The Docker registry the images will be fetched from
    # This is the default
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap"

    # Required for CaaSP 2
    auth: rbac
</screen>        
</sect1>

<sect1  xml:id="sec.cap.helm-deploy-prod">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository 
    </para>
</listitem>
  <listitem>
    <para>
        Create the UAA and SCF namespaces
    </para>
</listitem>
  <listitem>
    <para>
        Copy the storage secret of your storage cluster to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Deploy UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy the UAA certificate to the SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Deploy &scf;
    </para>
  </listitem>  
</itemizedlist>  
</sect1>

<sect1 xml:id="sec.cap.addrepo-prod">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse
NAME         VERSION   DESCRIPTION                                  
suse/cf      2.6.1     A Helm chart for SUSE Cloud Foundry          
suse/console 1.0.0     A Helm chart for deploying Stratos UI Console
suse/uaa     2.6.1     A Helm chart for SUSE UAA   
</screen>
</sect1>

<sect1 xml:id="sec.cap.create-namespaces-prod">
    <title>Create Namespaces</title>
<para>
    Create the UAA (User Account and Authentication) and 
    SCF (SUSE Cloud Foundry) namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
</screen>
</sect1>

<sect1 xml:id="sec.cap.copy-secret-prod">
    <title>Copy &ses; Secret</title>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect1>

<sect1 xml:id="sec.cap.install-uaa-prod">
    <title>Deploy UAA</title>
<para>
    Use &helm; to deploy the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install suse/uaa \
--name uaa \
--namespace uaa \ 
--values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
Pressing <keycombo>
    <keycap function="control"/>
    <keycap>C</keycap>
     </keycombo>
     stops it. When the status shows RUNNING for all of the UAA nodes, proceed 
     to deploying &suse; &cf;.
</para>
</sect1>   

<sect1 xml:id="sec.cap.install-scf-prod">
    <title>Deploy &scf;</title>
<para>
  First copy your UAA certificate authority (CA) into a shell variable, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;CA_CERT="$(kubectl get secret secret --namespace uaa -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
&prompt.user;helm install suse/cf \
--name scf \
--namespace scf \
--values scf-config-values.yaml \
--set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
    When all services are running use the &cf; command-line interface 
to log in to &suse; &cf; to deploy and manage your applications. (See
<xref linkend="sec.cap.cf-cli"/>)
</para>
</sect1>

<sect1 xml:id="sec.cap.cf-cli">
<title>Deploying and Managing Applications with the &cf; Client</title>
    <para>
    The &cf; command line interface (cf-cli) is for deploying and managing your
    applications. You may use it for all the orgs and spaces that you are a 
    member of. Install the client on a workstation for remote administration of 
    your &suse; &cf; instances. 
    </para>
    <para>
The complete guide is at
    <link xlink:href="https://docs.cloudfoundry.org/cf-cli/">Using the Cloud 
        Foundry Command Line Interface</link>, and source code with a 
    demo video is on GitHub at 
    <link xlink:href="https://github.com/cloudfoundry/cli/blob/master/README.md">Cloud Foundry CLI</link>.
</para>
    <para>
      The following examples demonstrate some of the commonly-used commands.
      The first task is to log into your new &suse; &cf; instance. When your
      installation completes it prints a welcome screen with the information
      you need to access it.
    </para>
    <screen>
       NOTES:
    Welcome to your new deployment of SCF.

    The endpoint for use by the `cf` client is
        https://api.example.com

    To target this endpoint run
        cf api --skip-ssl-validation https://api.example.com

    Your administrative credentials are:
        Username: admin
        Password: password

    Please remember, it may take some time for everything to come online.

    You can use
        kubectl get pods --namespace scf

    to spot-check if everything is up and running, or
        watch -c 'kubectl get pods --namespace scf'

    to monitor continuously.    
</screen>
<para>
    You can display this message anytime with this command:
</para>
<screen>
&prompt.user;helm status $(helm list | awk '/cf-([0-9]).([0-9]).*/{print$1}') | \
sed -n -e '/NOTES/,$p'
</screen>
<para>
You need to provide the API endpoint of your &scf; instance to log in. The API 
endpoint is the <envar>DOMAIN</envar> value you provided in 
<filename>scf-config-values.yaml</filename>, plus the 
<literal>api.</literal> prefix, as it shows in the above welcome screen. Set 
your endpoint, and use <command>--skip-ssl-validation</command> when you have 
self-signed SSL certificates. It asks for an email address, but you must enter 
<literal>admin</literal> instead (you cannot change this to a different username, 
though you may create additional users), and the password is the one you created 
in <filename>scf-config-values.yaml</filename>:
</para>
<screen>
&prompt.user;cf login --skip-ssl-validation  -a https://api.example.com 
API endpoint: https://api.example.com

Email> admin

Password> 
Authenticating...
OK

Targeted org system

API endpoint:   https://api.example.com (API version: 2.101.0)
User:           admin
Org:            system
Space:          No space targeted, use 'cf target -s SPACE'
</screen>

<para>
    <command>cf help</command> displays a list of commands and options.
    <command>cf help [command]</command> provides information on specific
    commands.
</para>
<para>
    You may pass in your credentials and set the API endpoint in a single command:
</para>
<screen>
&prompt.user;cf login -u admin -p password --skip-ssl-validation -a https://api.example.com    
</screen>        
<para>
    Log out with <command>cf logout</command>.
</para>
<para>
    View your current API endpoint, user, org, and space:
</para>
<screen>
&prompt.user;cf target  
</screen>
<para>
    Switch to a different org or space:
</para>
<screen>
&prompt.user;cf target -o <replaceable>org</replaceable>
&prompt.user;cf target -s <replaceable>space</replaceable>   
</screen>
<para>
  List all apps in the current space:  
</para>
<screen>
&prompt.user;cf apps   
</screen>
<para>
    Query the health and status of a particular app:
</para>
<screen>
&prompt.user;cf app <replaceable>appname</replaceable> 
</screen>
<para>
    View app logs. The first example tails the log of a running app. 
    The <command>--recent</command> option dumps recent logs instead of tailing,
    which is useful for stopped and crashed apps:
</para>
<screen>
&prompt.user;cf logs <replaceable>appname</replaceable>
&prompt.user;cf logs --recent <replaceable>appname</replaceable>
</screen>
<para>
    Restart all instances of an app:
</para>
<screen>
&prompt.user;cf restart <replaceable>appname</replaceable>
</screen>
<para>
    Restart a single instance of an app, identified by its index number, and
    restart it with the same index number:
</para>
<screen>
&prompt.user;cf restart-app-instance <replaceable>appname</replaceable> <replaceable>index</replaceable>
</screen>
<para>
    After you have set up a service broker (see TODO), create new services: 
</para>
<screen>
&prompt.user;cf create-service <replaceable>service-name</replaceable> <replaceable>default</replaceable> <replaceable>mydb</replaceable>
</screen>
<para>
 Then you may bind a service instance to an app:
</para>
<screen>
&prompt.user;cf bind-service <replaceable>appname</replaceable> <replaceable>service-instance</replaceable>
</screen>
<para>
    The most-used command is <command>cf push</command>, for pushing new apps and
    changes to existing apps.
</para>
<screen>
 &prompt.user;cf push <replaceable>new-app</replaceable> -b <replaceable>buildpack</replaceable>
</screen>
</sect1>
        
<sect1 xml:id="sec.cap.stratos-prod">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos with 
        &helm; after all of the UAA and SCF pods are running. Start by preparing 
        the environment:
    </para>
    <screen>        
&prompt.user;kubectl create namespace stratos
</screen>
<para>
If you are using &ses; as your storage backend, copy the secret into the Stratos
namespace. Skip this step if you are using the hostpath storage type.
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "stratos"/' | \
kubectl create -f -       
</screen>        
<para>       
You should already have the Stratos charts when you downloaded the SUSE charts
repository:
</para>        
<screen>
&prompt.user;helm search suse
NAME         VERSION DESCRIPTION                                  
suse/cf      2.6.1   A Helm chart for SUSE Cloud Foundry          
suse/console 1.0.0   A Helm chart for deploying Stratos UI Console
suse/uaa     2.6.1   A Helm chart for SUSE UAA 
</screen>
<para>
Install Stratos, and if you have not set a default storage class you must specify
it:
</para>
<screen>
&prompt.user;helm install suse/console \
    --namespace stratos \
    --values scf-config-values.yaml \
    --set storageClass=<replaceable>persistent</replaceable>
</screen>

<para>
    Monitor progress:
</para>
<screen>
$ watch -c 'kubectl get pods --namespace stratos'
 Every 2.0s: kubectl get pods --namespace stratos
 
NAME                               READY     STATUS    RESTARTS   AGE
console-0                          3/3       Running   0          30m
console-mariadb-3697248891-5drf5   1/1       Running   0          30m
</screen> 

<para>
    When all statuses show Ready, press 
    <keycombo>
    <keycap function="control"/>
    <keycap>C</keycap>
     </keycombo> to exit and to view your release information:
</para>
<screen>
NAME:   sanguine-heron
LAST DEPLOYED: Wed Feb 28 14:32:01 2018
NAMESPACE: stratos
STATUS: DEPLOYED

RESOURCES:
==> v1/Secret
NAME                           TYPE    DATA  AGE
sanguine-heron-secret          Opaque  2     2s
sanguine-heron-mariadb-secret  Opaque  2     2s

==> v1/PersistentVolumeClaim
NAME                          STATUS  VOLUME                                    CAPACITY  ACCESSMODES  STORAGECLASS  AGE
sanguine-heron-upgrade-volume Bound   pvc-334b6ed7-1cd7-11e8-ba13-90b8d0fcd216  20Mi    
</screen>        
<para>
    Another way to get the release name is with the <command>helm ls</command>
    command, then query the release name to get its IP address and port number:
</para>
<screen>
&prompt.user;helm ls   
NAME            REVISION UPDATED                  STATUS    CHART          NAMESPACE
sanguine-heron  1        Wed Feb 28 14:32:01 2018 DEPLOYED  console-1.0.0  stratos  
scf             1        Wed Feb 28 12:53:54 2018 DEPLOYED  cf-2.7.0       scf      
uaa             1        Wed Feb 28 12:31:18 2018 DEPLOYED  uaa-2.7.0      uaa

&prompt.user;helm status sanguine-heron
LAST DEPLOYED: Wed Feb 28 14:32:01 2018
NAMESPACE: stratos
STATUS: DEPLOYED

RESOURCES:
==> v1/Service
NAME                    CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
sanguine-heron-mariadb  172.24.241.109  &lt;none>        3306/TCP        35m
sanguine-heron-ui-ext   172.24.38.113   10.10.100.82  8443:31177/TCP  35m
[...]
</screen>
<para>
    In this example, pointing your web browser to https://10.10.100.82:8443 
    opens the console. Wade through the nag screens about the self-signed
    certificates and log in as <literal>admin</literal> with the password you 
    created in <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos-prod">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure> 
</sect1>

 <sect1 xml:id="sec.cap.update">
  <title>Updating &suse; &cf;, UAA, and Stratos</title>
  <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. Check for updates:
  </para>
<screen>
&prompt.user;helm repo update
Hang tight while we grab the latest from your chart repositories...
...Skip local chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "suse" chart repository
Update Complete. ⎈ Happy Helming!⎈   
</screen>      
<para>
      Get your release and chart names, and then apply the updates:
</para>
  <screen>
&prompt.user;helm list
NAME             REVISION UPDATED                 STATUS   CHART         NAMESPACE
bunking-manta    1        Tue Jan 9 08:29:57 2018 DEPLOYED uaa-2.0.2     uaa
irreverant-sloth 1        Tue Jan 9 09:36:29 2018 DEPLOYED cf-2.0.2      scf        
olfactory-turtle 1        Tue Jan 9 10:09:56 2018 DEPLOYED console-1.0.0 stratos

&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/             

&prompt.user;helm search suse
NAME          VERSION  DESCRIPTION                                  
suse/cf       2.6.1    A Helm chart for SUSE Cloud Foundry          
suse/console  1.0.0    A Helm chart for deploying Stratos UI Console
suse/uaa      2.6.1    A Helm chart for SUSE UAA

&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/uaa
&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/cf
&prompt.user;helm upgrade --recreate-pods <replaceable>release-name</replaceable> suse/console
</screen>
<para>
    In the above example, the <replaceable>release-names</replaceable> are found
    with the <command>helm list</command> command, bunking-manta, irreverant-sloth, and
    olfactory-turtle. Your release names will be different.
</para>

<!-- helm rollback does not work, so upgrades are one-way for now
-->
</sect1>
<sect1 xml:id="sec.cap.ha-prod">
  <title>Example &ha; Configuration</title>
  <para>
This example &ha; configuration needs two separate configuration files, one for 
UAA and one for SCF. The first example is for UAA, 
<filename>uaa-sizing.yaml</filename>.
</para>
<screen>
sizing:
  api:
    count: 2
  cf_usb:
    count: 2
  consul:
    count: 3
  diego_access:
    count: 2
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  doppler:
    count: 2
  etcd:
    count: 3
  loggregator:
    count: 2
  mysql:
    count: 1
  nats:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2 
</screen>

<para>
    The second example is for SCF, <filename>scf-sizing.yaml</filename>.
</para>
<screen>
sizing:
  api:
    count: 2
  cf_usb:
    count: 2
  consul:
    count: 3
  diego_access:
    count: 2
  diego_api:
    count: 3
  diego_brain:
    count: 2
  diego_cell:
    count: 3
  doppler:
    count: 2
  etcd:
    count: 3
  loggregator:
    count: 2
  mysql:
    count: 3
  nats:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2 
</screen>

<para>
    Follow the steps in <xref linkend="sec.cap.configure-prod"/> until you get to
    <xref linkend="sec.cap.install-uaa-prod"/>. Then deploy UAA with this command:
</para>

<screen>    
&prompt.user;helm install suse/uaa --name uaa \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml
</screen>

<para>
    When the status shows RUNNING for all of the UAA nodes, refer to 
    <xref linkend="sec.cap.install-scf-prod"/>, and then replace the command
    to deploy SCF with this command:
</para>
<screen>
&prompt.user;helm install suse/cf \
--name scf
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml
--set "env.UAA_CA_CERT=${CA_CERT}"
</screen>

<para>
    The HA pods with the following roles will enter in both passive and ready 
    states; there should always be at least one pod in each role that is ready.
</para>
    <itemizedlist>
	 <listitem>
         <para>
     diego-brain
     </para>
     </listitem>
     <listitem>
         <para>
     diego-database
     </para>
     </listitem>
     <listitem>
         <para>
     routing-api
     </para>
     </listitem>
    </itemizedlist>
    
    <para>
        You can confirm this by looking at the logs inside the container. Look
        for <literal>.consul-lock.acquiring-lock</literal>.
    </para>
    <para>
        Some roles cannot be scaled. mysql-proxy needs a proper active/passive 
        configuration. tcp-router has no mechanism for exposing ports correctly.
        blobstore needs shared volume support and an active/passive 
        configuration.
    </para>
    <para>
Some roles follow an active/passive scaling model, meaning all pods except 
the active one will be shown as NOT READY by &kube;. This is appropriate and 
expected behavior.
</para>

<sect2 xml:id="sec.cap.ha-prod-upgrade">
    <title>Upgrading a non-&ha; Deployment to &ha;</title>
    <para>
        You may make a non-&ha; deployment highly available by upgrading with
        &helm;:
    </para>
    <screen>
&prompt.user;helm upgrade suse/uaa --name uaa \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml 

&prompt.user;helm upgrade suse/cf \
--name scf
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml
--set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    This may take a long time, and your cluster will be unavailable until the
    upgrade is complete.
</para>
</sect2>
 </sect1>
</chapter>
