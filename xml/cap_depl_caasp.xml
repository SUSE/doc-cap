<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha-cap-depl-caasp"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deploying &productname; on &caasp;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <important>
  &readmefirst;
 </important>

 <para>
  &productname; supports deployment on &caasp;. &caasp; is an enterprise-class
  container management solution that enables IT and DevOps professionals to more
  easily deploy, manage, and scale container-based applications and services. It
  includes &kube; to automate lifecycle management of modern applications, and
  surrounding technologies that enrich Kubernetes and make the platform itself
  easy to operate. As a result, enterprises that use &caasp; can reduce
  application delivery cycle times and improve business agility. This chapter
  describes the steps to prepare a &productname; deployment on &caasp;. See
  <link xlink:href="&caasp-url;/"/>
  for more information on &caasp;.
 </para>

 <sect1 xml:id="sec-cap-prereqs-caasp">
  <title>Prerequisites</title>

  <para>
   The following are required to deploy and use &productname; on &caasp;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Access to one of the platforms listed at
     <link xlink:href="&caasp-url;/single-html/caasp-deployment/#_platform"/> to deploy &caasp;.
    </para>
   </listitem>
   <listitem>
    <para>
     A management workstation, which is used to deploy and control a &caasp;
     cluster, that is capable of running <literal>skuba</literal> (see
     <link xlink:href="https://github.com/SUSE/skuba"/> for installation
     instructions). The management workstation can be a regular
     desktop workstation or laptop running &sle; 15 SP1 or later.
    </para>
   </listitem>

   <!-- listitems: Install steps and links -->
   &cfcli-prereq;
   &kubectl-prereq;
   &jq-prereq;
   &curl-prereq;
   &sed-prereq;
  </itemizedlist>

  &min-deploy-note;
 </sect1>

 <sect1 xml:id="sec-cap-create-caasp-cluster">
  <title>Creating a &caasp; Cluster</title>

  <para>
   When creating a &caasp; cluster, take note of the following general
   guidelines to ensure there are sufficient resources available to run a
   &productname; deployment:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Minimum 2.3 GHz processor
    </para>
   </listitem>
   <listitem>
    <para>
     2 vCPU per physical core
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB RAM per vCPU
    </para>
   </listitem>
   <listitem>
    <para>
     Worker nodes need a minimum of 4 vCPU and 16 GB RAM
    </para>
   </listitem>
  </itemizedlist>
  <para>
   As a minimum, a &productname; deployment with a basic workload will require:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     1 master node
    </para>
    <itemizedlist>
     <listitem>
      <para>
       vCPU: 2
      </para>
     </listitem>
     <listitem>
      <para>
       RAM: 8 GB
      </para>
     </listitem>
     <listitem>
      <para>
       Storage: 60 GB (SSD)
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     2 worker nodes. Each node configured with:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       (v)CPU: 4
      </para>
     </listitem>
     <listitem>
      <para>
       RAM: 16 GB
      </para>
     </listitem>
     <listitem>
      <para>
       Storage: 100 GB
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Persistent storage: 40 GB
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For steps to deploy a &caasp; cluster, refer to the &caasp; Deployment Guide
   at <link xlink:href="&caasp-url;/single-html/caasp-deployment/"/>
  </para>
  <para>
   Before proceeding with the deployment, take note of the following to
   ensure the &caasp; cluster is suitable for a deployment of &productname;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Additional changes need to be applied to increase the maximum number of
     processes allowed in a container. If the maximum is insufficient,
     &productname; clusters with multiple application deployed will observe
     applications failing to start.
    </para>
    <para>
     Operators should be aware there are potential security concerns when raising
     the PIDs limit/maximum (fork bombs for example). As a best practice, these
     should be kept as low as possible. The example values are for guidance
     purposes only. Operators are encouraged to identify the typical PIDs usage
     for their workloads and adjust the modifications accordingly. If problems
     persist, these can be raised to a maximum of 32768 provided &productname;
     is the only workload on the &caasp; cluster.
    </para>
    <para>
     For &caasp; 4.5 clusters, apply the following changes directly to each node
     in the cluster.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Prior to rebooting/bootstrapping, modify
       <filename>/etc/crio/crio.conf.d/00-default.conf</filename> to increase the
       PIDs limit:
      </para>
<screen>&prompt.user;sudo sed -i -e 's|pids_limit = 1024|pids_limit = 3072|g' /etc/crio/crio.conf.d/00-default.conf</screen>
     </listitem>
    </itemizedlist>
    <para>
     For &caasp; 4.2 clusters, apply the following changes directly to each node
     in the cluster.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Prior to rebooting/bootstrapping, modify
       <filename>/etc/crio/crio.conf</filename> to increase the PIDs limit:
      </para>
<screen>&prompt.user;sudo sed -i -e 's|pids_limit = 1024|pids_limit = 3072|g' /etc/crio/crio.conf</screen>
     </listitem>
     <listitem>
      <para> 
       After rebooting/bootstrapping modify
       <filename>/sys/fs/cgroup/pids/kubepods/pids.max</filename> to increase the
       PIDs maximum:
      </para>
<screen>&prompt.user;sudo bash -c \"echo '3072' > /sys/fs/cgroup/pids/kubepods/pids.max\"</screen>
     </listitem>
    </itemizedlist>
    <para>
     Note that these modifications are not persistent and will need to be
     reapplied in the eventof a &caasp; node restart or update.
    </para>
   </listitem>
   <listitem>
    <para>
     At the cluster initialization step, <emphasis role="bold">do not</emphasis>
     use the <literal>--strict-capability-defaults</literal> option when running
    </para>
<screen>&prompt.user;skuba cluster init</screen>
    <para>
     This ensures the presence of extra CRI-O capabilities compatible with
     &docker; containers. For more details refer to the
     <link xlink:href="&caasp-url;/single-html/caasp-deployment/#_transitioning_from_docker_to_cri_o"/>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

  &install-helm;

 <sect1 xml:id="sec-cap-caasp-storage">
  <title>Storage Class</title>

  <para>
   In some &productname; instance groups, such as <literal>bits</literal>,
   <literal>database</literal> and <literal>singleton-blobstore</literal>
   require a storage class. To learn more about storage classes, see
   <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/"/>.
   Examples of provisioners include:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &ses; (see
     <link xlink:href="&caasp-url;/single-html/caasp-admin/#RBD-dynamic-persistent-volumes"/>)
    </para>
    <para>
     If you are using &ses; you must copy the Ceph admin secret to the
     <literal>kubecf</literal> namespaces used by &productname;:
    </para>
<screen>&prompt.user;kubectl get secret ceph-secret-admin --output json --namespace default | \
sed 's/"namespace": "default"/"namespace": "kubecf"/' | kubectl create --filename -
</screen>
   </listitem>
   <listitem>
    <para>
     Network File System (see
     <link xlink:href="https://kubernetes.io/docs/concepts/storage/volumes/#nfs"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   By default, &productname; will use the cluster&apos;s default storage class.
   To designate or change the default storage class, refer to
   <link xlink:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/"/>
   for instructions.
  </para>
  <para>
   In some cases, the default and predefined storage classes may not be suitable
   for certain workloads. If this is the case, operators can define their own
   custom StorageClass resource according to the specification at
   <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource"/>.
  </para>
  <para>
   With the storage class defined, run:
  </para>
<screen>&prompt.user;kubectl create --filename <replaceable>my-storage-class.yaml</replaceable></screen>
  <para>
   Then verify the storage class is available by running
  </para>
<screen>&prompt.user;kubectl get storageclass</screen>
  <para>
   If operators do no want to use the default storage class or one does not
   exist, a storage class <emphasis role="bold">must</emphasis> be specified by
   setting the <literal>kube.storage_class</literal> value in your
   &values-filename; configuration file to the name of the storage class as seen
   in this example.
  </para>
<screen>kube:
  storage_class: <replaceable>my-storage-class</replaceable>
</screen>
 </sect1>

 <sect1 xml:id="sec-cap-caasp-config">
  <title>Deployment Configuration</title>

  <para>
   &productname; is configured using &helm; values (see
   <link xlink:href="https://helm.sh/docs/chart_template_guide/values_files/"/>
   . &helm; values can be set as either command line parameters or using a
   <filename>values.yaml</filename> file. The following
   <filename>values.yaml</filename> file, called
   &values-filename; in this guide, provides an
   example of a &productname; configuration.
  </para>

  &values-file-changes;

  <para>
   Ensure <literal>system_domain</literal> maps to the load balancer configured for
   your &caasp; cluster (see
   <link xlink:href="&caasp-url;/single-html/caasp-deployment/#loadbalancer"/>).
  </para>

  &supported-domains;

  &example-config;
 </sect1>

 <!-- Begin optional features -->

 <sect1 xml:id="sec-cap-caasp-certificates">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &certificates;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-ingress">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &ingress-controller;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-affinity">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &affinity;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-high-availability">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &high-availability;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-external-blobstore">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &external-blobstore;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-external-database">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &external-database;
 </sect1>

 <!-- End optional features -->

 <sect1 xml:id="sec-cap-addrepo-caasp">
  <title>Add the &kube; Charts Repository</title>

  <para>
   Download the &suse; &kube; charts repository with &helm;:
  </para>

<screen>&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/</screen>

  <para>
   You may replace the example <replaceable>suse</replaceable> name with any
   name. Verify with <command>helm</command>:
  </para>

<screen>&prompt.user;helm repo list
NAME       URL
stable     https://kubernetes-charts.storage.googleapis.com
local      http://127.0.0.1:8879/charts
suse       https://kubernetes-charts.suse.com/
</screen>

  <para>
   List your chart names, as you will need these for some operations:
  </para>

&helm-search-suse;
 </sect1>

 <sect1 xml:id="sec-cap-cap-on-caasp">
  <title>Deploying &productname;</title>

  &kubecf-operator-versions;

  <sect2 xml:id="sec-cap-caasp-deploy-operator">
   &deploy-operator;
  </sect2>

  <sect2 xml:id="sec-cap-caasp-deploy-kubecf">
   <title>Deploy &kubecf;</title>
   <procedure>
    &deploy-kubecf;
    <step>
     <para>
      Create DNS A records for the public services.
     </para>
     &dns-mappings;
    </step>
    <step>
     <para>
      When all pods are fully ready, verify your deployment.
     </para>
     &cf-auth;
    </step>
   </procedure>
  </sect2>
 </sect1>

 <!-- Begin optional features -->

 <sect1 xml:id="sec-cap-caasp-ldap">
  <!-- Entry defined in xml/repeated-content-decl.ent -->
  &ldap;
 </sect1>

 <sect1 xml:id="sec-cap-caasp-add-capacity">
  <title>Expanding Capacity of a &cap; Deployment on &susecaaspreg;</title>

  <para>
   If the current capacity of your &cap; deployment is insufficient for your
   workloads, you can expand the capacity using the procedure in this section.
  </para>

  <para>
   These instructions assume you have followed the procedure in
   <xref linkend="cha-cap-depl-caasp"/> and have a running &cap;
   deployment on &susecaaspreg;.
  </para>

  <procedure>
   <step>
    <para>
     Add additional nodes to your &susecaaspreg; cluster as described in
     <link xlink:href="&caasp-url;/html/caasp-admin/#adding_nodes"/>.
    </para>
   </step>
   <step>
    <para>
     Verify the new nodes are in a <literal>Ready</literal> state before proceeding.
    </para>
<screen>&prompt.user;kubectl get nodes</screen>
   </step>
   <step>
    <para>
     Add or update the following in your
     &values-filename; file to increase the number of
     <literal>diego-cell</literal> in your &cap; deployment. Replace the
     example value with the number required by your workflow.
    </para>
<screen>sizing:
  diego_cell:
    instances: <replaceable>5</replaceable>
</screen>
   </step>
   <step>
    <para>
     Perform a <command>helm upgrade</command> to apply the change.
    </para>
<screen>&prompt.user;helm upgrade kubecf suse/kubecf \
--namespace <replaceable>kubecf</replaceable> \
--values &values-file; \
--version &kubecf_chart;
</screen>
   </step>
   <step>
    <para>
     Monitor progress of the additional <literal>diego-cell</literal> pods:
    </para>
<screen>&prompt.user;watch --color 'kubectl get pods --namespace kubecf'</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
