<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal &productname; Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     A production deployment of &productname; requires a significant number of
     physical or virtual nodes. For testing and learning, you can set up a minimal &productname; four-node deployment on a single PC by using a hypervisor such as 
     KVM or VirtualBox. This extremely minimal deployment uses your hypervisor's 
     name services and networking instead of a separate name server, and &kube;'s 
     hostpath storage type instead of a storage server such as &ses;.
 </para>
 <para>
     This minimal four-node deployment will run on a minimum of 32GB system 
     memory, though more memory is better. This is enough to test setting up 
     and configuring &suse; &caasp; and &cap;, and to run one or two lighweight 
     workloads. If you wish to test with your own name server or storage, 
     these should be on separate hosts from your &cap; host.
 </para>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any quad-core AMD64/Intel EM64T processor, 8GB of memory per node, and 
        500GB of storage. 32GB of system memory will run a four-node deployment, 
        though more memory is better. Allocate a minimum of 60GB of storage for 
        each of the &kube; nodes, and 40GB is adequate for the admin node.  
        </para>
      </listitem>  
      </varlistentry>
	<varlistentry>
	  <term>
	    Install &suse; &caasp; 2
	  </term>
	<listitem>
        <para>
            After installing &caasp; and logging into the Velum Web interface, 
            check the box to install Tiller (Helm's server component). 
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install Tiller</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>        
	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, one 
        Kubernetes master, and two Kubernetes minions. (See the 
        <link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">CaaSP 
        documentation</link>). You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        minions will each download ~10GB of Docker images.
        </para>
      <para>
          The easiest way to create the &kube; nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>. When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
              Bootstrapping the Cluster</link> you can proceed to 
          the next steps to install &productname;.
      </para>
	</listitem>
	</varlistentry>
 <varlistentry>
	  <term>
          Install &productname; 
      </term>
      <listitem>
 <para>
     Installing &suse; &cap; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will use Helm to install the required Kubernetes 
     applications to set up &cap;.
 </para>

  </listitem>
 </varlistentry>
</variablelist>
</sect1>
 
<sect1  xml:id="sec.cap.install">
    <title>Initiate Helm Client</title>
    <para>
        You should have already installed Tiller, Helm's server component, from 
        &suse; &caasp;'s Velum dashboard. Now you must initiate the Helm client, 
        the &kube; package manager, on your &kube; master. You cannot log into 
        the &kube; nodes directly, but must open an SSH session to your &kube; 
        master from your admin node.
    </para>
    <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        You can set up direct logins to your &kube; nodes from your host PC by 
        copying the SSH keys from your admin node to your &kube; nodes, and then 
        you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier, especially if you want to use mouse 
        select/middle-click paste to copy commands and output from a virtual 
        terminal on your host PC.
    </para>
</tip>
<para>
    Use this command to initialize Helm on your &kube; master. You must 
    initialize only the client, and not install Tiller a second time:
</para>
    <screen>
&prompt.root;helm init --client only
</screen>  
</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>
 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
<prompt>root # </prompt>systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", \
"metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -
</screen>
<para>
    Verify that your new storage class has been created:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link>
    for detailed information on storage classes.
</para>
</sect1> 
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a new configuration file for Helm to use. In this example it is
        called <filename>scf-config-values.yaml</filename>, and it is stored in 
        the <filename>/root/deploy</filename> directory. &caasp; has a read-only
        filesystem; <filename>/root/deploy</filename> is outside of this so
        you can use it like a normal read-write directory.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your 
    # User Account and Authentication (UAA) server,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The external IP address of your &kube; master
    external_ip: <replaceable>IP_ADDRESS</replaceable>
    
    # Run <command>kubectl get storageclasses</command>
    # to view your available storage classes
    storage_class:
    persistent: "<replaceable>hostpath</replaceable>"
        shared: "shared"
        
    # The registry the images will be fetched from. 
    # The values below should work for
    # a default installation from the &suse; registry.
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap-beta"

    # Required for CaaSP 2
    auth: rbac        
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with Helm</title>
 <para>
Run the following Helm commands to complete the deployment. There are five steps,
and they must be run in this order:
</para>
<itemizedlist>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        If you are using &ses;, copy the storage secret to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.create-namespaces">
    <title>Create Namespaces</title>
<para>
    Create the UAA (User Account and Authentication) and 
    SCF (SUSE Cloud Foundry) namespaces:
</para>    
    <screen>
&prompt.root;kubectl create namespace uaa
&prompt.root;kubectl create namespace scf
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret">
    <title>Copy &ses; Secret</title>
<para>
    If you are using the hostpath storage class 
    (see <xref linkend="sec.cap.storageclass"/>
    there is no secret so skip this step.
</para>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa">
    <title>Install UAA</title>
<para>
    Use Helm to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.root;helm install helm/uaa \
    --namespace uaa \
    --values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf">
    <title>Install SCF</title>
    <para>
       First copy your UAA certificate authority (CA) into your SCF namespace, 
       then use Helm to install &suse; &cf;:
    </para>
<screen>
&prompt.root;CA_CERT="$(kubectl get secret secret --namespace uaa -o \ jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.root;helm install helm/cf \
    --namespace scf \
    --values scf-config-values.yaml
    --set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for everything to be ready. Run this command to watch
    the pods come online:
</para>
    <screen>
&prompt.root;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
You should see output similar to this (which is shortened for this example):
</para>
<screen>
NAMESPACE     NAME                           READY  STATUS             RESTARTS AGE
kube-system   kube-dns-2830115714-tdl38      3/3    Completed          18       1m
kube-system   tiller-deploy-833494864-33ndq  1/1    Completed          1        1m
scf           api-0                          0/1    ContainerCreating  0        1m
scf           api-worker-3486565911-dptbh    0/1    ContainerCreating  0        1m
scf           blobstore-0                    0/1    Pending            0        1m
</screen>
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos">
    <title>Installing the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. Install Stratos on your &kube; master with 
        Helm. Start by preparing the environment:
    </para>
    <screen>        
&prompt.root;kubectl create namespace stratos-cap
&prompt.root;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "stratos-cap"/' | \
kubectl create -f -       
</screen>
<para>
    Add the Stratos repository:
</para>
<screen>
&prompt.root;helm repo add stratos-ui https://suse.github.io/stratos-ui
</screen>
<para>
    Verify that the repo was added successfully:
</para>
<screen>
&prompt.root;helm search console 
NAME                    VERSION DESCRIPTION                                  
stratos-ui/console      0.9.7   A Helm chart for deploying Stratos UI Console
</screen>   
<para>
   Install the Stratos console with this command:
</para>
<screen>
&prompt.root;helm install stratos-ui/console \
    --namespace stratos-cap \
    --values scf-config-values.yaml
    --name <replaceable>my-console</replaceable> 
</screen>
<para>
Use Helm to check installation status:
</para>
<screen>
&prompt.root;helm status <replaceable>my-console</replaceable>    
</screen>
<para>
    When it reaches the DEPLOYED state, get its IP address and port number:
</para>
<screen>
&prompt.root;helm status <replaceable>my-console</replaceable> | grep ui-ext
console-ui-ext 10.0.0.162 192.168.77.1  80:30933/TCP,443:30941/TCP  1m 
</screen>
<para>
    In this example, pointing your web browser to https://192.168.77.1:30941 
    opens the console. If you see a certificate warning you may safely
    ignore it. Log in with the &cap; credentials you created in 
    <filename>scf-config-values.yaml</filename>. 
    If you see an upgrade message, wait a few minutes and try again.
</para>
<figure xml:id="fig.cap.stratos">
  <title>Stratos UI Cloud Foundry Console</title>
  <mediaobject>
   <imageobject>
    <imagedata fileref="stratos.png" format="PNG" width="75%"/>
   </imageobject>
  </mediaobject>
</figure> 


</sect1>
</chapter>