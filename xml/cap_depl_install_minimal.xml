<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-minimal"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Minimal Installation for Testing</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
     A production deployment of &productname; requires a significant number of
     physical or virtual nodes. For testing and learning, you can set up a 
     minimal four-node deployment on a single workstation in a hypervisor 
     such as KVM or VirtualBox. This extremely minimal deployment uses your 
     hypervisor's DNS server and networking instead of a separate name server, 
     and &kube;'s hostpath storage type instead of a storage server such as &ses;.
 </para>
 <para>
     This minimal four-node deployment will run on a minimum of 32GB system 
     memory, though more memory is better. This is enough to test setting up 
     and configuring &suse; &caasp; and &susecf;, and to run one or two lightweight 
     workloads. If you wish to test with your own name server or storage, 
     these should be on separate hosts from your &cap; host.
 </para>
 <para>
     After you have set up your cluster you will administer it remotely from your
     host workstation, using tools such as the 
     <link xlink:href="https://docs.helm.sh/">&helm; package manager for
         Kubernetes</link>, and the Kubernetes command-line tool 
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/kubectl">
         kubectl</link>.
 </para>
   <warning>
        <title>Limitations of minimal test environment</title>
        <para>
        This is a limited deployment that is useful for testing basic 
        deployment and functionality, but it is NOT a production system, and
        cannot be upgraded to a production system. Its reduced complexity allows basic testing, it is portable (on laptops with enough memory), and is 
        useful in environments that have resource constraints.
    </para>
</warning>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites</title>
  <note>
        <title>Remote Administration</title>
        <para>
        You will run most of the commands in this chapter from your host 
        workstation, rather than directly on any of your cluster nodes. These are
        indicated by the unprivileged user Tux, while root prompts are on a
        node. There are few tasks that need to be performed directly on
        any of the nodes.
    </para>
</note>

 	  <para>
	    &caasp; requires a minimum of four physical or virtual nodes: one admin, 
        one Kubernetes master, and two Kubernetes minions. You also need an Internet connection, as the installer downloads additional packages, and the &kube;
        minions will each download ~10GB of Docker images.
        </para>
  <variablelist>
      <varlistentry>
	  <term>
	    Hardware requirements
	  </term>
	<listitem>
        <para>
        Any quad-core AMD64/Intel EM64T processor, 8GB of memory per node, and 
        500GB of storage. 32GB of system memory will run a four-node deployment, 
        though more memory is better. Allocate a minimum of 60GB of storage for 
        each of the &kube; nodes, and 40GB is adequate for the admin node.  
      </para>
    </listitem>  
  </varlistentry>
<varlistentry>
 <term>
	    Install &suse; &caasp; 2
  </term>
	<listitem>
        <para>
            Install &suse; &caasp; 2 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/">&caasp;</link>. 
            When you reach the step where you log
            into the Velum Web interface, check the box to install &tiller; (&helm;'s 
            server component).  
        </para>
<figure xml:id="fig.cap.install-tiller">
   <title>Install &tiller;</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
    <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These define 
      the networks that are exclusive to the internal cluster
      communications. They are not externally accessible, and you must avoid
      address collisions.
  </para>
  <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
  </para>
  <para>
          The easiest way to create the &kube; nodes is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_node_ay">
              Installation with AutoYaST</link>.
  </para>
  <para>
          When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-2/book_caasp_deployment/data/sec_caasp_installquick.html#sec_caasp_installquick_bootstrap">
    Bootstrapping the Cluster</link> click the <guimenu>kubectl config</guimenu>
button to download your new cluster's <filename>kubeconfig</filename> file. This
takes you to a login screen; use the login you created to access Velum. Save the
file as <filename>~/.kube/config</filename> on your host workstation. This 
file enables the remote administration of your cluster.
      </para>
      <figure xml:id="fig.cap.kubectl-download">
   <title>Download kubeconfig</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="kubectl-download.png" format="PNG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 </listitem>
</varlistentry>  
<varlistentry>
  <term>Install kubectl</term>          
  <listitem>
 <para>
     Follow the instructions at
     <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
         Install and Set Up kubectl</link> to install <command>kubectl</command>
     on your host workstation. After installation, run this command to verify
     that it is installed, and that is communicating correctly with your cluster:
 </para>
 <screen>
&prompt.user;kubectl version --short
Client Version: v1.9.1
Server Version: v1.7.7
</screen>
<para>
    As the client is on your workstation, and the server is on your cluster, 
    reporting the server version verifies that <command>kubectl</command> is using 
    <filename>~/.kube/config</filename> and is communicating with your cluster.
</para>
<para>
   The following examples query the cluster configuration and node status:
</para>
<screen>    
&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                        STATUS                     ROLES     AGE  VERSION
b70748d.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
cb77881.infra.caasp.local   Ready,SchedulingDisabled   &lt;none>    4h   v1.7.7
d028551.infra.caasp.local   Ready                      &lt;none>    4h   v1.7.7
</screen>        
 </listitem>  
</varlistentry>
 <varlistentry>
	  <term>
          Install &helm;
      </term>
      <listitem>
 <para>
     Installing &suse; &cap; is a little different than the usual method of
     installing software. Rather than installing packages in the usual way with
     YaST or Zypper, you will use the &helm; client on your host workstation 
     to install the required Kubernetes applications to set up &cap;, and to 
     administer your cluster remotely.
 </para>
      <warning>
        <title>Initialize Only the &helm; Client</title>
        <para>When you initialize &helm; on your workstation be sure
            to initialize only the client, as the server, &tiller;, was
            installed during the CaaSP installation. You do not want two &tiller;
            instances. Also make sure to install the &helm; version that matches
            your &tiller; version.
    </para>
</warning>
 <para>
     See the 
     <link xlink:href="https://docs.helm.sh/using_helm/#quickstart">
         &helm; Quickstart Guide</link> for installation instructions and basic 
     usage examples. You should match the &helm; version with the version of &tiller; 
     that is running on your cluster. To get your &tiller; version, log into your
     &kube; master and query the logs.
 </para>
 <screen>
&prompt.root;kubectl logs -l name=tiller --namespace=kube-system | grep "Starting &tiller;"
[main] 2018/01/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
</screen>
     <tip>
        <title>Log in Directly to &kube; Nodes</title>
        <para>
        By default, &suse; &caasp; allows logging into the &kube; nodes only from
        the admin node. You can set up direct logins to your &kube; nodes from 
        your host workstation by copying the SSH keys from your admin node to your 
        &kube; nodes, and then you will have password-less SSH logins. This is 
        not a best practice for a production deployment, but will make running a 
        test deployment a little easier, especially if you want to use mouse 
        select/middle-click paste to copy commands and output from a virtual 
        terminal on your host workstation.
    </para>
</tip>
<para>
    Download the matching &helm; binary into any directory that is in your PATH
    on your workstation, such as your <filename>~/bin</filename> directory. Then 
    initialize the client only:
 </para>
 <screen>
&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!
 </screen>
 </listitem>  
</varlistentry>
</variablelist>

</sect1>

<sect1 xml:id="sec.cap.storageclass">
 <title>Create Storage Class</title>
 <para>The &kube; cluster requires a persistent storage class for the databases
     to store persistent data. You can provide this with your own 
     storage (e.g. &ses;), or use the built-in hostpath 
     storage type. hostpath is NOT suitable for a production deployment, but it 
     is an easy option for a minimal test deployment.
 </para>

 <para>
    Open an SSH session to your &kube; master node and add the argument
    <literal>--enable-hostpath-provisioner</literal> to
    <filename>/etc/kubernetes/controller-manager</filename>:
</para>
<screen>
&prompt.root;vim /etc/kubernetes/controller-manager 
    KUBE_CONTROLLER_MANAGER_ARGS="\
        --enable-hostpath-provisioner \
        "
</screen>
<para>
    When you use the hostpath storage type you must create the directory 
    <filename>/tmp/hostpath_pv</filename> on all of your &kube; nodes, 
    chmod 777. (This is not necessary when using &ses;. or any other
    external storage backend.)
</para>
<para>
Restart the &kube; controller-manager:
</para>
<screen>
&prompt.root;systemctl restart kube-controller-manager
</screen>
<para>
    Create a <literal>persistent</literal> storage class named 
    <literal>hostpath</literal>: 
</para>
<screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", \
"metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
kubectl create -f -
</screen>
<para>
    Verify that your new storage class has been created:
</para>
<screen>
&prompt.root;kubectl get storageclass
NAME       TYPE
hostpath   kubernetes.io/host-path
</screen>
<para>
    See the &kube; document 
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</link> for detailed information on storage classes.
</para>
</sect1> 
 
<sect1 xml:id="sec.cap.configure">
    <title>Configuring the Minimal Test Deployment</title>
    <para>
        Create a new configuration file for &helm; to use. In this example it is
        called <filename>scf-config-values.yaml</filename>.
    </para>
        <screen>
env:
    # Create a password for your &cap; cluster
    CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
    # Enter the domain you created for your &cap; cluster
    DOMAIN: <replaceable>&exampledomain;</replaceable>
    
    # The fully-qualified domain name of your 
    # User Account and Authentication (UAA) server,
    # which is your &kube; master node
    UAA_HOST: <replaceable>uaa-host.&exampledomain;</replaceable>
    
    # The default is 2793
    UAA_PORT: 2793
    
    # Create a password for your UAA client secret
    UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

kube:
    # The external IP address of your &kube; master
    external_ip: <replaceable>IP_ADDRESS</replaceable>
    
    # Your chosen storage class
    storage_class:
        persistent: "<replaceable>hostpath</replaceable>"
        shared: "shared"
        
    # The Docker registry the images will be fetched from
    # This is the default
    registry:
       hostname: "registry.suse.com"
       username: ""
       password: ""
    organization: "cap-beta"

    # Required for CaaSP 2
    auth: rbac        
    </screen>
</sect1>

<sect1  xml:id="sec.cap.helm-deploy">
    <title>Deploy with &helm;</title>
 <para>
Run the following &helm; commands to complete the deployment. There are six steps,
and they must be run in this order:
</para>
<itemizedlist>
      <listitem>
    <para>
        Download the &suse; &kube; charts repository
    </para>
</listitem>
  <listitem>
    <para>
        Create namespaces
    </para>
</listitem>
  <listitem>
    <para>
        If you are using &ses;, copy the storage secret to the UAA and SCF 
        namespaces
    </para>
  </listitem>
  <listitem>
    <para>
        Install UAA
    </para>
  </listitem>
  <listitem>
    <para>
        Copy UAA CA certificate to SCF namespace
    </para>
  </listitem>
  <listitem>
    <para>
        Install SCF
    </para>
  </listitem>  
</itemizedlist>  

<sect2 xml:id="sec.cap.addrepo-min">
    <title>Install the &kube; charts repository</title>
    <para>
        Download the &suse; &kube; charts repository with &helm;:
    </para>
<screen>
&prompt.user;helm repo add <replaceable>suse-cap</replaceable> https://kubernetes-charts.suse.com/
</screen>
<para>
    You may replace the example <replaceable>suse-cap</replaceable> name with any
    name. Verify with <command>helm</command>:
</para>
<screen>
&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse-cap        https://kubernetes-charts.suse.com/
</screen>
<para>
    List your chart names, as you will need these for some operations:
</para>
<screen>
&prompt.user;helm search suse-cap
NAME             VERSION   DESCRIPTION                                  
suse-cap/cf      2.6.1-rc1 A Helm chart for SUSE Cloud Foundry          
suse-cap/console 1.0.0     A Helm chart for deploying Stratos UI Console
suse-cap/uaa     2.6.1-rc1 A Helm chart for SUSE UAA   
</screen>
</sect2>

<sect2 xml:id="sec.cap.create-namespaces">
    <title>Create Namespaces</title>
<para>
    Use <command>kubectl</command> on your host workstation to create and verify 
    the UAA (User Account and Authentication) and SCF (SUSE Cloud Foundry) 
    namespaces:
</para>    
    <screen>
&prompt.user;kubectl create namespace uaa
&prompt.user;kubectl create namespace scf
&prompt.user;kubectl get namespaces
NAME          STATUS    AGE
default       Active    4d
kube-public   Active    4d
kube-system   Active    4d
scf           Active    1m
uaa           Active    1m
</screen>
</sect2>

<sect2 xml:id="sec.cap.copy-secret">
    <title>Copy &ses; Secret</title>
<para>
    If you are using the hostpath storage class 
    (see <xref linkend="sec.cap.storageclass"/>
    there is no secret so skip this step.
</para>
<para>
If you are using &ses; you must copy the Ceph admin secret to the UAA and SCF
namespaces:
</para>
<screen>
&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -
</screen>
</sect2>

<sect2 xml:id="sec.cap.install-uaa">
    <title>Install UAA</title>
<para>
    Use &helm; to install the UAA (User Account and Authentication) server:
</para>
<screen>    
&prompt.user;helm install suse-cap/uaa --name uaa \
--namespace uaa --values scf-config-values.yaml
</screen>
<para>
    Wait until you have a successful UAA deployment before going to the next
    steps, which you can monitor with the <command>watch</command>
    command:
</para>
<screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
<para>
When the status shows RUNNING for all of the UAA nodes, then proceed to the next
step.
</para>
</sect2>   

<sect2 xml:id="sec.cap.install-scf">
    <title>Install &susecf;</title>
<para>
  First copy your UAA certificate authority (CA) into a shell variable, then 
  use &helm; to install &suse; &cf;:
</para>
<screen>
&prompt.user;CA_CERT="$(kubectl get secret secret --namespace uaa -o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
&prompt.user;helm install -f scf-config-values.yaml --namespace scf suse-cap/cf --set "env.UAA_CA_CERT=${CA_CERT}"
</screen>
<para>
    Now sit back and wait for the pods come online:
</para>
    <screen>
&prompt.user;watch -c 'kubectl get pods --all-namespaces'
</screen>
</sect2>
</sect1>

<sect1 xml:id="sec.cap.stratos-min">
    <title>Install the Stratos Console</title>
    <para>
        Stratos UI is a modern, web-based management application for Cloud 
        Foundry. It provides a graphical management console for both developers
        and system administrators. (See <xref linkend="sec.cap.stratos-prod"/>).
        If you are using hostpath storage, skip the 
        <literal>kubectl get secret ceph-secret-admin</literal> step, as that is
        for getting the &ses; secret.
    </para>
  </sect1>

<!-- upgrade does not work with hostpath  
 <sect1 xml:id="sec.cap.update-min">
  <title>Updating &suse; &cf;, UAA, and Stratos</title> 
     <para>
      Maintenance updates are delivered as container images from the &suse;
      registry and applied with &helm;. See <xref linkend="sec.cap.update"/>.
  </para> 
  </sect1> -->
  
  </chapter>
