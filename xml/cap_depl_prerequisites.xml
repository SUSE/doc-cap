<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<!-- this chapter is not used 
leaving in place until I'm sure it's not needed - cs -->

<chapter version="5.0" xml:id="cha.cap.depl.req"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&productname; Prerequisites</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
This chapter lists the minimum prerequisites to set up &productname; on &suse; &caasp;. 
Some of the commands are particular to either the &kube; master node or a worker 
node. These prerequisites also provide guidance for installing &cap; on any 
Kubernetes cluster. <xref linkend="kube.services"/> shows which services run on
the &kube; master and workers.
 </para>
 
     <figure xml:id="kube.services">
   <title>&suse; &caasp; &kube; Services</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="cap_architecture-3.svg" format="SVG" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>
 
<sect1 xml:id="sec.cap.prereqs">
 <title>Prerequisites to Installing &productname;</title>
  <variablelist>
	<varlistentry>
	  <term>
	    A working &suse; &caasp; installation
	  </term>
	<listitem>
	  <para>
	    This requires a minimum of four physical or virtual nodes: one admin, one 
        Kubernetes master, and two Kubernetes minions. (See the 
        <link xlink:href="https://www.suse.com/documentation/suse-caasp-1/">CaaSP 
        documentation</link>). You also need an Internet connection, as the 
        installer downloads additional packages, and the &kube;
        minions will download 8-10GB of Docker images each.
        </para>
        <para>System requirements depend on the workload; for a test system 60GB 
        of storage should be adequate for each of the &kube; nodes, and 40GB for 
        the admin node.
	  </para>
      <para>
          Any quad-core AMD64/Intel* EM64T processor, and a minimum of 8GB of
          memory per node.
      </para>
      <para>
          The easiest way to create the Kubernetes nodes, after you create the 
          admin node, is to use AutoYaST; see 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-1/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.node.ay">
              Installation with AutoYaST</link>. When you have completed 
<link xlink:href="https://www.suse.com/documentation/suse-caasp-1/singlehtml/book_caasp_deployment/book_caasp_deployment.html#sec.caasp.installquick.bootstrap">
              Bootstrapping the Cluster</link> you can proceed to 
          satisfying the remaining prerequisites, and then installing &suse; &cap;.
      </para>
	</listitem>
	</varlistentry>
	<varlistentry>
	  <term>
	    Name services
	  </term>
      <listitem>
      <para>
    &suse; &cap; (and &suse; &caasp;) requires an external DNS and DHCP server.
       </para>
	</listitem>
	</varlistentry>   
    	<varlistentry>
	  <term>
	    Kubernetes API versions 1.5.x and above
	  </term>
	<listitem>
        <para>
            Find your Kubernetes version by running the command <command>
                kubectl version --short</command> on any Kubernetes node.
	  </para>
	</listitem>
	</varlistentry>
    <varlistentry>
	  <term>
          The kernel parameter swapaccount=1 must be set
	  </term>
	<listitem>
        <para>
            This should already be set in <filename>/etc/default/grub</filename>
            on &caasp;. Look for the line 
            <literal>GRUB_CMDLINE_LINUX_DEFAULT="swapaccount=1".</literal> 
	  </para>
	</listitem>
	</varlistentry>
    <varlistentry>
	  <term>
          docker info must not show aufs as the storage driver
	  </term>
	<listitem>
        <para>
            On &caasp; <command>docker info| grep 'Storage Driver'</command> 
            should return <literal>Storage Driver: btrfs</literal>, and not 
            <literal>Storage Driver: aufs</literal>. On other systems it may not 
            be btrfs. The only requirement is something other than aufs.
	  </para>
	</listitem>
	</varlistentry>
    <varlistentry>
	  <term>
         NTP or systemd-timesyncd must be installed and active
	  </term>
	<listitem>
        <para>
          NTP is set up during the &caasp; installation, and systemd-timesyncd 
          runs on the Kubernetes nodes. Use the <command>ps</command> command 
          to verify:
          <screen>&prompt.root;ps ax | grep 'ntp\|timesyncd'
  1121 ?  Ssl  0:00 /usr/lib/systemd/systemd-timesyncd</screen>
	  </para>
	</listitem>
	</varlistentry>
<varlistentry>
 <term>
     kube-dns must be installed and running
 </term>
  <listitem>
   <para>
     Install kube-dns on your kube-master and verify with these commands:
     <screen>&prompt.root;kubectl apply -f \
  https://raw.githubusercontent.com/SUSE/caasp-services/master/contrib/addons/kubedns/dns.yaml
 
&prompt.root;kubectl cluster-info 
  Kubernetes master is running at http://localhost:8080
  KubeDNS is running at http://localhost:8080/api/v1/proxy/namespaces/kube-system/services/kube-dns  
</screen> 
(See (<link xlink:href="https://www.suse.com/documentation/suse-caasp-1/singlehtml/book_caasp_deployment/book_caasp_deployment.html#installing.dns">Installing Kubernetes DNS</link>)
  </para>
 </listitem>
</varlistentry>    
<varlistentry>
 <term>
   The &kube; cluster must have a storage class to use   
 </term>
  <listitem>
   <para>
    &kube;'s database components require persistent data storage. See 
    <link xlink:href="http://blog.kubernetes.io/2017/03/dynamic-provisioning-and-storage-classes-kubernetes.html">
    Dynamic Provisioning and Storage Classes in Kubernetes </link>
    and <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes</link> 
    for detailed information.
  </para>
  <para>
      For testing and proof-of-concept, you can quickly set up a <literal>hostpath</literal> 
      storage class. This is not for production deployments. Run these commands 
      on the kube-master to create the <literal>hostpath</literal> storage 
      class, and to confirm that it was created. For Kubernetes 1.5.x:
      <screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1beta1", \
 "metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
 kubectl create -f -
</screen>
</para>
<para>
    For Kubernetes 1.6.x:
    <screen>
&prompt.root;echo '{"kind":"StorageClass","apiVersion":"storage.k8s.io/v1", \
 "metadata":{"name":"hostpath"},"provisioner":"kubernetes.io/host-path"}' | \
 kubectl create -f -
</screen>
</para>
<para>
Then verify:
<screen>
&prompt.root;kubectl get storageclass
  NAME       TYPE
  hostpath   kubernetes.io/host-path
</screen>
</para>
</listitem>
</varlistentry>    
   <varlistentry>
 <term>
     TODO Docker must be configured to allow privileged containers.     
 </term>
  <listitem>
   <para>
      TODO How?
  </para>
 </listitem>
</varlistentry> 
 <varlistentry>
 <term>
  Privileged container must be enabled in kube-apiserver
 </term>
  <listitem>
   <para>
       This should already be enabled on the &caasp; kube-master. Look for <literal>KUBE_ALLOW_PRIV="--allow-privileged=true"</literal> in 
       <filename>/etc/kubernetes/config</filename>
       (See <link xlink:href="https://kubernetes.io/docs/admin/kube-apiserver">kube-apiserver</link>)
  </para>
 </listitem>
</varlistentry>
 <varlistentry>
 <term>
  Privileged must be enabled in kubelet
 </term>
  <listitem>
   <para>
       This is configured in <filename>/etc/kubernetes/kubelet</filename> on a
       kube-minion.
<screen>
KUBELET_ARGS="\
    --allow-privileged \
</screen>
   </para>
 </listitem>
</varlistentry> 
 <varlistentry>
 <term>
<!-- TODO The TasksMax property of the containerd service definition 
must be set to infinity. How?-->
 </term>
  <listitem>
   <para>
<!--     TODO how? Are these relevant:
     <screen>
/etc/systemd/logind.conf:#UserTasksMax=12288
/etc/systemd/system.conf:#DefaultTasksMax=512 
</screen>-->
  </para>
 </listitem>
</varlistentry>
<varlistentry>
 <term>
     Helm and Tiller must be installed and active     
 </term>
  <listitem>
   <para>
       Helm is the package manager for Kubernetes. It has a client, Helm, 
       and a server, Tiller. To install Helm, Tiller, and the &caasp; Helm
       charts see <xref linkend="sec.cap.install"/>.
       </para>
   </listitem>
 </varlistentry>
</variablelist>

<tip>
    <title>SSH directly into the &kube; nodes</title>
       <para>
       By default, SSH access to the &kube; nodes is allowed only from the &caasp; 
       admin node. You can log in to the &kube; nodes directly by copying the 
       SSH keys from the admin node to an external host. Copy the keys to their 
       own directory to avoid key name collisions, then specify the key in your 
       SCP command. Replace the hostnames, filenames, and key name in the 
       example with your own: 
<screen>
&prompt.root;external_host:~ mkdir caasp-ssh
&prompt.root;external_host:~ cd caasp-ssh
&prompt.root;external_host:~ scp -r root@caasp-admin:.ssh .
 Password: 
 id_rsa            100% 3247     3.2KB/s   00:00    
 id_rsa.pub        100%  741     0.7KB/s   00:00    
 authorized_keys   100%    0     0.0KB/s   00:00    
 known_hosts       100% 1032     1.0KB/s   00:00 
&prompt.root;external_host:~ scp -i .ssh/id_rsa /pathto/linux-amd64/helm \
 root@kube-master:/root/bin/
</screen>
  </para>
</tip>
<!-- TODO: explanation of readonly/overlay fs -->               


<para>
    Now that you have completed the prerequisites, continue to the next chapter
    to learn how to verify that your system is ready to install &productname;,
    and then install it.
</para>
</sect1>
</chapter>