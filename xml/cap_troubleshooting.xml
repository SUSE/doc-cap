<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha-cap-depl-troubleshooting"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Troubleshooting</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  Cloud stacks are complex, and debugging deployment issues often requires
  digging through multiple layers to find the information you need. Remember
  that the &kubecf; releases must be deployed in the correct order, and that each
  release must deploy successfully, with no failed pods, before deploying the
  next release.
 </para>
 <para>
  Before proceeding with in depth troubleshooting, ensure the following have
  been met as defined in the Support Statement at <xref linkend="sec-cap-platform-support"/>.
 </para>
 <orderedlist>
   <listitem>
    <para>
     The &kube; cluster satisfies the Requirements listed here at
     <link xlink:href="https://documentation.suse.com/suse-cap/&productnumber;/html/cap-guides/cha-cap-depl-kube-requirements.html#sec-cap-changes-kube-reqs"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The <filename>kube-ready-state-check.sh</filename> script has been run on
     the target &kube; cluster and does not show any configuration problems.
    </para>
   </listitem>
   <listitem>
    <para>
     A &suse; Services or Sales Engineer has verified that &productname; works
     correctly on the target &kube; cluster.
    </para>
   </listitem>
  </orderedlist>
 <sect1 xml:id="sec-cap-tbl-logging">
  <title>Logging</title>

  <para>
   There are two types of logs in a deployment of &productname;, applications
   logs and component logs. The following provides a brief overview of each log
   type and how to retrieve them for monitoring and debugging use.
  </para>

  <!-- itemizedlist of log types and how to fetch them -->
  &log-types-and-fetch;
 </sect1>
 <sect1  xml:id="sec-cap-tbl-supportconfig">
  <title>Using Supportconfig</title>

  <para>
   If you ever need to request support, or just want to generate detailed
   system information and logs, use the <command>supportconfig</command>
   utility. Run it with no options to collect basic system information, and
   also cluster logs including &docker;, etcd, flannel, and Velum.
   <command>supportconfig</command> may give you all the information you need.
  </para>

  <para>
   <command>supportconfig -h</command> prints the options. Read the "Gathering
   System Information for Support" chapter in any &sle; &admin; to
   learn more.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-toolong">
  <title>Deployment Is Taking Too Long</title>

  <para>
   A deployment step seems to take too long, or you see that some pods are not
   in a ready state hours after all the others are ready, or a pod shows a lot
   of restarts. This example shows not-ready pods many hours after the others
   have become ready:
  </para>

<screen>&prompt.user;kubectl get pods --namespace kubecf
NAME                     READY STATUS    RESTARTS  AGE
router-3137013061-wlhxb  0/1   Running   0         16h
routing-api-0            0/1   Running   0         16h</screen>

  <para>
   The <literal>Running</literal> status means the pod is bound to a node and
   all of its containers have been created. However, it is not
   <literal>Ready</literal>, which means it is not ready to service requests.
   Use <command>kubectl</command> to print a detailed description of pod events
   and status:
  </para>

<screen>&prompt.user;kubectl describe pod --namespace kubecf router-0</screen>

  <para>
   This prints a lot of information, including IP addresses, routine events,
   warnings, and errors. You should find the reason for the failure in this
   output.
  </para>

  <important>
&deployment-pod-status;
  </important>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-rebuild-depl">
  <title>Deleting and Rebuilding a Deployment</title>

  <para>
   There may be times when you want to delete and rebuild a deployment, for
   example when there are errors in your &values-filename; file, you wish to
   test configuration changes, or a deployment fails and you want to try it again.
  </para>

  <procedure>
   <step>
    <para>
     Remove the <literal>kubecf</literal> release. All resources associated with
     the release of the <literal>suse/kubecf</literal> chart will be removed.
     Replace the example release name with the one used during your installation. 
    </para>
<screen>&prompt.user;helm uninstall <replaceable>kubecf</replaceable></screen>
   </step>
   <step>
    <para>
     Remove the <literal>kubecf</literal> namespace. Replace with the namespace
     where the <literal>suse/kubecf</literal> chart was installed.
    </para>
<screen>&prompt.user;kubectl delete namespace <replaceable>kubecf</replaceable></screen>
   </step>
   <step>
    <para>
     Remove the <literal>cf-operator</literal> release. All resources associated
     with the release of the <literal>suse/cf-operator</literal> chart will be
     removed. Replace the example release name with the one used during your
     installation.
    </para>
<screen>&prompt.user;helm uninstall <replaceable>cf-operator</replaceable></screen>
   </step>
   <step>
    <para>
     Remove the <literal>cf-operator</literal> namespace. Replace with the namespace
     where the <literal>suse/cf-operator</literal> chart was installed.
    </para>
<screen>&prompt.user;kubectl delete namespace <replaceable>cf-operator</replaceable></screen>
   </step>
   <step>
    <para>
     Verify all of the releases are removed.
    </para>
<screen>&prompt.user;helm list --all-namespaces</screen>
   </step>
   <step>
    <para>
     Verify all of the namespaces are removed.
    </para>
<screen>&prompt.user;kubectl get namespaces</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-kubectl-queries">
  <title>Querying with Kubectl</title>

  <para>
   You can safely query with <command>kubectl</command> to get information
   about resources inside your &kube; cluster. <command>kubectl cluster-info
   dump | tee clusterinfo.txt</command> outputs a large amount of information
   about the &kube; master and cluster services to a text file.
  </para>

  <para>
   The following commands give more targeted information about your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     List all cluster resources:
    </para>
<screen>&prompt.user;kubectl get all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List all of your running pods, their internal IP addresses, and which
     &kube; nodes they are running on:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output wide</screen>
   </listitem>
   <listitem>
    <para>
     See all pods, including those with Completed or Failed statuses:
    </para>
<screen>&prompt.user;kubectl get pods --show-all --all-namespaces</screen>
   </listitem>
   <listitem>
    <para>
     List pods in one namespace:
    </para>
<screen>&prompt.user;kubectl get pods --namespace kubecf</screen>
   </listitem>
   <listitem>
    <para>
     Get detailed information about one pod:
    </para>
<screen>&prompt.user;kubectl describe --namespace kubecf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     Read the log file of a pod:
    </para>
<screen>&prompt.user;kubectl logs --namespace kubecf po/diego-cell-0</screen>
   </listitem>
   <listitem>
    <para>
     List all &kube; nodes, then print detailed information about a single
     node:
    </para>
<screen>&prompt.user;kubectl get nodes
&prompt.user;kubectl describe node 6a2752b6fab54bb889029f60de6fa4d5.infra.caasp.local</screen>
   </listitem>
   <listitem>
    <para>
     List all containers in all namespaces, formatted for readability:
    </para>
<screen>&prompt.user;kubectl get pods --all-namespaces --output jsonpath="{..image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c</screen>
   </listitem>
   <listitem>
    <para>
     These two commands check node capacities, to verify that there are enough
     resources for the pods:
    </para>
<screen>&prompt.user;kubectl get nodes --output yaml | grep '\sname\|cpu\|memory'
&prompt.user;kubectl get nodes --output json | \
jq '.items[] | {name: .metadata.name, cap: .status.capacity}'</screen>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-admission-webhook">
  <title>Admission webhook denied</title>
  <para>
    When switching back to Diego from Eirini, the error below can occur:
  </para>
<screen>
&prompt.user;helm install <replaceable>kubecf</replaceable> suse/kubecf --namespace <replaceable>kubecf</replaceable> --values &values-file;
Error: admission webhook "validate-boshdeployment.quarks.cloudfoundry.org" denied the request: Failed to resolve manifest: Failed to interpolate ops 'kubecf-user-provided-properties' for manifest 'kubecf': Applying ops on manifest obj failed in interpolator: Expected to find exactly one matching array item for path '/instance_groups/name=eirini' but found 0
  </screen>
  <para>
    To avoid this error, remove the <literal>eirini-persi-broker</literal> configuration
    before running the command.
  </para>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-helm-namespace">
  <title>Namespace does not exist</title>
  <para>
    When running a &helm; command, an error occurs stating that a namespace does not
    exist. To avoid this error, create the namespace manually with <command>kubectl</command>; before
    running the command:
  </para>
<screen>
&prompt.user;kubectl create namespace <replaceable>name</replaceable>
  </screen>
 </sect1>
 <sect1 xml:id="sec-cap-tbl-log-cache-memory">
  <title>Log-cache Memory Allocation Issue</title>
  <para>
   The log-cache component currently has a memory allocation issue where the node
   memory available is reported instead of the one assigned to the container under
   cgroups. In such a situation, log-cache would start allocating memory based on
   these values, causing a varying range of issues (OOMKills, performance
   degradation, etc.). To address this issue, node affinity must be used to tie
   log-cache to nodes of a uniform size, and then declaring the cache percentage
   based on that number. A limit of 3% has been identified as sufficient.
  </para>
  <para>
   Add the following to your &values-filename;. In the node affinity
   configuration, the values for <literal>key</literal> and
   <literal>values</literal> may need to be changed depending on how notes in
   your cluster are labeled. For more information on labels, see
   <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#built-in-node-labels"/>.
  </para>
<screen>properties:
  log-cache:
    log-cache:
      memory_limit_percent: 3

operations:
  inline:
  - type: replace
    path: /instance_groups/name=log-cache/env?/bosh/agent/settings/affinity
    value:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: <replaceable>kubernetes.io/hostname</replaceable>
              operator: In
              values:
              - <replaceable>0</replaceable>
</screen>
 </sect1>
</chapter>
