<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.depl-gke"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deploying &productname; on Google Kubernetes Engine (GKE)</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <important>
     &readmefirst;
 </important>
 <para>
  &productname; supports deployment on Google Kubernetes Engine (GKE). This chapter describes the steps
  to prepare a &productname; deployment on GKE using its integrated network load balancers. See
  <link xlink:href="https://cloud.google.com/kubernetes-engine/"/>
  for more information on GKE.
 </para>
 <sect1 xml:id="sec.cap.prereqs-gke">
  <title>Prerequisites</title>

  <para>
   The following are required to complete the deployment of &productname; on GKE:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The <literal>gcloud</literal> command line interface. See <link xlink:href="https://cloud.google.com/sdk/gcloud/"/> for more information and download instructions. 
    </para>
   </listitem>
   <listitem>
    <para>
     A Google Cloud Platform (GCP) user account or a service account with the following IAM roles:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>compute.admin</literal>. For details regarding this role, refer to
       <link xlink:href="https://cloud.google.com/iam/docs/understanding-roles#compute-engine-roles"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>container.admin</literal>. For details regarding this role, refer to
       <link xlink:href="https://cloud.google.com/kubernetes-engine/docs/how-to/iam#predefined"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>iam.serviceAccountUser</literal>. For details regarding this role, refer to
       <link xlink:href="https://cloud.google.com/kubernetes-engine/docs/how-to/iam#primitive"/>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     The &kube; command line tool, <literal>kubectl</literal>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.cap.create-gke-cluster">
  <title>Creating a GKE cluster</title>

  <para>
   In order to deploy &productname;, create a cluster that:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Is a <literal>Zonal</literal>, <literal>Regional</literal>, or
     <literal>Private</literal> type. Do not use a <literal>Alpha</literal>
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Uses <literal>Ubuntu</literal> as the host operating system. If using the
     <literal>gcloud</literal> CLI, include <command>--image-type=UBUNTU</command>
     during the cluster creation.
    </para>
   </listitem>
   <listitem>
    <para>
     Allows access to all Cloud APIs (in order for storage to work correctly).
	    <!--	    https://stackoverflow.com/questions/48759906/how-to-allow-allow-full-access-to-all-cloud-apis-for-cloud-api-access-scopes-f?rq=1

	    https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes
	    -->
    </para>
   </listitem>
   <listitem>
    <para>
     Has at least 3 nodes of machine type <literal>n1-standard-4</literal>. If using the
     <literal>gcloud</literal> CLI, include <command>--machine-type=n1-standard-4</command>
     and <command>--num-nodes=3</command> during the cluster creation. For details, see
     <link xlink:href="https://cloud.google.com/compute/docs/machine-types#standard_machine_types"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Has at least 80 GB local storage per node.
    </para>
   </listitem>
   <listitem>
    <para>
     (Optional) Uses preemptible nodes to keep costs low. For detials, see
     <link xlink:href="https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms"/>.
    </para>
   </listitem>
  </itemizedlist>
  <procedure>
   <step>
    <para>
     Set a name for your cluster:
    </para>
<screen>&prompt.user;export CLUSTER_NAME=<replaceable>"cap"</replaceable></screen>
   </step>
   <step>
    <para>
     Set the zone for your cluster:
    </para>
<screen>&prompt.user;export CLUSTER_ZONE=<replaceable>"us-west1-a"</replaceable></screen>
   </step>
   <step>
    <para>
     Set the number of nodes for your cluster:
    </para>
<screen>&prompt.user;export NODE_COUNT=3</screen>
   </step>
   <step>
    <para>
     Create the cluster:
    </para>
<screen>&prompt.user;gcloud container clusters create ${CLUSTER_NAME} \
--image-type=UBUNTU \
--machine-type=n1-standard-4 \
--zone ${CLUSTER_ZONE} \
--num-nodes=$NODE_COUNT \
--no-enable-basic-auth \
--no-issue-client-certificate \
--no-enable-autoupgrade \
--metadata disable-legacy-endpoints=true
</screen>
    <itemizedlist>
     <listitem>
      <para>
       Specify the <command>--no-enable-basic-auth</command> and
       <command>--no-issue-client-certificate</command> flags so that
       <command>kubectl</command> does not use basic or client certificate
       authentication, but uses OAuth Bearer Tokens instead. Configure the
       flags to suit your desired authentication mechanism.
      </para>
     </listitem>
     <listitem>
      <para>
       Specify <command>--no-enable-autoupgrade</command> to disable
       automatic upgrades as this affects the swap accounting changes
       that will be made (see <xref linkend="sec.cap.gke-swap-accounting"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Disable legacy metadata server endpoints using
       <command>--metadata disable-legacy-endpoints=true</command> as a best
       practice as indicated in
       <link xlink:href="https://cloud.google.com/compute/docs/storing-retrieving-metadata#default"/>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.gke-swap-accounting">
  <title>Enable Swap Accounting</title>

  <para>
   Swap accounting is required by &productname;, but not enabled by default.
   When your cluster finishes creating, enable swap accounting using the
   steps below. Ensure your <literal>gcloud</literal> CLI is configured
   correctly.
  </para>

  <procedure xml:id="pro.gke-swap-account">
   <step>
    <para>
     Get the node instance names:
    </para>
<screen>&prompt.user;instance_names=$(gcloud compute instances list --filter=name~${CLUSTER_NAME:?required} --format json | jq --raw-output '.[].name')</screen>
   </step>
   <step>
    <para>
     Set the correct zone:
    </para>
<screen>&prompt.user;gcloud config set compute/zone ${CLUSTER_ZONE:?required}</screen>
   </step>
   <step>
    <para>
     Update the kernel command line and GRUB then restart the virtual machines:
    </para>
    <screen>&prompt.user;echo "$instance_names" | xargs -i{} gcloud compute ssh {} -- "sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0\"/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0 cgroup_enable=memory swapaccount=1\"/g' /etc/default/grub.d/50-cloudimg-settings.cfg &amp;&amp; sudo update-grub &amp;&amp; sudo systemctl reboot -i"</screen>
   </step>
  </procedure>

  <para>
   In the event swap accounting is not successfully enabled on all nodes using
   the previous procedure, perform the following on each node that requires swap
   accounting to be enabled:
  </para>

  <procedure>
   <step>
    <para>
     <command>ssh</command> into the node.
    </para>
<screen>&prompt.user;gcloud compute ssh INSTANCE</screen>
   </step>
   <step>
    <para>
     When inside the node, run the command to update the kernel command line and
     GRUB, then restart the node.
    </para>
<screen>&prompt.user;sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0\"/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0 cgroup_enable=memory swapaccount=1\"/g' /etc/default/grub.d/50-cloudimg-settings.cfg &amp;&amp; sudo update-grub &amp;&amp; sudo systemctl reboot -i</screen>
   </step>
  </procedure>

 </sect1>
 <sect1 xml:id="sec.cap.gke-kubeconfig">
  <title>Get <literal>kubeconfig</literal> File</title>

  <para>
   Get the <literal>kubeconfig</literal> file for your cluster.
  </para>

<screen>&prompt.user;gcloud container clusters get-credentials --zone ${CLUSTER_ZONE:?required} ${CLUSTER_NAME:?required}</screen>

 </sect1>
 <sect1 xml:id="sec.cap.gke-helm-tiller">
  <title>Helm and Tiller</title>

  <para>
   Install &helm; as decribed in
   <link xlink:href="https://docs.helm.sh/using_helm/#installing-helm"/>.
   Then create a &tiller; service account in order to give &tiller;
   sufficient permissions to make changes on your cluster.
  </para>

  <procedure>
   <step>
    <para>
     Create a file named <filename>gke-helm-sa.yaml</filename> with the following:
    </para>
<screen>---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
</screen>
   </step>
   <step>
    <para>
     Create the service account and apply the change:
    </para>
<screen>&prompt.user;kubectl create -f gke-helm-sa.yaml

&prompt.user;helm init --service-account tiller
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.gke-storage">
  <title>Default Storage Class</title>
  <para>
   This example creates a <literal>pd-ssd</literal>
   storage class for your cluster.  Create a file named
   <filename>storage-class.yaml</filename> with the following:
  </para>
<screen>---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
    kubernetes.io/cluster-service: "true"
  name: persistent
parameters:
  type: pd-ssd
provisioner: kubernetes.io/gce-pd
reclaimPolicy: Delete
</screen>
  <para>
   Create the new storage class using the manifest defined:
  </para>
<screen>&prompt.user;kubectl create -f <replaceable>storage-class.yaml</replaceable></screen>
  <para>
   Specify the newly created created storage class, called <literal>persistent</literal>,
   as the value for <literal>kube.storage_class.persistent</literal> in your
   deployment configuration file, like this example:
  </para>
<screen>kube:
  storage_class:
    persistent: "persistent"
</screen>
  <para>
   See <xref linkend="sec.cap.gke-config"/> for a complete example deployment
   configuration file, <filename>scf-config-values.yaml</filename>.
  </para>
 </sect1>

 <sect1 xml:id="sec.cap.gke-dns">
  <title>DNS Configuration</title>

  <para>
   This section provides an overview of the domain and sub-domains that require
   A records to be set up for. The process is described in more detail in the
   deployment section.
  </para>

  <para>
   The following table lists the required domain and sub-domains, using
   <literal>example.com</literal> as the example domain:
  </para>

&dns-tables;
 </sect1>
 <sect1 xml:id="sec.cap.gke-config">
  <title>Deployment Configuration</title>
  <para>
   It is not necessary to create any DNS records before deploying
   <literal>uaa</literal>. Instead, after <literal>uaa</literal>
   is running you will find the load balancer IP address that was automatically
   created during deployment, and then create the necessary records.
  </para>
  <para>
   The following file, <filename>scf-config-values.yaml</filename>, provides a
   complete example deployment configuration. Enter the fully-qualified domain
   name (FQDN) that you intend to use for <literal>DOMAIN</literal> and
   <literal>UAA_HOST</literal>.
  </para>
<screen>
### example deployment configuration file
### scf-config-values.yaml

env:
  # the FQDN of your domain  
  DOMAIN: <replaceable>example.com</replaceable>
  # the UAA prefix is required
  UAA_HOST: uaa.<replaceable>example.com</replaceable>
  UAA_PORT: 2793
  GARDEN_ROOTFS_DRIVER: "overlay-xfs"

kube:
  storage_class:
    persistent: "persistent"
  auth: rbac

secrets:
  # create very strong passwords, and protect them just as 
  # carefully as you would protect any root passwords
  CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
  UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

services:
  loadbalanced: true
</screen>
 </sect1>

 <sect1 xml:id="sec.cap.addrepo-gke">
  <title>Add the &kube; charts repository</title>

  <para>
   Download the &suse; &kube; charts repository with &helm;:
  </para>

<screen>&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/</screen>

  <para>
   You may replace the example <replaceable>suse</replaceable> name with any
   name. Verify with <command>helm</command>:
  </para>

<screen>&prompt.user;helm repo list
NAME       URL
stable     https://kubernetes-charts.storage.googleapis.com
local      http://127.0.0.1:8879/charts
suse       https://kubernetes-charts.suse.com/
</screen>

  <para>
   List your chart names, as you will need these for some operations:
  </para>

&helm-search-suse;

 </sect1>
 <sect1 xml:id="sec.cap.cap-on-gke">
  <title>Deploying &productname;</title>
  <para>
   This section describes how to deploy &productname; on &gke;, and how to
   configure your DNS records.
  </para>

  <sect2 xml:id="sec.cap.gke-deploy-uaa">
   <title>Deploy <literal>uaa</literal></title>
   <para>
    Use &helm; to deploy the <literal>uaa</literal> server:
   </para>
<screen>&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace <replaceable>uaa</replaceable> \
--values <replaceable>scf-config-values.yaml</replaceable>
</screen>
   <para>
    Wait until you have a successful <literal>uaa</literal> deployment before
    going to the next steps. Monitor the progress using the
    <command>watch</command> command:
   </para>
<screen>&prompt.user;watch -c 'kubectl get pods --namespace uaa'</screen>
   <important>
&deployment-pod-status;
   </important>
   <para>
    Once the <literal>uaa</literal> deployment completes, a <literal>uaa</literal>
    service will be exposed on a load balancer public IP. The name of the service
    ends with <literal>-public</literal>. In the following example, the
    <literal>uaa-uaa-public</literal> service is exposed on
    <replaceable>35.197.11.229</replaceable> and port <literal>2793</literal>.
   </para>
<screen>&prompt.user;kubectl get services --namespace uaa | grep public
uaa-uaa-public    LoadBalancer   10.0.67.56     35.197.11.229  2793:30206/TCP
</screen>
   <para>
    Use the DNS service of your choice to set up DNS A records for the service
    from the previous step. Use the public load balancer IP associated with the
    service to create domain mappings:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      For the <literal>uaa-uaa-public</literal> service, map the following domains:
     </para>
     <variablelist>
      <varlistentry>
       <term>uaa.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <literal>uaa.example.com</literal> that points to
         <replaceable>35.197.11.229</replaceable>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>*.uaa.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <literal>*.uaa.example.com</literal> that points to
         <replaceable>35.197.11.229</replaceable>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </itemizedlist>
   <para>
    Use <command>curl</command> to verify you are able to connect to the
    <literal>uaa</literal> OAuth server on the DNS name configured:
   </para>
<screen>&prompt.user;curl -k https://uaa.example.com:2793/.well-known/openid-configuration</screen>
   <para>
    This should return a JSON object, as this abbreviated example shows:
   </para>
<screen>{"issuer":"https://uaa.example.com:2793/oauth/token",
"authorization_endpoint":"https://uaa.example.com:2793
/oauth/authorize","token_endpoint":"https://uaa.example.com:2793/oauth/token"
</screen>
  </sect2>

  <sect2 xml:id="sec.cap.gke-deploy-scf">
   <title>Deploy <literal>scf</literal></title>
   <para>
    Before deploying <literal>scf</literal>, ensure the DNS records for the
    <literal>uaa</literal> domains have been set up as specified in the
    previous section. Next, pass your <literal>uaa</literal> secret and
    certificate to <literal>scf</literal>, then use &helm; to deploy
    <literal>scf</literal>:
   </para>
<screen>&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
   <para>
    Monitor the deployment progress using the <command>watch</command> command:
   </para>
<screen>&prompt.user;watch -c 'kubectl get pods --namespace scf'</screen>
   <para>
    Once the deployment completes, a number of public services will be setup
    using load balancers that has been configured with corresponding load
    balancing rules and probes as well as having the correct ports opened in
    the firewall settings.
   </para>
   <para>
    List the services that have been exposed on the load balancer public IP.
    The name of these services end in <literal>-public</literal>:
   </para>
<screen>&prompt.user;kubectl get services --namespace scf | grep public
diego-ssh-ssh-proxy-public                  LoadBalancer   10.23.249.196  35.197.32.244  2222:31626/TCP                                                                                                                                    1d
router-gorouter-public                      LoadBalancer   10.23.248.85   35.197.18.22   80:31213/TCP,443:30823/TCP,4443:32200/TCP                                                                                                         1d
tcp-router-tcp-router-public                LoadBalancer   10.23.241.17   35.197.53.74   20000:30307/TCP,20001:30630/TCP,20002:32524/TCP,20003:32344/TCP,20004:31514/TCP,20005:30917/TCP,20006:31568/TCP,20007:30701/TCP,20008:31810/TCP   1d
</screen>
   <para>
    Use the DNS service of your choice to set up DNS A records for the services
    from the previous step. Use the public load balancer IP associated with the
    service to create domain mappings:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      For the <literal>router-gorouter-public</literal> service, map the following domains:
     </para>
     <variablelist>
      <varlistentry>
       <term>DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <replaceable>example.com</replaceable> that points to
         <replaceable>35.197.18.22</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>*.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <replaceable>*.example.com</replaceable> that points to
         <replaceable>35.197.18.22</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
    <listitem>
     <para>
      For the <literal>diego-ssh-ssh-proxy-public</literal> service, map the following domain:
     </para>
     <variablelist>
      <varlistentry>
       <term>ssh.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <replaceable>ssh.example.com</replaceable> that points to
         <replaceable>35.197.32.244</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
    <listitem>
     <para>
      For the <literal>tcp-router-tcp-router-public</literal> service, map the following domain:
     </para>
     <variablelist>
      <varlistentry>
       <term>tcp.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for
         <replaceable>tcp.example.com</replaceable> that points to
         <replaceable>35.197.53.74</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </itemizedlist>
   <para>
    Your load balanced deployment of &cap; is now complete. Verify you can
    access the API endpoint:
   </para>
<screen>&prompt.user;cf api --skip-ssl-validation https://api.example.com</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.cap.gke-add-capacity">
  <title>Expanding Capacity of a &cap; Deployment on &gke;</title>

  <para>
   If the current capacity of your &cap; deployment is insufficient for your
   workloads, you can expand the capacity using the procedure in this section.
  </para>

  <para>
   These instructions assume you have followed the procedure in
   <xref linkend="cha.cap.depl-gke"/> and have a running &cap; deployment on
   &aks;. The instructions below will use environment variables defined in
   <xref linkend="sec.cap.create-gke-cluster"/>.
  </para>

  <procedure>
   <step>
    <para>
     Get the most recently created node in the cluster.
    </para>
<screen>&prompt.user;RECENT_VM_NODE=$(gcloud compute instances list --filter=name~${CLUSTER_NAME:?required} --format json | jq --raw-output '[sort_by(.creationTimestamp) | .[].creationTimestamp ] | last | .[0:19] | strptime("%Y-%m-%dT%H:%M:%S") | mktime')</screen>
   </step>
   <step>
    <para>
     <!-- https://cloud.google.com/kubernetes-engine/docs/how-to/resizing-a-cluster -->
     Increase the &kube; node count in the cluster. Replace the example value
     with the number of nodes required for your workload.
    </para>
<screen>&prompt.user;gcloud container clusters resize $CLUSTER_NAME \
--num-nodes <replaceable>4</replaceable>
</screen>
   </step>
   <step>
    <para>
     Verify the new nodes are in a <literal>Ready</literal> state before proceeding.
    </para>
<screen>&prompt.user;kubectl get nodes</screen>
   </step>
   <step>
    <para>
     Enable swap accounting on the new nodes.
    </para>
    <substeps>
     <step>
      <para>
       Get the names of the new node instances.
      </para>
<screen>&prompt.user;export NEW_VM_NODES=$(gcloud compute instances list --filter=name~${CLUSTER_NAME:?required} --format json | jq --raw-output 'sort_by(.creationTimestamp) | .[] | if (.creationTimestamp | .[0:19] | strptime("%Y-%m-%dT%H:%M:%S") | mktime) > '$RECENT_VM_NODE' then .name else empty end')</screen>
     </step>
     <step>
      <para>
       Update the kernel command line and GRUB then restart the virtual machines.
      </para>
<screen>&prompt.user;echo "$NEW_VM_NODES" | xargs -i{} gcloud compute ssh {} -- "sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0\"/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0 cgroup_enable=memory swapaccount=1\"/g' /etc/default/grub.d/50-cloudimg-settings.cfg &amp;&amp; sudo update-grub &amp;&amp; sudo systemctl reboot -i"</screen>
     </step>
     <step>
      <para>
       In the event swap accounting is not successfully enabled on all nodes
       using the previous step, perform the following on each node that
       requires swap accounting to be enabled.
      </para>
      <substeps>
       <step>
	<para>
         <command>ssh</command> into the node.
	</para>
<screen>&prompt.user;gcloud compute ssh INSTANCE</screen>
       </step>
       <step>
        <para>
         When inside the node, run the command to update the kernel command line
	 and GRUB, then restart the node.
	</para>
<screen>&prompt.user;sudo sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0\"/GRUB_CMDLINE_LINUX_DEFAULT=\"console=ttyS0 net.ifnames=0 cgroup_enable=memory swapaccount=1\"/g' /etc/default/grub.d/50-cloudimg-settings.cfg &amp;&amp; sudo update-grub &amp;&amp; sudo systemctl reboot -i</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Verify the new nodes are in a <literal>Ready</literal> state before proceeding.
      </para>
<screen>&prompt.user;kubectl get nodes</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Add or update the following in your
     <filename>scf-config-values.yaml</filename> file to increase the number of
     <literal>diego-cell</literal> in your &cap; deployment. Replace the
     example value with the number required by your workflow.
    </para>
<screen>sizing:
  diego_cell:
    count: <replaceable>4</replaceable>
</screen>
   </step>
   <step>
    <para>
     Pass your <literal>uaa</literal> secret and certificate to <literal>scf</literal>.
    </para>
<screen>&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"
</screen>
   </step>
   <step>
    <para>
     Perform a <command>helm upgrade</command> to apply the change.
    </para>
<screen>&prompt.user;helm upgrade susecf-scf suse/cf \
--values scf-config-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
   </step>
   <step>
    <para>
     Monitor progress of the additional <literal>diego-cell</literal> pods:
    </para>
<screen>&prompt.user;watch -c 'kubectl get pods --namespace scf'</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
