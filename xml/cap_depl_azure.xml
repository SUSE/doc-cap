<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.depl-azure"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing Microsoft Azure for &productname;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <important>
     &readmefirst;
 </important>
 <para>
  &productname; supports deployment on Microsoft Azure Kubernetes Service
  (AKS), Microsoft's managed &kube; service. This chapter describes the steps
  for preparing Azure for a &productname; deployment, with a basic Azure load
  balancer. (See
  <link xlink:href="https://azure.microsoft.com/en-us/services/container-service/">
  Azure Kubernetes Service (AKS)</link> for more information.)
 </para>
 <para>
  In &kube; terminology a node used to be a minion, which was the name for a
  worker node. Now the correct term is simply node (see
  <link xlink:href="https://kubernetes.io/docs/concepts/architecture/nodes/"></link>).
  This can be confusing, as computing nodes have traditionally been defined as
  any device in a network that has an IP address. In Azure they are called
  agent nodes. In this chapter we call them agent nodes or &kube; nodes.
 </para>
 <sect1 xml:id="sec.cap.prereqs-azure">
  <title>Prerequisites</title>

  <para>
   Install <command>az</command>, the Azure command-line client, on your remote
   administration machine. See
   <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">
   Install Azure CLI 2.0</link> for instructions.
  </para>

  <para>
   See the
   <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/reference-index?view=azure-cli-latest">
   Azure CLI 2.0 Reference</link> for a complete <command>az</command> command
   reference.
  </para>

  <para>
   You also need the <command>kubectl</command>, <command>curl</command>,
   <command>sed</command>, and <command>jq</command> commands, &helm; 2.9 or
   newer, and the name of the SSH key that is attached to your Azure account.
   (Get &helm; from
   <link xlink:href="https://github.com/helm/helm/releases">Helm
   Releases</link>.)
  </para>

  <para>
   Log in to your Azure Account:
  </para>

<screen>&prompt.user;az login</screen>

  <para>
   Your Azure user needs the User Access Administrator role. Check your
   assigned roles with the <command>az</command> command:
  </para>

<screen>&prompt.user;az role assignment list --assignee <replaceable>login-name</replaceable>
[...]
"roleDefinitionName": "User Access Administrator",</screen>

  <para>
   If you do not have this role, then you must request it from your Azure
   administrator.
  </para>

  <para>
   You need your Azure subscription ID. Extract it with <command>az</command>:
  </para>

<screen>&prompt.user;az account show --query "{ subscription_id: id }"
{
"subscription_id": <replaceable>"a900cdi2-5983-0376-s7je-d4jdmsif84ca"</replaceable>
}</screen>

  <para>
   Replace the example <varname>subscription-id</varname> in the next command
   with your <varname>subscription-id</varname>. Then export it as an
   environment variable and set it as the current subscription:
  </para>

<screen>&prompt.user;export SUBSCRIPTION_ID=<replaceable>"a900cdi2-5983-0376-s7je-d4jdmsif84ca"</replaceable>

&prompt.user;az account set --subscription $SUBSCRIPTION_ID</screen>

  <para>
   Verify that the <literal>Microsoft.Network</literal>,
   <literal>Microsoft.Storage</literal>, <literal>Microsoft.Compute</literal>,
   and <literal>Microsoft.ContainerService</literal> providers are enabled:
  </para>

<screen>&prompt.user;az provider list | egrep -w 'Microsoft.Network|Microsoft.Storage|Microsoft.Compute|Microsoft.ContainerService'</screen>

  <para>
   If any of these are missing, enable them with the <command>az provider
   register --name <replaceable>provider</replaceable></command> command.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.create-aks-instance">
  <title>Create Resource Group and AKS Instance</title>

  <para>
   Now you can create a new Azure resource group and AKS instance. Set the
   required variables as environment variables, which helps to speed up the
   setup, and to reduce errors. Verify your environment variables at any time
   with <command>echo $<replaceable>VARNAME</replaceable></command>, for
   example:
  </para>

<screen>&prompt.user;echo $RGNAME
cap-aks</screen>

  <para>
   This is especially useful when you run long compound commands to extract and
   set environment variables.
  </para>

  <note>
   <title>Use different names</title>
   <para>
    Ensure each of your AKS clusters use unique resource group and managed
    cluster names, and not copy the examples, especially when your Azure
    subscription supports multiple users. Azure has no tools for sorting
    resources by user, so creating unique names and putting everything in your
    deployment in a single resource group helps you keep track, and you can
    delete the whole deployment by deleting the resource group.
   </para>
  </note>

  <procedure xml:id="pro.envvars-azure">
   <step>
    <para>
     Create the resource group name:
    </para>
<screen>&prompt.user;export RGNAME="<replaceable>cap-aks</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create the AKS managed cluster name. Azure's default is to use the
     resource group name, then prepend it with <literal>MC</literal> and append
     the location, e.g. <literal>MC_cap-aks_cap-aks_eastus</literal>. This
     example uses the creator's initials for the <literal>AKSNAME</literal>
     environment variable, which will be mapped to the <command>az</command>
     command's <command>--name</command> option. The <command>--name</command>
     option is for creating arbitrary names for your AKS resources. This
     example will create a managed cluster named
     <literal>MC_cap-aks_cjs_eastus</literal>:
    </para>
<screen>&prompt.user;export AKSNAME=<replaceable>cjs</replaceable></screen>
   </step>
   <step>
    <para>
     Set the Azure location. (See
     <link xlink:href="https://docs.microsoft.com/en-us/azure/aks/container-service-quotas">
     Quotas and region availability for Azure Kubernetes Service (AKS)</link>
     for supported locations.)
    </para>
<screen>&prompt.user;export REGION="<replaceable>eastus</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the Kubernetes agent node count. (&cap; requires a minimum of 3.)
    </para>
<screen>&prompt.user;export NODECOUNT="<replaceable>3</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the virtual machine size (see
     <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general">
     General purpose virtual machine sizes</link>). A virtual machine size of
     at least <literal>Standard_DS3_v2</literal> using premium storage (see
     <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage">High-performance
     Premium Storage and managed disks for VMs</link>) is recommended. Note
     <literal>managed-premium</literal> has been specified in the example
     <filename>scf-azure-values.yaml</filename> used (see
     <xref linkend="sec.cap.cap-on-azure"/>):
    </para>
<screen>&prompt.user;export NODEVMSIZE="<replaceable>Standard_DS3_v2</replaceable>"</screen>
   </step>
   <step>
    <para>
     Set the public SSH key name associated with your Azure account:
    </para>
<screen>&prompt.user;export SSHKEYVALUE="<replaceable>~/.ssh/id_rsa.pub</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create and set a new admin username:
    </para>
<screen>&prompt.user;export ADMINUSERNAME="<replaceable>scf-admin</replaceable>"</screen>
   </step>
   <step>
    <para>
     Create a unique nodepool name. The default is
     <literal>aks-nodepool</literal> followed by an auto-generated number, e.g.
     <literal>aks-nodepool1-39318075-2</literal>. You have the option to change
     <literal>nodepool1</literal> and create your own unique identifier, for
     example, <literal>mypool</literal>:
    </para>
<screen>&prompt.user;export NODEPOOLNAME="<replaceable>mypool</replaceable>"</screen>
    <para>
     Which results in something like <literal>aks-mypool-39318075-2</literal>.
    </para>
   </step>
  </procedure>

  <para>
   Now that your environment variables are in place, create a new resource
   group:
  </para>

<screen>&prompt.user;az group create --name $RGNAME --location $REGION</screen>

  <para>
   List the &kube; versions currently supported by AKS (see
   <link xlink:href="https://docs.microsoft.com/en-us/azure/aks/supported-kubernetes-versions"/>
   for more information on the AKS version support policy). When creating your cluster
   in the next step, specify a &kube; version for consistent deployments:
  </para>

<screen>&prompt.user;az aks get-versions --location $REGION --output table</screen>

  <para>
   Now you can create a new AKS managed cluster:
  </para>

<screen>&prompt.user;az aks create --resource-group $RGNAME --name $AKSNAME \
 --node-count $NODECOUNT --admin-username $ADMINUSERNAME \
 --ssh-key-value $SSHKEYVALUE --node-vm-size $NODEVMSIZE \
 --node-osdisk-size=60 --nodepool-name $NODEPOOLNAME \
 --kubernetes-version <replaceable>1.11.6</replaceable>
</screen>

  <note>
   <para>
    An OS disk size of at least 60GB must be specified using the
    <command>--node-osdisk-size</command> flag.
   </para>
  </note>

  <para>
   This takes a few minutes. When it is completed, fetch your
   <command>kubectl</command> credentials. The default behavior for <command>az
   aks get-credentials</command> is to merge the new credentials with the
   existing default configuration, and to set the new credentials as as the
   current &kube; context. The context name is your AKSNAME value. You should
   first backup your current configuration, or move it to a different location,
   then fetch the new credentials:
  </para>

<screen>&prompt.user;az aks get-credentials --resource-group $RGNAME --name $AKSNAME
 Merged "cap-aks" as current context in /home/&exampleuser_plain;/.kube/config</screen>

  <para>
   Verify that you can connect to your cluster:
  </para>

<screen>&prompt.user;kubectl get nodes
NAME                    STATUS    ROLES     AGE       VERSION
aks-mypool-47788232-0   Ready     agent     5m        v1.11.6
aks-mypool-47788232-1   Ready     agent     6m        v1.11.6
aks-mypool-47788232-2   Ready     agent     6m        v1.11.6

&prompt.user;kubectl get pods --all-namespaces
NAMESPACE     NAME                                READY  STATUS    RESTARTS   AGE
kube-system   azureproxy-79c5db744-fwqcx          1/1    Running   2          6m
kube-system   heapster-55f855b47-c4mf9            2/2    Running   0          5m
kube-system   kube-dns-v20-7c556f89c5-spgbf       3/3    Running   0          6m
kube-system   kube-dns-v20-7c556f89c5-z2g7b       3/3    Running   0          6m
kube-system   kube-proxy-g9zpk                    1/1    Running   0          6m
kube-system   kube-proxy-kph4v                    1/1    Running   0          6m
kube-system   kube-proxy-xfngh                    1/1    Running   0          6m
kube-system   kube-svc-redirect-2knsj             1/1    Running   0          6m
kube-system   kube-svc-redirect-5nz2p             1/1    Running   0          6m
kube-system   kube-svc-redirect-hlh22             1/1    Running   0          6m
kube-system   kubernetes-dashboard-546686-mr9hz   1/1    Running   1          6m
kube-system   tunnelfront-595565bc78-j8msn        1/1    Running   0          6m</screen>

  <para>
   When all nodes are in a ready state and all pods are running, proceed to the
   next steps.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.tiller-service">
  <title>Create &tiller; Service Account</title>

  <para>
   You must create a &tiller; service account in order to give &tiller;
   sufficient permissions to make changes on your cluster. There are two ways
   to create a service account: from the command line with
   <command>kubectl</command>, or applying a configuration file with
   <command>kubectl</command> and <command>helm</command>.
  </para>

  <para>
   Run these <command>kubectl</command> commands to create your &tiller;
   service account:
  </para>

<screen>&prompt.user;kubectl create serviceaccount tiller --namespace kube-system
 serviceaccount "tiller" created

&prompt.user;kubectl create clusterrolebinding tiller \
--clusterrole cluster-admin \
--serviceaccount=kube-system:tiller
 clusterrolebinding.rbac.authorization.k8s.io "tiller" created

&prompt.user;helm init --upgrade --service-account tiller
 $HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.

 Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
 Happy Helming!</screen>

  <para>
   Creating your &tiller; service account with a configuration file requires
   &helm; version 2.9 or newer (see
   <link xlink:href="https://github.com/helm/helm/releases"/>
  </para>

  <para>
   First create the configuration file, which this example is
   <filename>rbac-config.yaml</filename>:
  </para>

<screen>apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system</screen>

  <para>
   Apply it to your cluster with these commands:
  </para>

<screen>&prompt.user;kubectl create -f rbac-config.yaml
&prompt.user;helm init --service-account tiller</screen>
 </sect1>
 <sect1 xml:id="sec.cap.psp">
  <title>Pod Security Policies</title>

  <para>
   Role-based access control (RBAC) is enabled by default on AKS. &productname;
   1.3.1 and later do not need to be configured manually. Older &cap; releases
   require manual PSP configuration; see <xref linkend="app.psps"/> for
   instructions.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.swap-accounting">
  <title>Enable Swap Accounting</title>

  <para>
   Identify and set the cluster resource group, then enable kernel swap
   accounting. Swap accounting is required by &cap;, but it is not the default
   in AKS nodes. The following commands use the <command>az</command> command
   to modify the GRUB configuration on each node, and then reboot the virtual
   machines.
  </para>

  <procedure xml:id="pro.swap-account">
   <step>
<!--<screen>&prompt.user;export MCRGNAME=$(az group list -o table | grep MC_"$RGNAME"_ | awk '{print $1}')</screen>-->
<screen>&prompt.user;export MCRGNAME=$(az aks show --resource-group $RGNAME --name $AKSNAME --query nodeResourceGroup -o json | jq -r '.')</screen>
   </step>
   <step>
<screen>&prompt.user;export VMNODES=$(az vm list --resource-group $MCRGNAME -o json | jq -r '.[] | select (.tags.poolName | contains("'$NODEPOOLNAME'")) | .name')</screen>
   </step>
   <step>
<screen>&prompt.user;for i in $VMNODES
 do
   az vm run-command invoke -g $MCRGNAME -n $i --command-id RunShellScript --scripts \
   "sudo sed -i -r 's|^(GRUB_CMDLINE_LINUX_DEFAULT=)\"(.*.)\"|\1\"\2 swapaccount=1\"|' \
   /etc/default/grub.d/50-cloudimg-settings.cfg &amp;&amp; sudo update-grub"
   az vm restart -g $MCRGNAME -n $i
done</screen>
   </step>
   <step>
    <para>
     Verify that all nodes are in state "Ready" again, before you continue.
    </para>
<screen>&prompt.user;kubectl get nodes</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.cap-on-azure">
  <title>Deploy &productname; with a Load Balancer</title>
  <para>
   In &productname;, load balancing is enabled by setting the <literal>services.loadbalanced</literal>
   parameter to <literal>true</literal> and specifying a fully qualified domain name (FQDN)
   for the <literal>DOMAIN</literal> parameter in your configuration file. When
   <literal>services.loadbalanced</literal> is set to <literal>true</literal>, the
   <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/#publishing-services-service-types">ServiceType</link>
   of public-facing services will change to
   <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/#loadbalancer">Type LoadBalancer</link>
   and a load balancer will be provisioned for the service. In &azureks;, this provisions a
   <link xlink:href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-overview#skus">Basic Load Balancer</link>.
  </para>

  <para>
   The following is an example configuration file, called <filename>scf-azure-values.yaml</filename>,
   used to deploy &cap; on Azure Kubernetes Service:
  </para>

<screen>env:
  # Use a FQDN
  DOMAIN: <replaceable>autolb.example.com</replaceable>

  # uaa prefix is required
  UAA_HOST: uaa.<replaceable>autolb.example.com</replaceable>
  UAA_PORT: 2793

  #Azure deployment requires overlay
  GARDEN_ROOTFS_DRIVER: "overlay-xfs"

kube:
  storage_class:
    # Azure supports only "default" or "managed-premium"
    persistent: "managed-premium"
    shared: "shared"

  registry:
    hostname: "registry.suse.com"
    username: ""
    password: ""
  organization: "cap"

secrets:
  # Password for user 'admin' in the cluster
  CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>

  # Password for SCF to authenticate with UAA
  UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>

services:
  loadbalanced: true
</screen>

  <sect2 xml:id="sec.cap.loadbalancer-azure-uaa">
   <title>Deploy UAA</title>
   <para>
    Use &helm; to deploy the UAA server:
   </para>
<screen>&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace <replaceable>uaa</replaceable> \
--values <replaceable>scf-azure-values.yaml</replaceable>
</screen>
   <para>
    Wait until you have a successful UAA deployment before going to the next steps. Monitor
    the progress using the <command>watch</command> command: 
   </para>
<screen>&prompt.user;watch -c 'kubectl get pods --namespace uaa'</screen>
   <para>
    Once the deployment completes, a &kube; service for UAA will be exposed on an Azure load
    balancer that is automatically set up by AKS (named <literal>kubernetes</literal>
    in the resource group that hosts the worker node VMs).
   </para>
   <para>
    List the services that have been exposed on the load balancer public
    IP. The name of these services end in <literal>-public</literal>. For example, 
    the <literal>uaa</literal> service is exposed on 
    <replaceable>40.85.188.67</replaceable> and port 2793.
   </para>
<screen>&prompt.user;kubectl get services --namespace uaa | grep public
uaa-uaa-public    LoadBalancer   10.0.67.56     40.85.188.67   2793:32034/TCP
</screen>
   <para>
    Use the DNS service of your choice to set up DNS A records for the service from 
    the previous step. Use the public load balancer IP associated with the service
    to create domain mappings:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      For the <literal>uaa</literal> service, map the following domains:
     </para>
     <variablelist>
      <varlistentry>
       <term>uaa.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <literal>uaa.autolb.example.com</literal> that points to <replaceable>40.85.188.67</replaceable>
        </para>    
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>*.uaa.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <literal>*.uaa.autolb.example.com</literal> that points to <replaceable>40.85.188.67</replaceable>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </itemizedlist>
   <para>
    If you wish to use the DNS service provided by Azure, see the
    <link xlink:href="https://docs.microsoft.com/en-us/azure/dns/"> Azure DNS
    Documentation</link> to learn more.
   </para>
   <para>
    Use <command>curl</command> to verify you are able to connect to the UAA OAuth server
    on the DNS name configured:
   </para>
<screen>&prompt.user;curl -k https://uaa.autolb.example.com:2793/.well-known/openid-configuration</screen>
   <para>
    This should return a JSON object, as this abbreviated example shows:
   </para>

<screen>{"issuer":"https://uaa.autolb.example.com:2793/oauth/token",
"authorization_endpoint":"https://uaa.autolb.example.com:2793
/oauth/authorize","token_endpoint":"https://uaa.autolb.example.com:2793/oauth/token"
</screen>

  </sect2>
  <sect2 xml:id="sec.cap.loadbalancer-azure-scf">
   <title>Deploy SCF</title>
   <para>
    Before deploying SCF, ensure the DNS records for the UAA domains have been
    set up as specified in the previous section. Next, pass your UAA secret and
    certificate to SCF, then use &helm; to deploy SCF:
   </para>
<screen>&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-azure-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
   <para>
    Monitor the deployment progress using the <command>watch</command> command:
   </para>
<screen>&prompt.user;watch -c 'kubectl get pods --namespace scf'</screen>
   <para>
    Once the deployment completes, a number of public services will be setup using
    a load balancer that has been configured with corresponding load balancing rules
    and probes as well as having the correct ports opened in Network Security Group.
   </para>
   <para>
    List the services that have been exposed on the load balancer public
    IP. The name of these services end in <literal>-public</literal>. For example,
    the <literal>gorouter</literal> service is exposed on
    <replaceable>23.96.32.205</replaceable>: 
   </para>
<screen>&prompt.user;kubectl get services --namespace scf | grep public
diego-ssh-ssh-proxy-public                  LoadBalancer   10.0.44.118    40.71.187.83   2222:32412/TCP                                                                                                                                    1d
router-gorouter-public                      LoadBalancer   10.0.116.78    23.96.32.205   80:32136/TCP,443:32527/TCP,4443:31541/TCP                                                                                                         1d
tcp-router-tcp-router-public                LoadBalancer   10.0.132.203   23.96.46.98    20000:30337/TCP,20001:31530/TCP,20002:32118/TCP,20003:30750/TCP,20004:31014/TCP,20005:32678/TCP,20006:31528/TCP,20007:31325/TCP,20008:30060/TCP   1d
</screen>
   <para>
    Use the DNS service of your choice to set up DNS A records for the services from 
    the previous step. Use the public load balancer IP associated with the service
    to create domain mappings:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      For the <literal>gorouter</literal> service, map the following domains:
     </para>
     <variablelist>
      <varlistentry>
       <term>DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <replaceable>autolb.example.com</replaceable>
         that points to <replaceable>23.96.32.205</replaceable> would be created.
        </para>   
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>*.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <replaceable>*.autolb.example.com</replaceable>
         that points to <replaceable>23.96.32.205</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
    <listitem>
     <para>
      For the <literal>diego-ssh</literal> service, map the following domain:
     </para>
     <variablelist>
      <varlistentry>
       <term>ssh.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <replaceable>ssh.autolb.example.com</replaceable>
         that points to <replaceable>40.71.187.83</replaceable> would be created.
        </para>
       </listitem>
       </varlistentry>
     </variablelist>
    </listitem>
    <listitem>
     <para>
      For the <literal>tcp-router</literal> service, map the following domain:
     </para>
     <variablelist>
      <varlistentry>
       <term>tcp.DOMAIN</term>
       <listitem>
        <para>
         Using the example values, an A record for <replaceable>tcp.autolb.example.com</replaceable>
         that points to <replaceable>23.96.46.98</replaceable> would be created.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </itemizedlist>
   <para>
    If you wish to use the DNS service provided by Azure, see the
    <link xlink:href="https://docs.microsoft.com/en-us/azure/dns/"> Azure DNS
    Documentation</link> to learn more.
   </para>
   <para>
    Your load balanced deployment of &cap; is now complete. Verify you can access the API endpoint:
   </para>
   <screen>&prompt.user;cf api --skip-ssl-validation https://api.autolb.example.com</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.cap.azure-service-broker">
  <title>Configuring and Testing the Native &aks; Service Broker</title>

  <para>
   &ms; &azureks; provides a service broker. This section describes how to use
   it with your &productname; deployment.
  </para>

  <para>
   Start by extracting and setting a batch of environment variables:
  </para>

<screen>&prompt.user;SBRGNAME=$(tr -dc 'a-zA-Z0-9' &lt; /dev/urandom | head -c 8)-service-broker
      
&prompt.user;REGION=<replaceable>eastus</replaceable>

&prompt.user;export SUBSCRIPTION_ID=$(az account show | jq -r '.id')

&prompt.user;az group create --name ${SBRGNAME} --location ${REGION}

&prompt.user;SERVICE_PRINCIPAL_INFO=$(az ad sp create-for-rbac --name ${SBRGNAME})

&prompt.user;TENANT_ID=$(echo ${SERVICE_PRINCIPAL_INFO} | jq -r '.tenant')

&prompt.user;CLIENT_ID=$(echo ${SERVICE_PRINCIPAL_INFO} | jq -r '.appId')

&prompt.user;CLIENT_SECRET=$(echo ${SERVICE_PRINCIPAL_INFO} | jq -r '.password')

&prompt.user;echo SBRGNAME=${SBRGNAME}

&prompt.user;echo REGION=${REGION}

&prompt.user;echo SUBSCRIPTION_ID=${SUBSCRIPTION_ID} \; TENANT_ID=${TENANT_ID}\; CLIENT_ID=${CLIENT_ID}\; CLIENT_SECRET=${CLIENT_SECRET}
</screen>

  <para>
   Add the necessary &helm; repositories and download the charts:
  </para>

<screen>&prompt.user;helm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com
        
&prompt.user;helm repo update

&prompt.user;helm install svc-cat/catalog --name catalog \
 --namespace catalog \
 --set controllerManager.healthcheck.enabled=false \
 --set apiserver.healthcheck.enabled=false

&prompt.user;kubectl get apiservice

&prompt.user;helm repo add azure https://kubernetescharts.blob.core.windows.net/azure

&prompt.user;helm repo update
</screen>

  <para>
   Set up the service broker with your variables, then create it:
  </para>

<screen>
&prompt.user;helm install azure/open-service-broker-azure \
--name <replaceable>osba</replaceable> \
--namespace <replaceable>osba</replaceable> \
--set azure.subscriptionId=${SUBSCRIPTION_ID} \
--set azure.tenantId=${TENANT_ID} \
--set azure.clientId=${CLIENT_ID} \
--set azure.clientSecret=${CLIENT_SECRET} \
--set azure.defaultLocation=${REGION} \
--set redis.persistence.storageClass=default \
--set basicAuth.username=$(tr -dc 'a-zA-Z0-9' &lt; /dev/urandom | head -c 16) \
--set basicAuth.password=$(tr -dc 'a-zA-Z0-9' &lt; /dev/urandom | head -c 16) \
--set tls.enabled=false

&prompt.user;cf login

&prompt.user;cf create-service-broker azure $(kubectl get deployment osba-open-service-broker-azure \
--namespace osba -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name == "BASIC_AUTH_USERNAME")].value}') $(kubectl get secret --namespace osba osba-open-service-broker-azure -o jsonpath='{.data.basic-auth-password}' | base64 -d) http://osba-open-service-broker-azure.osba
</screen>

  <para>
   Register the service broker in &scf;:
  </para>

<screen>
&prompt.user;cf service-access -b azure | \
awk '($2 ~ /basic/) { system("cf enable-service-access " $1 " -p " $2)}'
</screen>

  <para>
   Test your new service broker with an example Rails app. First create a space
   and org for your test app:
  </para>

<screen>
&prompt.user;cf create-org <replaceable>testorg</replaceable>

&prompt.user;cf create-space <replaceable>scftest</replaceable> -o <replaceable>testorg</replaceable>

&prompt.user;cf target -o "<replaceable>testorg</replaceable>" -s "<replaceable>scftest</replaceable>"

&prompt.user;cf create-service azure-mysql-5-7 basic <replaceable>scf-rails-example-db</replaceable> \
-c "{ \"location\": \"${REGION}\", \"resourceGroup\": \"${SBRGNAME}\", \"firewallRules\": [{\"name\": \
\"AllowAll\", \"startIPAddress\":\"0.0.0.0\",\"endIPAddress\":\"255.255.255.255\"}]}"

&prompt.user;cf service <replaceable>scf-rails-example-db</replaceable> | grep status
</screen>

  <para>
   Find your new service and optionally disable TLS. You should not disable TLS
   on a production deployment, but it simplifies testing. The
   <literal>mysql2</literal> gem must be configured to use TLS, see
   <link xlink:href="https://github.com/brianmario/mysql2#ssl-options">brianmario/mysql2/SSL
   options</link> on GitHub:
  </para>

<screen>
&prompt.user;az mysql server list --resource-group $SBRGNAME

&prompt.user;az mysql server update --resource-group $SBRGNAME \
--name <replaceable>scftest</replaceable> --ssl-enforcement Disabled
</screen>

  <para>
   Look in your Azure portal to find your database <literal>--name</literal>.
  </para>

  <para>
   Build and push the test Rails app:
  </para>

<screen>
&prompt.user;git clone https://github.com/scf-samples/rails-example

&prompt.user;cd rails-example

&prompt.user;cf push

&prompt.user;cf ssh scf-rails-example -c "export PATH=/home/vcap/deps/0/bin:/usr/local/bin:/usr/bin:/bin &amp;&amp; \
export BUNDLE_PATH=/home/vcap/deps/0/vendor_bundle/ruby/2.5.0 &amp;&amp; \
export BUNDLE_GEMFILE=/home/vcap/app/Gemfile &amp;&amp; cd app &amp;&amp; bundle exec rake db:seed"

&prompt.user;cf service scf-rails-example-db # => bound apps
</screen>

  <para>
   Test your new service deployment with <command>curl</command>, replacing the
   example IP address with your own IP address:
  </para>

<screen>
&prompt.user;curl -k https://scf-rails-example.<replaceable>&publicip;</replaceable>
</screen>

  <para>
   A successful deployment returns output like this abbreviated example:
  </para>

<screen>
 &lt;h1>Hello from Rails on SCF!&lt;/h1>
2018-12-19T12:53:30+00:00

&lt;ul>
  &lt;li>#1: Drink coffee&lt;/li>
  &lt;li>#2: Go to work&lt;/li>
  &lt;li>#3: Have some rest&lt;/li>
&lt;/ul>
</screen>
 </sect1>
</chapter>
