<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.install-production"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deploying &productname; on &suse; &caasp;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <important>
&readmefirst;
</important>
 <para>
  You may set up a minimal deployment on four &kube; nodes for testing. This is
  not sufficient for a production deployment. A basic &productname; production
  deployment requires at least eight hosts plus a storage backend: one &suse;
  &caasp; admin server, three &kube; masters, three &kube; workers, a DNS/DHCP
  server, and a storage backend such as &ses; or NFS. This is a bare minimum,
  and actual requirements are likely to be much larger, depending on your
  workloads. You also need an external workstation for administering your
  cluster. (See <xref linkend="sec.cap.requirements"/>.) You may optionally
  make your &productname; instance highly-available.
 </para>
 <note>
  <title>Remote Administration</title>
  <para>
   You will run most of the commands in this chapter from a remote workstation,
   rather than directly on any of the &productname; nodes. These are indicated
   by the unprivileged user Tux, while root prompts are on a cluster node.
   There are few tasks that need to be performed directly on any of the cluster
   hosts.
  </para>
 </note>
 <para>
  The optional &ha; example in this chapter provides HA only for the
  &productname; cluster, and not for &caasp; or &ses;. See
  <xref linkend="sec.cap.ha-prod"/>.
 </para>
 <sect1 xml:id="sec.cap.prereqs-prod">
  <title>Prerequisites</title>

  <para>
   Calculating hardware requirements is best done with an analysis of your
   expected workloads, traffic patterns, storage needs, and application
   requirements. The following examples are bare minimums to deploy a running
   cluster, and any production deployment will require more.
  </para>

  <variablelist>
   <varlistentry>
    <term>Minimum Hardware Requirements</term>
    <listitem>
     <para>
      8GB of memory per &caasp; dashboard and &kube; master nodes.
     </para>
     <para>
      16GB of memory per &kube; worker.
     </para>
     <para>
      40GB disk space per &caasp; dashboard and &kube; master nodes.
     </para>
     <para>
      60GB disk space per &kube; worker.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Network Requirements</term>
    <listitem>
     <para>
      Your &kube; cluster needs its own domain and network. Each node should
      resolve to its hostname, and to its fully-qualified domain name.
      Typically, a &kube; cluster sits behind a load balancer, which also
      provides external access to the cluster. Another option is to use DNS
      round-robin to the &kube; workers to provide external access. It is also
      a common practice to create a wildcard DNS entry pointing to the domain,
      for example *.&exampledomain;, so that applications can be deployed
      without creating DNS entries for each application. This guide does not
      describe how to set up a load balancer or name services, as these depend
      on customer requirements and existing network architectures.
     </para>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-caasp-3/book_caasp_deployment/data/sec_deploy_requirements_system.html#sec_deploy_requirements_network">
      &suse; &caasp; Deployment Guide: Network Requirements</link> provides
      guidance on network and name services configurations.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Install &suse; &caasp;</term>
    <listitem>
     <para>
      &productname; is supported on &suse; &caasp; 3.x.
     </para>
     <para>
      After installing
      <link xlink:href=
      "https://www.suse.com/documentation/suse-caasp-2/">&caasp;
      2</link> or
      <link xlink:href=
      "https://www.suse.com/documentation/suse-caasp-3/">&caasp;
      3</link> and logging into the Velum Web interface, check the box to
      install Tiller (&helm;'s server component).
     </para>
     <figure xml:id="fig.cap.install-tiller-prod">
      <title>Install Tiller</title>
      <mediaobject>
       <imageobject>
        <imagedata fileref="install-tiller.png" format="PNG" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Take note of the <guimenu>Overlay network settings</guimenu>. These
      define the networks that are exclusive to the internal &kube; cluster
      communications. They are not externally accessible. You may assign
      different networks to avoid address collisions.
     </para>
     <para>
      There is also a form for proxy settings; if you're not using a proxy then
      leave it empty.
     </para>
     <para>
      The easiest way to create the &kube; nodes, after you create the admin
      node, is to use AutoYaST; see
      <link xlink:href="https://www.suse.com/documentation/suse-caasp-3/book_caasp_deployment/data/sec_deploy_nodes_worker_install.html#sec_deploy_nodes_worker_install_manual_autoyast">
      Installation with AutoYaST</link>. Set up &caasp; with one admin node and
      at least three Kubernetes masters and three Kubernetes workers. You also
      need an Internet connection, as the installer downloads additional
      packages, and the &kube; workers will each download ~10GB of Docker
      images.
     </para>
     <figure xml:id="fig.cap.select-master-nodes">
      <title>Assigning Roles to Nodes</title>
      <mediaobject>
       <imageobject>
        <imagedata fileref="select-master-nodes.png" format="PNG" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When you have completed
      <link xlink:href="https://www.suse.com/documentation/suse-caasp-3/book_caasp_deployment/data/sec_deploy_install_bootstrap.html">
      Bootstrapping the Cluster</link> click the <guimenu>kubectl
      config</guimenu> button to download your new cluster's
      <filename>kubeconfig</filename> file. This takes you to a login screen;
      use the login you created to access Velum. Save the file as
      <filename>~/.kube/config</filename> on your workstation. This file
      enables the remote administration of your cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Install kubectl</term>
    <listitem>
     <para>
      To install <command>kubectl</command> on a &slea; 12 SP3 or 15
      workstation, install the package <package>kubernetes-client</package>
      from the <emphasis>Public Cloud</emphasis> module. For other operating
      systems, follow the instructions at
      <link xlink:href=
      "https://kubernetes.io/docs/tasks/tools/install-kubectl/" />.
      After installation, run this command to verify that it is installed, and
      that is communicating correctly with your cluster:
     </para>
<screen>&prompt.user;kubectl version --short
Client Version: v1.10.7
Server Version: v1.10.11</screen>
     <para>
      As the client is on your workstation, and the server is on your cluster,
      reporting the server version verifies that <command>kubectl</command> is
      using <filename>~/.kube/config</filename> and is communicating with your
      cluster.
     </para>
     <para>
      The following <command>kubectl</command> examples query the cluster
      configuration and node status:
     </para>
<screen>&prompt.user;kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://&kubeip;:6443
  name: local
contexts:
[...]

&prompt.user;kubectl get nodes
NAME                  STATUS   ROLES     AGE  VERSION
ef254d3.example.com   Ready    Master    4h   v1.10.11
b70748d.example.com   Ready    Master    4h   v1.10.11
cb77881.example.com   Ready    Master    4h   v1.10.11
d028551.example.com   Ready    &lt;none>    4h   v1.10.11
[...]</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Install &helm;</term>
    <listitem>
     <para>
      Deploying &productname; is different than the usual method of installing
      software. Rather than installing packages in the usual way with YaST or
      Zypper, you will install the &helm; client on your workstation to install
      the required Kubernetes applications to set up &productname;, and to
      administer your cluster remotely. &helm; is the &kube; package manager.
      The &helm; client goes on your remote administration computer, and Tiller
      is &helm;'s server, which is installed on your &kube; cluster.
     </para>
     <para>
      &helm; client version 2.9 or higher is required.
     </para>
     <warning>
      <title>Initialize Only the &helm; Client</title>
      <para>
       When you initialize &helm; on your workstation be sure to initialize
       only the client, as the server, &tiller;, was installed during the
       &caasp; installation. You do not want two &tiller; instances.
      </para>
     </warning>
     <para>
      If the Linux distribution on your workstation doesn't provide the correct
      &helm; version, or you are using some other platform, see the
      <link xlink:href="https://docs.helm.sh/using_helm/#quickstart"> &helm;
      Quickstart Guide</link> for installation instructions and basic usage
      examples. Download the &helm; binary into any directory that is in your
      PATH on your workstation, such as your <filename>~/bin</filename>
      directory. Then initialize the client only:
     </para>
<screen>&prompt.user;helm init --client-only
Creating /home/&exampleuser_plain;/.helm 
Creating /home/&exampleuser_plain;/.helm/repository 
Creating /home/&exampleuser_plain;/.helm/repository/cache 
Creating /home/&exampleuser_plain;/.helm/repository/local 
Creating /home/&exampleuser_plain;/.helm/plugins 
Creating /home/&exampleuser_plain;/.helm/starters 
Creating /home/&exampleuser_plain;/.helm/cache/archive 
Creating /home/&exampleuser_plain;/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Adding local repo with URL: http://127.0.0.1:8879/charts 
$HELM_HOME has been configured at /home/&exampleuser_plain;/.helm.
Not installing &tiller; due to 'client-only' flag having been set
Happy Helming!</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.cap.psps">
  <title>Pod Security Policy</title>

  <para>
   &suse; &caasp; 3 includes Pod Security Policy (PSP) support. This change
   adds two new PSPs to &caasp; 3:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <literal>unprivileged</literal>, which is the default assigned to all
     users. The unprivileged Pod Security Policy is intended as a reasonable
     compromise between the reality of &kube; workloads, and the
     <literal>suse:caasp:psp:privileged</literal> role. By default, this PSP is
     granted to all users and service accounts.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>privileged</literal>, which is intended to be assigned only to
     trusted workloads. It applies few restrictions, and should only be
     assigned to highly trusted users.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &productname; &productnumber; includes the necessary PSP configurations in
   the &helm; charts to run on &suse; &caasp;, and are set up automatically
   without requiring manual configuration. See <xref linkend="app.psps"/> for
   instructions on applying the necessary PSPs manually on older &cap;
   releases.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.storageclass-prod">
  <title>Choose Storage Class</title>

  <para>
   The &kube; cluster requires a persistent storage class for the databases to
   store persistent data. Your available storage classes depend on which
   storage cluster you are using (&ses; users, see
   <link xlink:href="https://www.suse.com/documentation/suse-caasp-3/book_caasp_admin/data/cha_admin_integration.html">&suse;
   &caasp; Integration with SES</link>). After connecting your storage backend
   use <command>kubectl</command> to see your available storage classes. This
   example is for an NFS storage class:
  </para>

<screen>&prompt.user;kubectl get storageclass
NAME         PROVISIONER   AGE
persistent   nfs           10d    
    </screen>

  <para>
   Creating a default storage class is useful for a number of scenarios, such
   as using MiniBroker. When your storage class is already created, run the
   following command (substituting the name of your storage class) to make it
   the default:
  </para>

<screen>&prompt.user;kubectl patch storageclass <replaceable>persistent</replaceable> \
 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    
&prompt.user;kubectl get storageclass
NAME                   PROVISIONER   AGE
persistent (default)   nfs           10d
</screen>

  <para>
   See <xref linkend="sec.cap.configure-prod"/> to learn where to configure
   your storage class for &productname;. See the &kube; document
   <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent
   Volumes</link> for detailed information on storage classes.
  </para>
 </sect1>
 <sect1 xml:id="sec.cap.teststorageclass-prod">
  <title>Test Storage Class</title>

  <para>
   You may test that your storage class is properly configured before deploying
   &productname; by creating a persistent volume claim on your storage class,
   then verifying that the status of the claim is <literal>bound</literal>, and
   a volume has been created.
  </para>

  <para>
   First copy the following configuration file, which in this example is named
   <filename>test-storage-class.yaml</filename>, substituting the name of your
   <replaceable>storageClassName</replaceable>:
  </para>

<screen>---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-sc-persistent
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: <replaceable>persistent</replaceable></screen>

  <para>
   Create your persistent volume claim:
  </para>

<screen>&prompt.user;kubectl create -f test-storage-class.yaml
persistentvolumeclaim "test-sc-persistent" created</screen>

  <para>
   Check that the claim has been created, and that the status is
   <literal>bound</literal>:
  </para>

<screen>&prompt.user;kubectl get pv,pvc
NAME                                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                        STORAGECLASS   REASON    AGE
pv/pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c   1Gi        RWO            Delete           Bound     default/test-sc-persistent   persistent               2m

NAME                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc/test-sc-persistent   Bound     pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c   1Gi        RWO            persistent     2m</screen>

  <para>
   This verifies that your storage class is correctly configured. Delete your
   volume claims when you're finished:
  </para>

<screen>&prompt.user;kubectl delete pv/pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c
persistentvolume "pvc-c464ed6a-3852-11e8-bd10-90b8d0c59f1c" deleted

&prompt.user;kubectl delete pvc/test-sc-persistent
persistentvolumeclaim "test-sc-persistent" deleted</screen>

  <para>
   If something goes wrong and your volume claims get stuck in
   <literal>pending</literal> status, you can force deletion with the
   <command>--grace-period=0</command> option:
  </para>

<screen>&prompt.user;kubectl delete pvc/test-sc-persistent --grace-period=0</screen>
 </sect1>
 <sect1 xml:id="sec.cap.configure-prod">
  <title>Configure the &productname; Production Deployment</title>

  <para>
   Create a configuration file on your workstation for &helm; to use. In this
   example it is called <filename>scf-config-values.yaml</filename>. (See
   <xref linkend="app.psps"/> for configuration instructions for releases older
   than 1.3.1.)
  </para>

  <para>
   The example <filename>scf-config-values.yaml</filename> file is for a simple
   deployment without an ingress controller or load balancer. Instead, assign
   one worker node an external IP address, and map this to the domain name to
   provide external access to the cluster. The <literal>external_ips</literal>
   needs this address to provide access to the Stratos Web interface.
   <literal>external_ips</literal> also needs the internal IP addresses of the
   worker nodes to provide access to services.
  </para>

<screen>
env:
  # Enter the domain you created for your CAP cluster
  DOMAIN: <replaceable>&exampledomain;</replaceable>
    
  # <literal>uaa</literal> host and port
  UAA_HOST: uaa.<replaceable>&exampledomain;</replaceable>
  UAA_PORT: 2793

kube:
  external_ips: ["<replaceable>&kubeip;</replaceable>", "<replaceable>&subnetI;</replaceable>", "<replaceable>&subnetII;</replaceable>", "<replaceable>&subnetIII;</replaceable>"]

  storage_class:
    persistent: <replaceable>"persistent"</replaceable>
    shared: "shared"
        
  # The registry the images will be fetched from.
  # The values below should work for
  # a default installation from the SUSE registry.
  registry:
    hostname: "registry.suse.com"
    username: ""
    password: ""
  organization: "cap"

secrets:
  # Create a password for your CAP cluster
  CLUSTER_ADMIN_PASSWORD: <replaceable>password</replaceable>
    
  # Create a password for your uaa client secret
  UAA_ADMIN_CLIENT_SECRET: <replaceable>password</replaceable>
  </screen>

  <important>
   <title>Protect UAA_ADMIN_CLIENT_SECRET</title>
   <para>
    The UAA_ADMIN_CLIENT_SECRET is the master password for access to your &cap;
    cluster. Make this a very strong password, and protect it just as carefully
    as you would protect any root password.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="sec.cap.helm-deploy-prod">
  <title>Deploy with &helm;</title>

  <para>
   The following list provides an overview of &helm; commands to complete the
   deployment. Included are links to detailed descriptions.
  </para>

  <procedure>
   <step>
    <para>
     Download the &suse; &kube; charts repository
     (<xref
     linkend="sec.cap.addrepo-prod" />)
    </para>
   </step>
   <step>
    <para>
     Copy the storage secret of your storage cluster to the
     <literal>uaa</literal> and <literal>scf</literal> namespaces
     (<xref linkend="sec.cap.copy-secret-prod" />)
    </para>
   </step>
   <step>
    <para>
     Deploy <literal>uaa</literal>
     (<xref linkend="sec.cap.install-uaa-prod" />)
    </para>
   </step>
   <step>
    <para>
     Copy the <literal>uaa</literal> secret and certificate to the
     <literal>scf</literal> namespace, deploy <literal>scf</literal>
     (<xref linkend="sec.cap.install-scf-prod" />)
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.cap.addrepo-prod">
  <title>Add the &kube; charts repository</title>

  <para>
   Download the &suse; &kube; charts repository with &helm;:
  </para>

<screen>&prompt.user;helm repo add <replaceable>suse</replaceable> https://kubernetes-charts.suse.com/</screen>

  <para>
   You may replace the example <replaceable>suse</replaceable> name with any
   name. Verify with <command>helm</command>:
  </para>

<screen>&prompt.user;helm repo list
NAME            URL                                             
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts                    
suse            https://kubernetes-charts.suse.com/</screen>

  <para>
   List your chart names, as you will need these for some operations:
  </para>
  
&helm-search-suse;
 
</sect1>
 <sect1 xml:id="sec.cap.copy-secret-prod">
  <title>Copy &ses; Secret</title>

  <para>
   If you are using &ses; you must copy the Ceph admin secret to the
   <literal>uaa</literal> and <literal>scf</literal> namespaces:
  </para>

<screen>&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed 's/"namespace": "default"/"namespace": "uaa"/' | kubectl create -f -

&prompt.user;kubectl get secret ceph-secret-admin -o json --namespace default | \
sed's/"namespace": "default"/"namespace": "scf"/' | kubectl create -f -</screen>
 </sect1>
 <sect1 xml:id="sec.cap.install-uaa-prod">
  <title>Deploy <literal>uaa</literal></title>

  <para>
   Use &helm; to deploy the <literal>uaa</literal> (User Account and
   Authentication) server. You may create your own release
   <literal>--name</literal>:
  </para>

<screen>&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \
--values scf-config-values.yaml</screen>

  <para>
   Wait until you have a successful <literal>uaa</literal> deployment before
   going to the next steps, which you can monitor with the
   <command>watch</command> command:
  </para>

<screen>&prompt.user;watch -c 'kubectl get pods --namespace uaa'</screen>

  <para>
   When the status shows RUNNING for all of the <literal>uaa</literal> nodes,
   proceed to deploying &suse; &cf;. Pressing <keycombo>
   <keycap function="control"/> <keycap>C</keycap> </keycombo> stops the
   <command>watch</command> command.
  </para>

  <important>
&completed-pods;
  </important>
 </sect1>
 <sect1 xml:id="sec.cap.install-scf-prod">
  <title>Deploy <literal>scf</literal></title>

  <para>
   First pass your <literal>uaa</literal> secret and certificate to
   <literal>scf</literal>, then use &helm; to install &suse; &cf;:
  </para>

<screen>&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
-o jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
-o jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"</screen>

  <para>
   Now sit back and wait for the pods come online:
  </para>

<screen>&prompt.user;watch -c 'kubectl get pods --namespace scf'</screen>

  <para>
   When all services are running use the &cf; command-line interface to log in
   to &suse; &cf; to deploy and manage your applications. (See
   <xref linkend="sec.cap.cf-cli"/>)
  </para>
 </sect1>
</chapter>
