<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.cap.ha"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&productname; &ha;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec.cap.ha-prod">
  <title>Configuring &cap; for &ha;</title>

  <para>
   There are two ways to make your &productname; deployment highly available.
   The simplest method is to set the <literal>HA</literal> parameter in your
   deployment configuration file to <literal>true</literal>. The second method
   is to create custom configuration files with your own custom values.
  </para>

  <sect2 xml:id="sec.cap.ha-defaults">
   <title>Finding Default and Allowable Sizing Values</title>
   <para>
    The <literal>sizing:</literal> section in the &helm;
    <filename>values.yaml</filename> files for each namespace describes which
    roles can be scaled, and the scaling options for each role. You may use
    <command>helm inspect</command> to read the <literal>sizing:</literal>
    section in the &helm; charts:
   </para>
<screen>
&prompt.user;helm inspect suse/uaa | less +/sizing:
&prompt.user;helm inspect suse/cf | less +/sizing:
</screen>
   <para>
    Another way is to use Perl to extract the information for each role from
    the <literal>sizing:</literal> section. The following example is for the
    <literal>uaa</literal> namespace.
   </para>
<screen>
&prompt.user;helm inspect values suse/uaa | \
perl -ne '/^sizing/..0 and do { print $.,":",$_ if /^ [a-z]/ || /high avail|scale|count/ }'
151:    # The mysql instance group can scale between 1 and 3 instances.
152:    # For high availability it needs at least 2 instances.
153:    count: 1
178:    # The secret-generation instance group cannot be scaled.
179:    count: 1
207:  #   for managing user accounts and for registering OAuth2 clients, as well as
216:    # The uaa instance group can scale between 1 and 65535 instances.
217:    # For high availability it needs at least 2 instances.
218:    count: 1</screen>
   <para>
    The default <filename>values.yaml</filename> files are also included in
    this guide at <xref linkend="app.uaa.values.yaml"/> and
    <xref linkend="app.scf.values.yaml"/>. .
   </para>
  </sect2>

  <sect2 xml:id="sec.cap.ha-simple">
   <title>Simple &ha; Configuration</title>
   <important>   
&fresh-namespace;
</important>
   <para>
    The simplest way to make your &productname; deployment highly available is
    to set <literal>HA</literal> to <literal>true</literal> in your deployment
    configuration file, for example
    <filename>scf-config-values.yaml</filename>:
   </para>
<screen>
config:
  # Flag to activate high-availability mode
  HA: true
</screen>
   <para>
    Or, you may pass it as a command line option when you are deploying with
    &helm;, for example:
   </para>
<screen>
&prompt.user;helm install suse/uaa \
 --name susecf-uaa \
 --namespace uaa \ 
 --values scf-config-values.yaml \
 --set config.HA=true
</screen>
   <para>
    This changes all roles with a default size of 1 to the minimum required for
    a &ha; deployment. It is not possible to customize any of the sizing
    values.
   </para>
  </sect2>

  <sect2 xml:id="sec.cap.ha-custom">
   <title>Example Custom &ha; Configurations</title>
   <para>
    The following two example &ha; configuration files are for the
    <literal>uaa</literal> and <literal>scf</literal> namespaces. The example
    values are not meant to be copied, as these depend on your particular
    deployment and requirements. Do not change the <literal>config.HA</literal>
    flag to <literal>true</literal> (see <xref linkend="sec.cap.ha-simple"/>.)
   </para>

   &uaa-in-scf;

   <para>
    The first example is for the <literal>uaa</literal> namespace,
    <filename>uaa-sizing.yaml</filename>. The values specified are the minimum
    required for a &ha; deployment (that is equivalent to setting
    <literal>config.HA</literal> to true):
   </para>
<screen>
sizing:
  mysql:
    count: 2
  uaa:
    count: 2
</screen>
   <para>
    The second example is for <literal>scf</literal>,
    <filename>scf-sizing.yaml</filename>. The values specified are the minimum
    required for a &ha; deployment (that is equivalent to setting
    <literal>config.HA</literal> to true), except for
    <literal>diego-cell</literal> which includes additional instances:
   </para>
<screen>
sizing:
  adapter:
    count: 2
  api_group:
    count: 2
  cc_clock:
    count: 2
  cc_uploader:
    count: 2
  cc_worker:
    count: 2
  cf_usb:
    count: 2
  diego_api:
    count: 2
  diego_brain:
    count: 2
  diego_cell:
    count: 6
  diego_ssh:
    count: 2
  doppler:
    count: 2
  log-api:
    count: 2
  mysql:
    count: 2
  nats:
    count: 2
  nfs_broker:
    count: 2
  router:
    count: 2
  routing_api:
    count: 2
  syslog_scheduler:
    count: 2
  tcp_router:
    count: 2
</screen>
   <important>
&fresh-namespace;
</important>
   <para>
    After creating your configuration files, follow the steps in
    <xref linkend="sec.cap.configure-prod"/> until you get to
    <xref linkend="sec.cap.install-uaa-prod"/>. Then deploy
    <literal>uaa</literal> with this command:
   </para>
<screen>    
&prompt.user;helm install suse/uaa \
--name <replaceable>susecf-uaa</replaceable> \
--namespace uaa \ 
--values scf-config-values.yaml \
--values uaa-sizing.yaml
</screen>
   &uaa-deploy-complete;
   <para>
    When the <literal>uaa</literal> deployment completes, deploy SCF with these
    commands:
   </para>
<screen>&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
--output jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
--output jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm install suse/cf \
--name <replaceable>susecf-scf</replaceable> \
--namespace scf \
--values scf-config-values.yaml \
--values scf-sizing.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
   <para>
    The HA pods with the following roles will enter in both passive and ready
    states; there should always be at least one pod in each role that is ready.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      diego-brain
     </para>
    </listitem>
    <listitem>
     <para>
      diego-database
     </para>
    </listitem>
    <listitem>
     <para>
      routing-api
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You can confirm this by looking at the logs inside the container. Look for
    <literal>.consul-lock.acquiring-lock</literal>.
   </para>
   <para>
    Some roles follow an active/passive scaling model, meaning all pods except
    the active one will be shown as NOT READY by &kube;. This is appropriate
    and expected behavior.
   </para>
  </sect2>

  <sect2 xml:id="sec.cap.ha-prod-upgrade">
   <title>Upgrading a non-&ha; Deployment to &ha;</title>
   <para>
    You may make a non-&ha; deployment highly available by upgrading with
    &helm;:
   </para>
<screen>
&prompt.user;helm upgrade <replaceable>susecf-uaa</replaceable> suse/uaa \
--values scf-config-values.yaml \
--values uaa-sizing.yaml 

&prompt.user;SECRET=$(kubectl get pods --namespace uaa \
--output jsonpath='{.items[?(.metadata.name=="uaa-0")].spec.containers[?(.name=="uaa")].env[?(.name=="INTERNAL_CA_CERT")].valueFrom.secretKeyRef.name}')

&prompt.user;CA_CERT="$(kubectl get secret $SECRET --namespace uaa \
--output jsonpath="{.data['internal-ca-cert']}" | base64 --decode -)"    

&prompt.user;helm upgrade <replaceable>susecf-scf</replaceable> suse/cf \
--values scf-config-values.yaml \
--values scf-sizing.yaml \
--set "secrets.UAA_CA_CERT=${CA_CERT}"
</screen>
   <para>
    This may take a long time, and your cluster will be unavailable until the
    upgrade is complete.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.cap.availability-zones">
  <title>Availability Zones</title>

  <para>
   Availability Zones (AZ) are logical arrangements of compute nodes within a
   region that provide isolation from each other. A deployment that is
   distributed across multiple AZs can use this separation to increase
   resiliency against downtime in the event a given zone experiences issues.
  </para>
  <para>
   Refer to the following for platform-specific information about availability
   zones:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <link xlink:href="https://azure.microsoft.com/en-ca/updates/availability-zones-az-support-for-aks/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://cloud.google.com/compute/docs/regions-zones/"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://docs.cloudfoundry.org/concepts/high-availability.html#azs"/>
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec.cap.availability-zone-handling">
   <title>Availability Zone Information Handling</title>
   <para>
    In &cap;, availability zone handling is done using the
    <literal>AZ_LABEL_NAME</literal> &helm; chart value. By default,
    <literal>AZ_LABEL_NAME</literal> is set to
    <literal>failure-domain.beta.kubernetes.io/zone</literal>, which is the
    predefined &kube; label for availability zones. On most public cloud
    providers, nodes will already have this label set and availability zone
    support will work without further configuration. For on-premise
    installations, it is recommended that nodes are labeled with the same label.
   </para>
   <para>
    Run the following to see the labels on your nodes.
   </para>
<screen>&prompt.user;kubectl get nodes --show-labels</screen>
   <para>
    To label a node, use <command>kubectl label nodes </command>. For example:
   </para>
<screen>&prompt.user;kubectl label nodes <replaceable>cap-worker-1</replaceable> <replaceable>failure-domain.beta.kubernetes.io/zone=zone-1</replaceable></screen>
   <para>
    To see which node and availability zone a given
    <literal>diego-cell</literal> pod is assigned to, refer to the following
    example:
   </para>
<screen>&prompt.user;kubectl logs <replaceable>diego-cell-0</replaceable> --namespace scf | grep ^AZ</screen>
   <para>
    For more information on the
    <literal>failure-domain.beta.kubernetes.io/zone</literal> label, see
    <link xlink:href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone"/>.
   </para>
   <para>
    Note that due to a bug in &cap; 1.4 and earlier, this label was not working for <literal>AZ_LABEL_NAME</literal>.
   </para>
   <important>
    <title>Performance with Availability Zones</title>
    <para>
     For the best performance, all availability zones should have a similar
     number of nodes because app instances will be evenly distributed, so that
     each zone has about the same number of instances.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
